class io_context
  : public execution_context
{
    typedef detail::io_context_impl impl_type;
	
	//和mongodb相关的核心接口如下
	std::size_t run_for(const chrono::duration<Rep, Period>& rel_time);
	ASIO_DECL count_type run(asio::error_code& ec);
	std::size_t run_one_for(const chrono::duration<Rep, Period>& rel_time);
	ASIO_DECL count_type poll(asio::error_code& ec);
	io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
	io_context::post(ASIO_MOVE_ARG(CompletionHandler) handler)
	
  // The implementation.
  //以上接口的具体实现实际上在该类中完成，也就是scheduler类
  impl_type& impl_;
}

io_context类中包含一个impl_成员，该成员类型impl_type实际上就是detail::io_context_impl，io_context_impl的来源定义如下：
typedef class scheduler io_context_impl;io_context.impl_对应scheduler类，类成员及其核心接口实现在scheduler.hpp和scheduler.ipp文件完成。

_acceptorIOContext这个io上下文结构用于处理accept相关请求，该上下文在mongodb服务层调用asio方式为_acceptorIOContext->run()和_acceptorIOContext->stop();;
   _acceptorIOContext->run();这两个接口的实现如下：
io_context::count_type io_context::run()
{
  asio::error_code ec;
  //scheduler::run调度处理
  count_type s = impl_.run(ec);
  asio::detail::throw_error(ec);
  return s;
}

_acceptorIOContext->stop()具体实现如下:
//io调度结束
void io_context::stop()
{
  //scheduler::stop
  impl_.stop();
}

_workerIOContext用于处理accept返回的新链接fd上面的数据读写服务，同时处理网络状态机的调度。asio实际上在实现的时候，把各种读写操作及其数据收发后的回调组装成各种不同的operation handler放入队列，然后由mongodb中创建的各种
工作线程来从队列中取出opration handler执行，见后面opration及调度相关实现。和_workerIOContext关联的几个主要opration出队相关操作接口如下：
std::size_t io_context::run_for(
    const chrono::duration<Rep, Period>& rel_time)
{
  //直接调用io_context::run_until，时间转换为当前时间之后rel_time这个时间点
  return this->run_until(chrono::steady_clock::now() + rel_time);
}


std::size_t io_context::run_until(
    const chrono::time_point<Clock, Duration>& abs_time)
{
  std::size_t n = 0;
  //注意这里是循环掉用run_one_until接口
  while (this->run_one_until(abs_time))
    if (n != (std::numeric_limits<std::size_t>::max)())
      ++n;
  return n;
}

std::size_t io_context::run_one_for(
    const chrono::duration<Rep, Period>& rel_time)
{ 
  //只调用一次
  return this->run_one_until(chrono::steady_clock::now() + rel_time);
}


std::size_t io_context::run_one_until(
    const chrono::time_point<Clock, Duration>& abs_time) //abs_time表示运行该函数到期的绝对事件点
{
  typename Clock::time_point now = Clock::now();
  while (now < abs_time)
  {
    //在本循环中还需要运行多久
    typename Clock::duration rel_time = abs_time - now;
	//最多一次循环执行1s，也就是schedule::wait_one最多单次wait 1s
    if (rel_time > chrono::seconds(1))
      rel_time = chrono::seconds(1);

    asio::error_code ec;
	//schedule::wait_one 从操作队列中获取对应的operation执行，如果获取到operation并执行成功则返回1，否则返回0
    std::size_t s = impl_.wait_one(
        static_cast<long>(chrono::duration_cast<
          chrono::microseconds>(rel_time).count()), ec);
    asio::detail::throw_error(ec);

	//已经获取执行完一个operation，则直接返回，否则继续循环调度schedule::wait_one获取operation执行
    if (s || impl_.stopped())
      return s;

	//重新计算当前时间
    now = Clock::now();
  }

  return 0;
}
	从上面的代码分析可以看出io_context::run_one_for()和io_context::run_for()两个接口最终都掉调用schedule类的wait_one接口。
schedule::wait_one接口的实现细节请参考下一节的schedule调度设计与实现
    io_context::run_one_for()和io_context::run_for()区别主要是run_one_for只调用run_one_until一次，而run_for会持续循环调用run_one_until处理operation操作。


和_workerIOContext关联的几个主要入队相关操作接口如下：
io_context::post(ASIO_MOVE_ARG(CompletionHandler) handler)
{
  //构造async_completion类，handler回调初始化到该类相应成员
  async_completion<CompletionHandler, void ()> init(handler);
  
  //mongodb  asio::async_read(), asio::async_write(), asio::async_connect(), 默认返回true。其他handler返回false
  bool is_continuation =   
    asio_handler_cont_helpers::is_continuation(init.completion_handler);

  // Allocate and construct an operation to wrap the handler.
  //构造complete handler,completion_handler继承基础operation 
  typedef detail::completion_handler<
    typename handler_type<CompletionHandler, void ()>::type> op;
  typename op::ptr p = { detail::addressof(init.completion_handler),
      op::ptr::allocate(init.completion_handler), 0 };
  p.p = new (p.v) op(init.completion_handler);

  //调用scheduler::post_immediate_completion，completion_handler传递给该函数
  impl_.post_immediate_completion(p.p, is_continuation);
  
}
另一个和mongodb入队相关的接口如下：
io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
{
  async_completion<CompletionHandler, void ()> init(handler);

  //scheduler::can_dispatch  本线程已经加入到了工作线程队列，则本handler直接由本线程运行
  if (impl_.can_dispatch())
  {
    detail::fenced_block b(detail::fenced_block::full);
	//直接运行handler
    asio_handler_invoke_helpers::invoke(
        init.completion_handler, init.completion_handler); 
  }
  else
  {
    // Allocate and construct an operation to wrap the handler.
    //构造complete handler,completion_handler继承基础operation 
    typedef detail::completion_handler<
      typename handler_type<CompletionHandler, void ()>::type> op;
    typename op::ptr p = { detail::addressof(init.completion_handler),
      op::ptr::allocate(init.completion_handler), 0 };
    p.p = new (p.v) op(init.completion_handler); //获取op操作，也就是handler回调
    ......
	
	//mongodb的ServiceExecutorAdaptive::schedule调用->io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
	//->scheduler::do_dispatch
	//scheduler::do_dispatch 
    impl_.do_dispatch(p.p); 
    p.v = p.p = 0;
  }
  ......
}


	从上面的分析可以看出，和mongodb直接相关的几个接口最终都是调用schedule类的相关接口，整个实现过程参考下一节scheduler调度实现模块。
总结： 
1. asio::io_context类中和mongodb直接相关的几个接口主要是run_for、run、run_one_for、poll、dispatch
2. 这些接口按照功能不同，可以分为入队型接口(poll、dispatch)和出队型接口(run_for、run、run_one_for)
3. 按照和io_context的关联性不同，可以分为accept相关io(_acceptorIOContext)处理的接口(run、stop)和新链接fd对应Io(_workerIOContext)数据收发相关处理及回调的接口(run_for、run_one_for、poll、dispatch)
4. io_context上下文的上述接口，除了dispatch在某些情况下直接运行handler外，其他接口最终都会间接调用scheduler调度类接口

scheduler多线程并发调度实现过程
    上一节的io_context上下文中提到mongodb操作的io上下文最终都会调用scheduler的几个核心接口。scheduler类主要工作在于完成任务调度，该类和mongodb相关的几个主要成员变量及接口如下：
class scheduler {
public:
  //reactor_op  completion_handler继承该类    
  //mongodb中operation分为两种，一种是completion_handler，另一种是reactor_op
  typedef scheduler_operation operation;
  
  //listen线程做accept异步处理的循环调度过程
  std::size_t scheduler::run(asio::error_code& ec)
  
  //全局任务入队相关的接口
  void scheduler::do_dispatch(scheduler::operation* op)
  
  //任务出队执行的，如果队列为空则获取epoll事件集对应的网络IO任务放入全局op_queue_队列
  std::size_t scheduler::wait_one(long usec, asio::error_code& ec)
  std::size_t scheduler::do_wait_one(mutex::scoped_lock& lock,
    scheduler::thread_info& this_thread, long usec, const asio::error_code& ec)
	
  //任务调度启停相关接口
  void scheduler::restart()
  void scheduler::stop_all_threads(mutex::scoped_lock& lock)
  
private:
  //全局锁，多线程互斥，对全局op_queue_队列做互斥
  mutable mutex mutex_;
  //全局任务队列，全局任务和网络事件相关任务都添加到该队列
  op_queue<operation> op_queue_;
  
  
  struct task_operation : operation //特殊operation
  {
	//这个特殊op的作用是保证从epoll_wait返回值中获取到一批网络事件对应的IO回调op接入全局队列op_queue_后，都加上该特殊op到队列中
	//下次如果从队列头部获取到该特殊Op操作，会继续获取epoll网络事件任务，避免网络IO长时间不被处理引起的"饥饿"状态
    task_operation() : operation(0) {}
  } task_operation_; 
  
  //stop_all_threads中置为true, 为true后，将不再处理epoll相关事件，参考scheduler::do_run_one
  bool stopped_;  
  
  //唤醒等待锁得线程(实际event由信号量封装)
  event wakeup_event_; 
  
  //也就是epoll_reactor，借助epoll实现网络事件异步处理
  reactor* task_; //epoll_reactor
  
  //套接字描述符个数：accept获取到的链接数fd个数+1(定时器fd)
  atomic_count outstanding_work_;
}


ASIO调度的任务(注: 每个任务对应一个operation类，operation类实现讲在后面章节详述)包括以下两种：1. 全局task任务  2. 网络IO事件处理相关任务(新链接、数据读取及其对应回调)，所有的任务通过一个全局队列链接在一起，并有序的调度执行。
1. 任务入队流程
    所有的任务都通过scheduler.op_queue_统一管理，和op_queue_全局队列任务入队相关的函数接口如下：
//全局状态任务入队
void scheduler::do_dispatch(
    scheduler::operation* op)
{
  work_started();
  //默认多线程，所以这里需要对队列加锁
  mutex::scoped_lock lock(mutex_);
  op_queue_.push(op);
  wake_one_thread_and_unlock(lock);
}

除了全局状态任务外，Asio还需要处理网络IO相关的任务，网络IO相关的任务入队相关接口如下：
struct scheduler::task_cleanup
{
  //scheduler::do_wait_one中获取到的epoll网络IO任务先入队到线程私有队列，在task_cleanup析构函数中一次性入队到全局队列
  ////将本线程的私有队列放入全局队列中，然后用task_operation_来标记一个线程私有队列的结束。
  //每次执行epoll_reactor::run去获取epoll对应的事件列表的时候都会执行该cleanup
  ~task_cleanup()
  {
    if (this_thread_->private_outstanding_work > 0)
    { //本线程上的私有任务后面会全部入队到全局op_queue_队列，全局任务数增加
      asio::detail::increment( 
          scheduler_->outstanding_work_,
          this_thread_->private_outstanding_work);
    }
    this_thread_->private_outstanding_work = 0;

    // Enqueue the completed operations and reinsert the task at the end of
    // the operation queue.
    lock_->lock();
	//将本线程的私有队列放入全局队列中，然后用task_operation_来标记一个线程私有队列的结束。
	//task_operation_标记一个线程私有队列的结束。
    scheduler_->task_interrupted_ = true;
    scheduler_->op_queue_.push(this_thread_->private_op_queue);

	//注意这里，加了一个特殊的op task_operation_到全局队列
    scheduler_->op_queue_.push(&scheduler_->task_operation_);
  }

  scheduler* scheduler_;
  //本作用域的锁
  mutex::scoped_lock* lock_;
  //线程私有信息
  thread_info* this_thread_;
};
    从上面的代码分析可以看出，当释放task_cleanup资源的时候，会把本线程私有private_op_queue队列上的所有网络IO相关任务一次性入队到全局op_queue_队列。

	
2. 任务出队执行
    空闲worker线程从op_queue_全局队列中获取任务执行，任务从全局队列出队流程相关接口如下:
std::size_t scheduler::wait_one(long usec, asio::error_code& ec)
{
  ec = asio::error_code();
  if (outstanding_work_ == 0) //如果连工作线程都没有，则说明没有调度的意义，停止所有调度
  {
    stop();
    return 0;
  }

  thread_info this_thread;
  this_thread.private_outstanding_work = 0;
  //线程入队到call_stack.top_链表
  thread_call_stack::context ctx(this, this_thread);
  //上锁
  mutex::scoped_lock lock(mutex_);

  //从操作队列中获取对应的operation执行，如果获取到operation并执行成功则返回1，否则返回0
  return do_wait_one(lock, this_thread, usec, ec);
}
    接口scheduler::wait_one最终调用scheduler::do_wait_one接口，该接口实现如下：
std::size_t scheduler::do_wait_one(...) //获取全局队列首的一个op执行
{
  //stop_all_threads中置为true, 为true后，将不再处理epoll相关事件，参考scheduler::do_run_one
  if (stopped_)
    return 0;
  //去队列头部任务
  operation* o = op_queue_.front();
  if (o == 0) //如果队列为空，则等待usec
  {
    //等待被唤醒
    wakeup_event_.clear(lock);
    wakeup_event_.wait_for_usec(lock, usec);
    usec = 0; // Wait at most once.
    //等一会儿后我们继续判断队列中是否有可执行的op
    o = op_queue_.front();
  }
  
  //该op是一个特殊的op, 说明需求去获取epoll上面的网络时间任务来处理，避免网络IO长期不被处理而处于饥饿状态
  if (o == &task_operation_) 
  {
    op_queue_.pop();
	//队列上还有其他等待执行的任务
    bool more_handlers = (!op_queue_.empty());

    task_interrupted_ = more_handlers;
    //既然队列上还有任务需要被调度执行，则通知其他线程可以继续获取队列任务执行了，这样可以提高并发
    if (more_handlers && !one_thread_)//默认mongodb多线程
      wakeup_event_.unlock_and_signal_one(lock); //唤醒其他工作线程
    else
      lock.unlock();

    {
	  //该类析构函数中会把下面从epoll获取的网络事件任务入队到全局scheduler.op_queue_队列
      task_cleanup on_exit = { this, &lock, &this_thread };
	  //函数exit的时候执行前面的on_exit析构函数从而把this_thread.private_op_queue入队到scheduler.op_queue_
      (void)on_exit;
      //scheduler::do_run_one->epoll_reactor::run
      //通过epoll获取所有得网络事件op入队到private_op_queue, 最终再通过scheduler::poll_one scheduler::poll入队到op_queue_
      //this_thread.private_op_queue队列成员的op类型为descriptor_state
	  task_->run(more_handlers ? 0 : usec, this_thread.private_op_queue);
    }
  }
  
  //没有任务可执行，直接返回
  if (o == 0)
    return 0;

  //先把该op任务出队
  op_queue_.pop();
  
  ......
  
  //执行取出的这个任务
  o->complete(this, ec, task_result);

  return 1;  
}

3. 任务队列
    从上面的分析可以看出，asio管理任务调度的队列包含两种：1. 全局任务队列op_queue_ 2. 工作线程私有队列private_op_queue
全局任务队列op_queue_结构参考前面的scheduler类，工作线程私有队列存放于scheduler_thread_info私有结构中，结构成员如下：
struct scheduler_thread_info : public thread_info_base
{
  //scheduler::do_wait_one->epoll_reactor::run中通过epoll获取对应网络事件任务
  //工作线程私有任务队列
  op_queue<scheduler_operation> private_op_queue; 
  //本线程私有private_op_queue队列中op任务数
  long private_outstanding_work;
};
在scheduler::do_wait_one接口中，工作线程遍历全局任务队列，如果队列头部的任务类型为task_operation特殊任务，则会调用epoll_reactor::run进行epoll
调度，通过epoll_wait获取到所有的网络IO事件任务，同时全部一次性入队到本线程的私有队列this_thread.private_op_queue。最后在task_cleanup析构函数中再一次性
从线程私有队列入队到全局队列，这样做的目的可以保证锁的粒度最小，同时性能最好，这也是设计一个线程私有队列的原因。

operation
    从前面的分析可以看出，一个任务对应一个operation类结构，asio中schduler调度的任务分为全局状态机任务和网络IO处理回调任务，这两种任务都有对应的回调函数。
此外，asio还有一种特殊的operation，该Operastion什么也不做，只是一个特殊标记。全局状态机任务、网络IO处理任务、特殊任务这三类任务分别对应、
reactor_op、task_operation_三个类实现，这三个类都会继承operation。
1. operation基类
    operation基类实际上就是scheduler_operation类，通过typedef scheduler_operation operation指定，是其他三个任务的父类，其主要实现接口如下：
class scheduler_operation  
{
public:
  typedef scheduler_operation operation_type;

  //执行func回调
  void complete(void* owner, const asio::error_code& ec,
      std::size_t bytes_transferred)
  {
    func_(owner, this, ec, bytes_transferred);
  }

protected:
  //初始化
  scheduler_operation(func_type func)
    : next_(0),
      func_(func),
      task_result_(0)
  {}

private:
  //reactor_op类(对应网络事件处理任务):epoll_reactor::descriptor_state::do_complete
  //completion_handler类(对应全局状态机任务):对应completion_handler::do_complete
  func_type func_;
protected:
  //本opration对应的scheduler
  friend class scheduler;
  //获取epoll_wait返回的event信息，赋值见set_ready_events add_ready_events
  //所有的网络事件通过task_result_位图记录，生效见epoll_reactor::descriptor_state::do_complete
  unsigned int task_result_; // Passed into bytes transferred.
};

//统一用scheduler_operation类型定义opration，外部使用Opration都用scheduler_operation统一代替
typedef scheduler_operation operation;


2. 全局状态机任务(completion_handler)
     当mongodb通过listener线程接受到一个新链接后，会生成一个状态机调度任务，然后入队到全局队列op_queue_，worker线程从全局队列获取到该任务后调度执行，从而
进入状态机调度流程，在该流程中会触发epoll相关得网络IO注册及异步IO处理。一个全局状态机任务对应一个completion_handler类，该类实现如下：
class completion_handler : public operation  
{
public:
  ASIO_DEFINE_HANDLER_PTR(completion_handler);
  //初始化
  completion_handler(Handler& h)
    //completion_handler对应的func
    : operation(&completion_handler::do_complete),  
	  //handler_赋值
      handler_(ASIO_MOVE_CAST(Handler)(h)) 
  {
    handler_work<Handler>::start(handler_); //标记任务开始执行
  }

  //真正执行在scheduler::do_wait_one->operation::complete->completion_handler::do_complete
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
      ......
      //获取对应handler执行
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN(());
      w.complete(handler, handler);
      ASIO_HANDLER_INVOCATION_END;
  }

private:
  //任务对应的回调函数
  Handler handler_;
}
    全局任务入队的时候有两种方式，一种是io_context::dispatch方式，另一种是io_context::post。从前面章节对这两个接口的代码分析可以看出，任务直接入队到全局队列
op_queue_中，然后工作线程通过scheduler::do_wait_one从队列获取该任务执行。
注意：任务入队由Listener线程完成，任务出队调度执行由mongodb工作线程执行。
	
3. 网络IO事件处理任务
    网络IO事件对应的Opration任务最终由reactor_op类实现，该类主要成员及接口如下：
class reactor_op   
  : public operation //也就是scheduler_operation
{
public:
  // The error code to be passed to the completion handler.
  asio::error_code ec_;

  // The number of bytes transferred, to be passed to the completion handler.
  //发送或者接收fd的数据量
  std::size_t bytes_transferred_;

  //发送或者接收fd数据的状态
  enum status { not_done, done, done_and_exhausted };

  // Perform the operation. Returns true if it is finished.
  status perform() //执行perform_func_
  { 
    //执行func，网络IO对应的func见epoll_reactor::descriptor_state::perform_io中调度执行
    return perform_func_(this);
  }

protected:
  typedef status (*perform_func_type)(reactor_op*);
  //例如accept操作过程回调过程，一个执行accept操作，一个执行后续的complete_func操作(mongodb中的task)
  //例如recvmsg操作过程，一个执行reactive_socket_recv_op_base::do_perform(最终recvmsg)，一个执行后续complete_func操作(mongodb中的task)
  reactor_op(perform_func_type perform_func, func_type complete_func)
  //例如接受数据的complete_func为reactive_socket_recv_op::do_complete
    : operation(complete_func),  //complete_func赋值给operation，在operation中执行
      bytes_transferred_(0),
      perform_func_(perform_func)
  {
  }

private:
  //perform_func也就是fd数据收发底层实现底层实现，赋值给reactor_op.perform_func_, 
  //complete_func赋值给父类operation的func,见reactor_op构造函数
  perform_func_type perform_func_;
};	
    从reactor_op类可以看出，该类的主要两个函数成员：perform_func_和complete_func。其中perform_func_函数主要负责异步网络IO底层处理，complete_func用于数据接收或者发送后的后续处理逻辑。
perform_func_具体功能如下：
1. 通过epoll事件集处理底层accept获取新连接fd。
2. fd上的数据异步接收
3. fd上的数据异步发送
	针对上面的三个网络IO处理功能，ASIO在实现的时候，分别通过三个不同的继承类(reactive_socket_accept_op_base、reactive_socket_recv_op_base、
reactive_socket_send_op_base)实现。
1. accept异步处理实现
    accept的异步接收过程，perform_func_底层IO实现主要通过reactive_socket_accept_op_base实现，具体实现方式如下：

//reactive_socket_service::async_accept中注册
class reactive_socket_accept_op_base : public reactor_op
{
public:
  //reactive_socket_accept_op中构造赋值
  reactive_socket_accept_op_base(socket_type socket,
      socket_ops::state_type state, Socket& peer, const Protocol& protocol,
      typename Protocol::endpoint* peer_endpoint, func_type complete_func)
      //初始化accept的底层IO处理接口及接收到新链接的后续处理接口
    : reactor_op(&reactive_socket_accept_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      peer_(peer),
      protocol_(protocol),
      peer_endpoint_(peer_endpoint),
      addrlen_(peer_endpoint ? peer_endpoint->capacity() : 0)
  {
  }

  //这里是底层的accept接收处理
  static status do_perform(reactor_op* base)
  {
    ......
    status result = socket_ops::non_blocking_accept(o->socket_,
        o->state_, o->peer_endpoint_ ? o->peer_endpoint_->data() : 0,
        o->peer_endpoint_ ? &o->addrlen_ : 0, o->ec_, new_socket)
    ? done : not_done;
    ......
	
    return result;
  }

  //在外层继承类reactive_socket_accept_op中的do_complete中执行
  //把接收到的新链接new_socket_注册到epoll事件集中，这样就可以异步收发该fd的数据
  void do_assign()  
  {
       ......
	  //reactive_socket_service_base::do_assign,这里面把new_socket_注册到epoll事件集
      peer_.assign(protocol_, new_socket_.get(), ec_);
	  ......
  }

private:
  //sock套接字sd
  socket_type socket_; 
  //accept获取到的新链接fd，见reactive_socket_accept_op_base::do_perform
  socket_holder new_socket_;
  //客户端地址信息记录在这里面
  Socket& peer_; 
  typename Protocol::endpoint* peer_endpoint_;
};
    complete_func用于处理accept获取到新链接new_socket_后的后续处理，后续处理通过reactive_socket_accept_op继承类实现，如下：
class reactive_socket_accept_op :
  public reactive_socket_accept_op_base<Socket, Protocol>
{
public:
  //初始化构造
  reactive_socket_accept_op(socket_type socket,
      socket_ops::state_type state, Socket& peer, const Protocol& protocol,
      typename Protocol::endpoint* peer_endpoint, Handler& handler)
    : reactive_socket_accept_op_base<Socket, Protocol>(socket, state, peer,
        protocol, peer_endpoint, &reactive_socket_accept_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }

  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
    ......
	//执行前面的新连接epoll事件集注册reactive_socket_accept_op_base::do_assign
    if (owner)
	//reactive_socket_accept_op::do_assign
      o->do_assign();

    .......
	//执行accept后回调，也就是mongodb中的ServiceEntryPointImpl::startSession
    w.complete(handler, handler.handler_);
    ASIO_HANDLER_INVOCATION_END;
	......
  }

private:
  //接收到新链接的回调，mongodb中对应ServiceEntryPointImpl::startSession
  Handler handler_;
};
    complete_func函数接口主要事accept新链接后的处理，包括新连接new_sock注册到epoll事件集，这样该新链接的数据收发就可以异步实现。此外，
还会执行mongodb服务层的handler回调，也就是ServiceEntryPointImpl::startSession。

2. 网络数据接收异步处理实现
    网络数据异步接收过程，perform_func_底层IO实现主要通过reactive_socket_recv_op_base实现，具体实现方式如下：    
class reactive_socket_recv_op_base : public reactor_op
{
public:
  //初始化构造
  reactive_socket_recv_op_base(socket_type socket,
      socket_ops::state_type state, const MutableBufferSequence& buffers,
      socket_base::message_flags flags, func_type complete_func)
	//网络数据接收及其后续处理
    : reactor_op(&reactive_socket_recv_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      buffers_(buffers),
      flags_(flags)
  {
  }

  static status do_perform(reactor_op* base)
  {
    reactive_socket_recv_op_base* o(
        static_cast<reactive_socket_recv_op_base*>(base));
	......
	//异步接收数据存入bufs中
    status result = socket_ops::non_blocking_recv(o->socket_,
        bufs.buffers(), bufs.count(), o->flags_,
        (o->state_ & socket_ops::stream_oriented) != 0,
        o->ec_, o->bytes_transferred_) ? done : not_done;
	......
	
    return result;
  }

private:
  //套接字
  socket_type socket_;
  //存储读取的数据的buf,里面需要读取数据的长度
  MutableBufferSequence buffers_;
};
    接收到数据后的handler回调在reactive_socket_recv_op类中实现，该类继承reactive_socket_recv_op_base，其具体实现如下:
class reactive_socket_recv_op :
  public reactive_socket_recv_op_base<MutableBufferSequence>
{
public:
  //初始化构造
  reactive_socket_recv_op(socket_type socket,
      socket_ops::state_type state, const MutableBufferSequence& buffers,
      socket_base::message_flags flags, Handler& handler)
	//handler及do_complete赋值
    : reactive_socket_recv_op_base<MutableBufferSequence>(socket, state,
        buffers, flags, &reactive_socket_recv_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }

  //读取到一个完整的mongo数据后，回调在这里执行，实际上是工作线程从队列获取op执行的，见
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
	  ......
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN((handler.arg1_, handler.arg2_));
	  //执行对应handler
      w.complete(handler, handler.handler_);
      ASIO_HANDLER_INVOCATION_END;
	  ......
  }

private:
  //mongodb接收数据的handler对应TransportLayerASIO::ASIOSourceTicket::_headerCallback(接收到mongo协议头部对应的handler)、
  //TransportLayerASIO::ASIOSourceTicket::_bodyCallback(接收到包体部分的handler)
  Handler handler_;
};
3. 网络数据异步发送处理实现
	网络数据异步发送过程和接收过程类似，perform_func_底层IO数据发送实现主要通过reactive_socket_send_op_base实现，具体实现方式如下：   
class reactive_socket_send_op_base : public reactor_op
{
public:
  //初始化构造
  reactive_socket_send_op_base(socket_type socket,
      socket_ops::state_type state, const ConstBufferSequence& buffers,
      socket_base::message_flags flags, func_type complete_func)
	//初始化do_perform和complete_func
    : reactor_op(&reactive_socket_send_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      buffers_(buffers),
      flags_(flags)
  {
  }
  
  //数据异步发送的底层实现
  static status do_perform(reactor_op* base)
  {
	......
    status result = socket_ops::non_blocking_send(o->socket_,
          bufs.buffers(), bufs.count(), o->flags_,
          o->ec_, o->bytes_transferred_) ? done : not_done;
	......

    return result;
  }

private:
  //fd
  socket_type socket_;
  //发送的数据在该buffer中
  ConstBufferSequence buffers_;
}
    发送完buffers_数据后的handler回调在reactive_socket_send_op类中实现，该类继承reactive_socket_send_op_base，其具体实现如下:
class reactive_socket_send_op :
  public reactive_socket_send_op_base<ConstBufferSequence>
{
public:
  //初始化构造
  reactive_socket_send_op(socket_type socket,
      socket_ops::state_type state, const ConstBufferSequence& buffers,
      socket_base::message_flags flags, Handler& handler)
    : reactive_socket_send_op_base<ConstBufferSequence>(socket,
        state, buffers, flags, &reactive_socket_send_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }
  
  //数据发送完成后的handler回调处理
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
	  ......
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN((handler.arg1_, handler.arg2_));
      w.complete(handler, handler.handler_);
      ASIO_HANDLER_INVOCATION_END;
      ......
  }

private:
  //mongodb异步数据发送成功后的handler回调对应TransportLayerASIO::ASIOSinkTicket::_sinkCallback
  Handler handler_;
};
4. 总结
    从上面的网络IO任务的实现可以看出，整个实现过程都是大同小异的，都有底层fd的处理及其对应的handler回调处理，整个逻辑比较清晰。

epoll_reactor模式底层实现
    mongodb默认使用epoll方式来实现异步网络IO事件处理，epoll网上资料比较多，可以参考：https://en.wikipedia.org/wiki/Epoll。
本文只分析和ASIO实现密切关联的一些接口。
1. epoll_reactor类实现
class epoll_reactor 
  : public execution_context_service_base<epoll_reactor>
{
public:
  //任务操作类型
  enum op_types {  
  	read_op = 0,    //对应读事件
    write_op = 1,   //对应写事件
    connect_op = 1, //对应connect事件
    except_op = 2,  //异常事件
    max_ops = 3     //类型最大值
  };

  //新链接new_sock对应的私有描述符信息，用于
  class descriptor_state : operation  
  {
    //并发锁
    mutex mutex_;
	//本descriptor_state所属的epoll reactor
    epoll_reactor* reactor_;
	//新链接对应的套接字描述符
    int descriptor_;  
	//epoll_wait获取到的事件集位图
    uint32_t registered_events_;
	
	//套接字描述符对应的读、写、accept及异常事件对应的Opration任务分别入队到各自的数组队列中
	//op_queue_[I],i也就是对应上面的op_types
    op_queue<reactor_op> op_queue_[max_ops]; 
    bool try_speculative_[max_ops];
	//epoll_reactor::deregister_descriptor置为true，套接字描述符注册掉
    bool shutdown_;

	//descriptor_state初始化构造
	ASIO_DECL descriptor_state(bool locking);
	//epoll事件集对应的位图信息，每个位置1表示对应网络事件发生
	//这些位图上的事件处理在epoll_reactor::descriptor_state::do_complete
    void set_ready_events(uint32_t events) { task_result_ = events; }
    void add_ready_events(uint32_t events) { task_result_ |= events; }

	//对应accept、读、写事件的底层Io处理，实现见reactive_socket_accept_op_base	
	//reactive_socket_recv_op_base reactive_socket_send_op_base
	ASIO_DECL operation* perform_io(uint32_t events);
	//descriptor_state对应的do_complete为epoll_reactor::descriptor_state::do_complete
    ASIO_DECL static void do_complete(
        void* owner, operation* base,
        const asio::error_code& ec, std::size_t bytes_transferred);
  }
  
  //给descriptor_state起一个别名per_descriptor_data
  typedef descriptor_state* per_descriptor_data;
  //构造epoll reactor
  ASIO_DECL epoll_reactor(asio::execution_context& ctx);
  //新链接描述符fd关注的所有读、写、异常等事件注册到epoll事件集中，当对应事件到达通过epoll_wait返回回去即可
  ASIO_DECL int register_descriptor(socket_type descriptor,
      per_descriptor_data& descriptor_data);
  //把读、写操作的回调注册到描述符对应私有信息队列descriptor_data.op_queue_[]中
  void epoll_reactor::start_op(int op_type, socket_type descriptor,
  epoll_reactor::per_descriptor_data& descriptor_data, reactor_op* op,
  bool is_continuation, bool allow_speculative)
    
  //epoll_reactr对应的scheduler_
  scheduler& scheduler_;
  //事件私有信息保护锁
  mutex mutex_;
  //timer fd，定时器对应的fd，本章不做定时器实现描述
  int timer_fd_;
  //timer队列集
  timer_queue_set timer_queues_;
  
  //所有链接描述符私有信息都存入该poll池中，用于资源统一的分配和释放
  object_pool<descriptor_state> registered_descriptors_;
  //registered_descriptors_并发锁控制
  mutex registered_descriptors_mutex_;
}
   从上面对epoll_reactor类的分析，可以看出，所有链接通过descriptor_state管理私有结构，该结构
负责链接上所有的底层epoll事件收集与处理。descriptor_state通过一个op_queue_数组队列来把不同reactor类型的操作记录
到各自不同的队列中，这样可以更好的管理。
   
2. 主要接口成员实现
   epoll reactor类接口实现在epoll_reactor.ipp文件中实现，主要接口包括：初始化、描述符注册、各自事件回调注册、描述符注销、
epoll调度执行、底层IO处理、IO处理后的回调实现等
2.1 epoll相关实现
epoll_reactor::epoll_reactor(asio::execution_context& ctx) 
{
   //初始化构造，代码比较简单
}  

//把链接描述符关注的几个常用事件注册到epoll
int epoll_reactor::register_descriptor(socket_type descriptor,
    epoll_reactor::per_descriptor_data& descriptor_data) //套接字 回调等相关信息
{
  //获取一个描述符descriptor_state信息，分配对应空间
  descriptor_data = allocate_descriptor_state();

  ASIO_HANDLER_REACTOR_REGISTRATION((
        context(), static_cast<uintmax_t>(descriptor),
        reinterpret_cast<uintmax_t>(descriptor_data)));

  {
    mutex::scoped_lock descriptor_lock(descriptor_data->mutex_);

	//下面对descriptor_data进行相应的赋值
    descriptor_data->reactor_ = this;
    descriptor_data->descriptor_ = descriptor;
    descriptor_data->shutdown_ = false;
    for (int i = 0; i < max_ops; ++i)
      descriptor_data->try_speculative_[i] = true;
  }

  epoll_event ev = { 0, { 0 } };
  //同时把这些事件添加到epoll事件集，表示关注这些事件，注意这里是边沿触发
  ev.events = EPOLLIN | EPOLLERR | EPOLLHUP | EPOLLPRI | EPOLLET; 
  descriptor_data->registered_events_ = ev.events; 
  //赋值记录到ev.data.ptr中，当对应网络事件到底执行回调的时候可以通过该指针获取descriptor_data
  //descriptor_data记录到该指针，epoll_reactor::run中通过对应事件获取该私有信息
  ev.data.ptr = descriptor_data; 
  //通过epoll_ctl把events添加到事件集，当对应事件发生，epoll_wait可以获取对应事件
  int result = epoll_ctl(epoll_fd_, EPOLL_CTL_ADD, descriptor, &ev);
  ......

  return 0;
}

//1. 对描述符关注的epoll事件做跟新
//2. 把reactor_op对应的网络IO相关opration任务入队到descriptor_data对应的私有队列信息
void epoll_reactor::start_op(int op_type, socket_type descriptor,
    epoll_reactor::per_descriptor_data& descriptor_data, reactor_op* op,
    bool is_continuation, bool allow_speculative)
{
  ......
  
  //如果reactor_op还没有加入对应的op_queue_[i]队列，则EPOLL_CTL_MOD跟新epoll对应事件信息
  if (descriptor_data->op_queue_[op_type].empty())
  {
    ......
    {
	  //如果是写类型，添加EPOLLOUT进去
      if (op_type == write_op)
      {
        descriptor_data->registered_events_ |= EPOLLOUT;
      }
 
      epoll_event ev = { 0, { 0 } };
      ev.events = descriptor_data->registered_events_;
      ev.data.ptr = descriptor_data;
	  //再次跟新一下事件
      epoll_ctl(epoll_fd_, EPOLL_CTL_MOD, descriptor, &ev);
    }
  }
  //把任务回调opration入队到descriptor_data的对应队列
  descriptor_data->op_queue_[op_type].push(op);
  //scheduler::work_started  实际上就是链接数
  scheduler_.work_started();
}

//epoll异步IO事件处理流程
void epoll_reactor::run(long usec, op_queue<operation>& ops) //ops队列内容为descriptor_state
{
  ......
  epoll_event events[128];
  //epoll_wait获取到IO事件后返回，或者超时事件内没有对应网络IO事件，也返回
  int num_events = epoll_wait(epoll_fd_, events, 128, timeout);
  ......
  //遍历获取对应的事件信息
  for (int i = 0; i < num_events; ++i)
  {
      void* ptr = events[i].data.ptr;

	  //等待完成之后，我们开始分发事件:

      unsigned event_mask = 0;
	  //accept事件、网络数据到达事件
      if ((events[i].events & EPOLLIN) != 0)
        event_mask |= ASIO_HANDLER_REACTOR_READ_EVENT;
	  //写事件
      if ((events[i].events & EPOLLOUT))
        event_mask |= ASIO_HANDLER_REACTOR_WRITE_EVENT;
	  //异常事件
      if ((events[i].events & (EPOLLERR | EPOLLHUP)) != 0)
        event_mask |= ASIO_HANDLER_REACTOR_ERROR_EVENT;
      ASIO_HANDLER_REACTOR_EVENTS((context(),
            reinterpret_cast<uintmax_t>(ptr), event_mask));

  }

  // Dispatch the waiting events.
  //IO事件类型有三种:interrupt,timer和普通的IO事件
  for (int i = 0; i < num_events; ++i)
  {
      //该事件对应的私有信息指针，通过该指针就可以获取到对应的descriptor_data，该结构记录了对应的读写react_op
      void* ptr = events[i].data.ptr;

      //通过ptr获取对应descriptor_data信息
      descriptor_state* descriptor_data = static_cast<descriptor_state*>(ptr);
      if (!ops.is_enqueued(descriptor_data))  //不在队列中，则添加
      {
        //对应事件位图置位
        descriptor_data->set_ready_events(events[i].events);
		//epoll operation对应的回调函数是epoll_reactor::descriptor_state::do_complete
        ops.push(descriptor_data); 
      }
      else  //descriptor_data已经在ops的队列中了，对应事件位图置位
      {
        descriptor_data->add_ready_events(events[i].events);
      }
    }
  }
  ......
}

2.2 react_op对应网络IO事件处理流程
    前面章节已经知道，ASIO的网络IO处理及其回调都是通过reactor_op的几个基础类中赋值，其真正的调度执行通过
以下几个接口实现：
//初始化一个描述符信息descriptor_state
epoll_reactor::descriptor_state::descriptor_state(bool locking)
  //这里可以看出opration对应的func为descriptor_state::do_complete
  : operation(&epoll_reactor::descriptor_state::do_complete),
    mutex_(locking)
{
}
当scheduler::do_wait_one中获取operation执行complete的时候，最终会调用该descriptor_state::do_complete执行，该接口实现如下：
void epoll_reactor::descriptor_state::do_complete(
    void* owner, operation* base,
    const asio::error_code& ec, std::size_t bytes_transferred)
{
  if (owner)
  {
    descriptor_state* descriptor_data = static_cast<descriptor_state*>(base);
	//事件位图
    uint32_t events = static_cast<uint32_t>(bytes_transferred); 
	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
    if (operation* op = descriptor_data->perform_io(events)) 
    {
      //执行complete_func, 也就是reactive_socket_accept_op_base  reactive_socket_recv_op_base reactive_socket_send_op_base  
      //这三个IO操作对应的complete_func回调

	  //注意这里只执行了网络IO事件任务中的一个，其他的在perform_io中入队到全局队列中了，等待其他线程执行
      op->complete(owner, ec, 0);
    }
  }
}
    descriptor_state::do_complete最终会调用descriptor_state::perform_io执行，perform_io实现如下：
//网络IO相关的任务处理，如accept接收新链接、接收数据、发送数据
operation* epoll_reactor::descriptor_state::perform_io(uint32_t events)
{
  //上锁
  mutex_.lock();
  //这里构造一个perform_io_cleanup_on_block_exit类，用于后续析构时候的收尾处理
  perform_io_cleanup_on_block_exit io_cleanup(reactor_);
  mutex::scoped_lock descriptor_lock(mutex_, mutex::scoped_lock::adopt_lock);


  //分别对应新链接到达或者数据来临、可以写数据、有紧急的数据可读(这里应该表示有带外数据到来)
  static const int flag[max_ops] = { EPOLLIN, EPOLLOUT, EPOLLPRI };
  //循环处理如下三种类型的reactor_op:
  // 1. reactive_socket_accept_op_base
  // 2. reactive_socket_recv_op_base
  // 3. reactive_socket_send_op_base    
  for (int j = max_ops - 1; j >= 0; --j)
  {
    //有读写事件、或者epoll_wait有获取到异常，如链接断开等
    if (events & (flag[j] | EPOLLERR | EPOLLHUP)) 
    {
      try_speculative_[j] = true;
	  //遍历对应数组成员的整个队列
      while (reactor_op* op = op_queue_[j].front())
      {
       //reactive_socket_accept_op_base::do_perform  reactive_socket_recv_op_base::do_perform
  //reactive_socket_send_op_base::do_perform  分别对应新链接，读取数据，发送数据的底层实现
  	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
        if (reactor_op::status status = op->perform()) 
		//status为true表示成功，false表示底层处理失败
        {
          op_queue_[j].pop();
          io_cleanup.ops_.push(op);
          if (status == reactor_op::done_and_exhausted)
          {
            try_speculative_[j] = false;
            break;
          }
        }
		//例如如果没读到一个完整的数据，则这里直接退出，继续循环处理其他网络数据，下次继续读取。
        else
          break;
      }
    }
  }

  // The first operation will be returned for completion now. The others will
  // be posted for later by the io_cleanup object's destructor.
   //这里只返回了第一个op，其他的op在~perform_io_cleanup_on_block_exit处理
  io_cleanup.first_op_ = io_cleanup.ops_.front();
  //把返回的第一个从队列中清除，剩余的op还在队列中
  io_cleanup.ops_.pop(); 

  //该逻辑的总体思路：队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行

  //只返回第一个op,外层的epoll_reactor::descriptor_state::do_complete中执行该op对应的complete
  return io_cleanup.first_op_;
}

//epoll_reactor::descriptor_state::do_complete执行
//网络IO相关的任务处理，如accept接收新链接、接收数据、发送数据
operation* epoll_reactor::descriptor_state::perform_io(uint32_t events)
{
  ......
  
  //分别对应链接到达或者数据来临、可以写数据、有紧急的数据可读(这里应该表示有带外数据到来)
  static const int flag[max_ops] = { EPOLLIN, EPOLLOUT, EPOLLPRI };
  //循环处理各自不同的reactor_op(reactive_socket_accept_op_base   reactive_socket_recv_op_base reactive_socket_send_op_base)
  for (int j = max_ops - 1; j >= 0; --j)
  {
    if (events & (flag[j] | EPOLLERR | EPOLLHUP)) //有读写事件、或者epoll_wait有获取到异常，如链接断开等
    {
      try_speculative_[j] = true;
      while (reactor_op* op = op_queue_[j].front())
      {
       //reactive_socket_accept_op_base::do_perform  reactive_socket_recv_op_base::do_perform
    //reactive_socket_send_op_base::do_perform  分别对应新链接，读取数据，发送数据的底层实现
  	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
        if (reactor_op::status status = op->perform()) 
		//status为true表示成功，false表示底层处理失败
        {
          //取出对应的op，入队到临时队列io_cleanup.ops_
          op_queue_[j].pop();
          io_cleanup.ops_.push(op);
          if (status == reactor_op::done_and_exhausted)
          {
            try_speculative_[j] = false;
            break;
          }
        }
		//如果有异常
        else
          break;
      }
    }
  }

   //这里只返回了第一个op，其他的op在~perform_io_cleanup_on_block_exit处理
  io_cleanup.first_op_ = io_cleanup.ops_.front();
  //把返回的第一个从队列中清除，剩余的op还在队列中
  io_cleanup.ops_.pop(); 

  //该逻辑的总体思路：队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行

  //只返回第一个op,外层的epoll_reactor::descriptor_state::do_complete中执行该op对应的complete
  return io_cleanup.first_op_;
}
    从上面的分析可以看出，op_queue_队列上的所有reactor_op的底层IO处理成功后，会把该reactor_op入队到临时io_cleanup.first_op_队列中，
函数结束的时候把队首的reactor_op取出然后返回，在外层的do_complete中直接执行op->complete(也就是reactor_op中的complete_func，分别对应accetp、读、写中的
reactive_socket_accept_op_base::do_perform、reactive_socket_accept_op_base::do_perform、reactive_socket_recv_op_base reactive_socket_send_op_base::do_perform  )。
    那么这里有个问题？临时队列的其他reactor_op是如何处理的呢？其他reactor_op的处理实际上由perform_io_cleanup_on_block_exit析构函数处理，下面看下该析构函数
实现：

struct epoll_reactor::perform_io_cleanup_on_block_exit
{
 //初始化构造
  explicit perform_io_cleanup_on_block_exit(epoll_reactor* r)
    : reactor_(r), first_op_(0)
  {
  }
  
  //epoll_reactor::descriptor_state::perform_io中后续收尾处理
  ~perform_io_cleanup_on_block_exit()
  { 
    //配合epoll_reactor::descriptor_state::perform_io阅读，可以看出第一个op由本线程获取，其他op放入到了ops_队列
    //队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行
	if (first_op_)
    {
      //队列不为空
      if (!ops_.empty())
	  	//scheduler::post_deferred_completions，op任务放入全局队列，延迟执行
        reactor_->scheduler_.post_deferred_completions(ops_);
    }
    else
    {
	  //队首的线程由本线程处理，其他op任务放入全局任务队列, 见epoll_reactor::descriptor_state::do_complete
      reactor_->scheduler_.compensating_work_started();
    }
  }
  
  //所属epoll_reactor
  epoll_reactor* reactor_;
  //临时队列
  op_queue<operation> ops_;
  //标记队首位置
  operation* first_op_;
};





线程队列：thread_context

除了全局任务队列op_queue_外，每个工作线程还有各自的私有线程队列，该私有队列由scheduler_thread_info结构管理，如下所示：

全局任务队列，为啥还要设计一个私有任务队列，因为可以一次性入队
	completion_handler
	
如何保证读写某个fd的数据有序，如果第一个线程度一部分数据，第二个线程又读取一部分数据，无法连贯
并发如何控制

asio
1. 并发控制
2. 队列组成
3. 调度实现



























第二章：mongodb网络模块(transport)处理实现
    ASIO部分分析了网络异步IO详细实现过程，本文将结合mongodb内核服务层代码来分析mongodb与ASIO库得完美结合，了解学习mongodb是如何触发异步IO操作，
以及解析报文后的各自状态处理实现，也就是网络状态机实现流程。
2.1 带着问题分析
    本章主要了解mongodb如下问题：
1. mongodb服务层如何与ASIO库结合，如何使用ASIO库接口？
2. 收到数据后如何处理？
3. 如何实现网络处理的各自状态转换(网络状态机)？

2.2 transport代码目录结构
。。。xxxx todo  这里需要截图代码目录，并且画出类继承关系

    在阅读代码前首先了解transport相关代码目录结构，这样有助于快速阅读代码。从上图可以看出，传输层模块可以分为如下子模块：
1. transport_layer传输层子模块
2. Ticket数据收发回调处理子模块
3. Session会话子模块
4. service_state_machine状态机子模块
5. service_executor服务运行(网络线程模型)子模块 
6. service_entry_point_impl服务入口子模块
7. networkMessageCompressors消息压缩子模块
   各个子模块功能如下表所示：  todo  xxxx  画图

2.3 transport_layer传输层网络模块实现
    传输层模块主要负责套接字相关初始化相关处理，该模块的基类由TransportLayer完成，具体实现如下：
class TransportLayer { //TransportLayerManager继承该类    TransportLayerManager::createWithConfig构造使用
    MONGO_DISALLOW_COPYING(TransportLayer);

public:
	//析构函数
    virtual ~TransportLayer() = default;

    //下面这些虚函数，实际上由继承类TransportLayerASIO、TransportLayerLegacy具体实现
    virtual Ticket sourceMessage(const SessionHandle& session,
                                 Message* message,
                                 Date_t expiration = Ticket::kNoExpirationDate) = 0;
    virtual Ticket sinkMessage(const SessionHandle& session,
                               const Message& message,
                               Date_t expiration = Ticket::kNoExpirationDate) = 0;
    virtual Status wait(Ticket&& ticket) = 0;
    using TicketCallback = stdx::function<void(Status)>;
    virtual void asyncWait(Ticket&& ticket, TicketCallback callback) = 0;
    virtual void end(const SessionHandle& session) = 0;
    virtual Status start() = 0;
    virtual void shutdown() = 0;
    virtual Status setup() = 0;

protected:
    //初始化构造
    TransportLayer() = default;

    //获取ticket对应的接口Impl
    TicketImpl* getTicketImpl(const Ticket& ticket) {
        return ticket.impl();
    }

    std::unique_ptr<TicketImpl> getOwnedTicketImpl(Ticket&& ticket) {
        return std::move(ticket).releaseImpl();
    }
    
	//获取ticket对应的tl
    TransportLayer* getTicketTransportLayer(const Ticket& ticket) {
        return ticket._tl;
    }
};
    从TransportLayer基类可以看出，该类中有很多虚函数，这些虚函数实际上在继承类TransportLayerManager、TransportLayerLegacy
、TransportLayerASIO中实现。todoxxx  这里画一个基层图
2.3.1 TransportLayerManager传输层管理模块实现
    TransportLayerManager传输层管理模块主要完成Legacy和asio两种模式的管理，通过transportLayer配置项来进行不同逻辑流程的处理，该类实现如下：
class TransportLayerManager final : public TransportLayer {
public:
    //初始化构造，注意这里的tls
    TransportLayerManager(std::vector<std::unique_ptr<TransportLayer>> tls)
        : _tls(std::move(tls)) {}
    TransportLayerManager();
    
	//默认对应TransportLayerASIO的sourceMessage和sinkMessage接口
    Ticket sourceMessage(const SessionHandle& session,
                         Message* message,
                         Date_t expiration = Ticket::kNoExpirationDate) override;
    Ticket sinkMessage(const SessionHandle& session,
                       const Message& message,
                       Date_t expiration = Ticket::kNoExpirationDate) override;
    //套接字处理相关的几个接口
    Status start() override;
    void shutdown() override;
    Status setup() override;

    //该接口实际上已经废弃，忽略
    Status addAndStartTransportLayer(std::unique_ptr<TransportLayer> tl);
    //通过配置文件获取对应的TransportLayer，默认ASIO对应TransportLayerASIO
    static std::unique_ptr<TransportLayer> createWithConfig(const ServerGlobalParams* config,
                                                            ServiceContext* ctx);

private:
    template <typename Callable>
	//TransportLayerManager::shutdown调用，遍历tls，执行cb
    void _foreach(Callable&& cb) const;
    //锁
    mutable stdx::mutex _tlsMutex;
    //createWithConfig中赋值,对应TransportLayerASIO
    std::vector<std::unique_ptr<TransportLayer>> _tls;
};
    该类几个核心接口实现如下：
//根据配置构造相应类信息  _initAndListen中调用
std::unique_ptr<TransportLayer> TransportLayerManager::createWithConfig(
    const ServerGlobalParams* config, ServiceContext* ctx) {
    std::unique_ptr<TransportLayer> transportLayer;
	//ServiceEntryPointMongod或者ServiceEntryPointMongos
    auto sep = ctx->getServiceEntryPoint();
	//如果配置是asio模式
    if (config->transportLayer == "asio") {
		//获取asio模式对应子配置信息
        transport::TransportLayerASIO::Options opts(config);

		//同步方式还是异步方式，默认synchronous
        if (config->serviceExecutor == "adaptive") {
			//动态线程池模型
            opts.transportMode = transport::Mode::kAsynchronous;
        } else if (config->serviceExecutor == "synchronous") {
            //一个链接一个线程模型
            opts.transportMode = transport::Mode::kSynchronous;
        } else {
            MONGO_UNREACHABLE;
        }

		//构造TransportLayerASIO类
        auto transportLayerASIO = stdx::make_unique<transport::TransportLayerASIO>(opts, sep);

		//ServiceExecutorSynchronous对应线程池同步模式，ServiceExecutorAdaptive对应线程池异步自适应模式
        if (config->serviceExecutor == "adaptive") { //异步方式
            ctx->setServiceExecutor(stdx::make_unique<ServiceExecutorAdaptive>(
                ctx, transportLayerASIO->getIOContext()));
        } else if (config->serviceExecutor == "synchronous") { //同步方式
            ctx->setServiceExecutor(stdx::make_unique<ServiceExecutorSynchronous>(ctx));
        }
		//transportLayerASIO转换为transportLayer类
        transportLayer = std::move(transportLayerASIO);
    } else if (serverGlobalParams.transportLayer == "legacy") {
		//获取legacy模式相关配置及初始化对应transportLayer
		transport::TransportLayerLegacy::Options opts(config);
        transportLayer = stdx::make_unique<transport::TransportLayerLegacy>(opts, sep);
        ctx->setServiceExecutor(stdx::make_unique<ServiceExecutorSynchronous>(ctx));
    }

	//transportLayer转存到对应retVector数组中并返回
    std::vector<std::unique_ptr<TransportLayer>> retVector;
    retVector.emplace_back(std::move(transportLayer));
    return stdx::make_unique<TransportLayerManager>(std::move(retVector));
}
     createWithConfig从配置文件中获取配置，构造对应的TransportLayer返回，默认配置为asio异步模式，所以默认传输层对应
TransportLayerASIO，后续分析只分析TransportLayerASIO相关接口实现，legacy配置模式不再分析。

//调用TransportLayerASIO::setup()进行套接字相关初始化及bind处理
Status TransportLayerManager::setup() {
    //_tls来源见TransportLayerManager::createWithConfig返回的retVector
    for (auto&& tl : _tls) {
		//TransportLayerASIO::setup() 套接字相关初始化及bind处理
        auto status = tl->setup(); 
        if (!status.isOK()) {
            _tls.clear();
            return status;
        }
    }

    return Status::OK();
}

//调用TransportLayerASIO::start进行accept异步初始化相关处理
Status TransportLayerManager::start() {
    for (auto&& tl : _tls) {
		//默认TransportLayerASIO::start 
        auto status = tl->start();  
        if (!status.isOK()) {
            _tls.clear();
            return status;
        }
    }

    return Status::OK();
}

//TransportLayerManager::shutdown调用，遍历tls，执行cb
void TransportLayerManager::_foreach(Callable&& cb) const {
    {
        stdx::lock_guard<stdx::mutex> lk(_tlsMutex);
        for (auto&& tl : _tls) {
			//传输层套接字相关回收处理TransportLayerManager::shutdown
            cb(tl.get());
        }
    }
}
//传输层套接字相关回收处理TransportLayerManager::shutdown
void TransportLayerManager::shutdown() {
	//TransportLayerASIO::shutdown，传输层回收处理
    _foreach([](TransportLayer* tl) { tl->shutdown(); });
}
     总结：TransportLayerManager类注意功能包括通过配置文件获取对应的TransportLayer，默认asio配置对应transportLayerASIO。
同时套接字初始化、accept异步初始化及套接字回收相关处理都由manager管理，最终调用transportLayerASIO类相应的接口完成。

2.3.2 transportLayerASIO异步网络传输层模块实现
    TransportLayerManager类最终会调用transportLayerASIO实现底层异步网络IO操作，该类主要成员即接口如下：
class TransportLayerASIO final : public TransportLayer {
public:
    struct Options {  //TransportLayerASIO::Options保持asio传输层相关参数信息
        explicit Options(const ServerGlobalParams* params);
        //默认监听端口
        int port = ServerGlobalParams::DefaultDBPort;  // port to bind to
        //ip配置列表，例如bindIp: 127.0.0.1,30.25.x.17，可以绑定多个IP
        std::string ipList;                             
        bool enableIPv6 = false;                   
        //同步还是异步，赋值见createWithConfig
        Mode transportMode = Mode::kSynchronous;  
        //默认最大链接数限制，net.maxIncomingConnections配置                                          
        size_t maxConns = DEFAULT_MAX_CONN;        
    };
	
	//该类的以下核心接口在transport_layer_asio.cpp文件实现 
    TransportLayerASIO(const Options& opts, ServiceEntryPoint* sep);
    virtual ~TransportLayerASIO();
    Ticket sourceMessage(const SessionHandle& session,
                         Message* message,
                         Date_t expiration = Ticket::kNoExpirationDate) final;
    Ticket sinkMessage(const SessionHandle& session,
                       const Message& message,
                       Date_t expiration = Ticket::kNoExpirationDate) final;
    Status wait(Ticket&& ticket) final;
    void asyncWait(Ticket&& ticket, TicketCallback callback) final;
    void end(const SessionHandle& session) final;
    Status setup() final;
    Status start() final;
    void shutdown() final;
    const std::shared_ptr<asio::io_context>& getIOContext();

private:
	//using说明，简化下
    using ASIOSessionHandle = std::shared_ptr<ASIOSession>;
    using ConstASIOSessionHandle = std::shared_ptr<const ASIOSession>;
    using GenericAcceptor = asio::basic_socket_acceptor<asio::generic::stream_protocol>;
	//accept异步处理，调用ASIO库实现
    void _acceptConnection(GenericAcceptor& acceptor);
	//共享资源保护锁
    stdx::mutex _mutex;
	
	//新链接套接字相关的IO操作，配合adaptive线程模型使用，后续线程模型详细分析
	std::shared_ptr<asio::io_context> _workerIOContext; 
    //listener线程进行accept操作的io上下文，请配合ASIO源码分析部分阅读
    std::unique_ptr<asio::io_context> _acceptorIOContext;  
    
    //赋值见TransportLayerASIO::setup，创建套接字，然后bind  一台服务器可以bind多个IP地址
    //一个服务端监听的ip地址对应一个acceptor
    std::vector<std::pair<SockAddr, GenericAcceptor>> _acceptors;

    //listener线程，专门做accept接收新连接处理
    stdx::thread _listenerThread;
    
	///服务入口，mongod和mongos有不同的入口点
    ServiceEntryPoint* const _sep = nullptr;
	//运行状态标识
    AtomicWord<bool> _running{false};

    //生效使用见TransportLayerASIO::setup，配置来验ServerGlobalParams
    Options _listenerOptions;
};
    上面的类成员及杰克可以直观的看出，该类负责套接字、accept等处理，其核心接口实现如下：
//传输层套接字相关配置
TransportLayerASIO::Options::Options(const ServerGlobalParams* params)
	//端口、ip、最大链接数
    : port(params->port),
      ipList(params->bind_ip),
      enableIPv6(params->enableIPv6),
      maxConns(params->maxConns) {
}

//套接字初始化及监听处理
Status TransportLayerASIO::setup() {
    std::vector<std::string> listenAddrs;
	//如果不配置bindIp，则默认监听127这个地址
    if (_listenerOptions.ipList.empty()) {
        listenAddrs = {"127.0.0.1"};
        if (_listenerOptions.enableIPv6) {
            listenAddrs.emplace_back("::1");
        }
    } else {
    	//配置文件中的bindIp:1.1.1.1,2.2.2.2，以逗号分隔符获取ip列表存入ipList
        boost::split(
            listenAddrs, _listenerOptions.ipList, boost::is_any_of(","), boost::token_compress_on);
    }

    //遍历ip地址列表
    for (auto& ip : listenAddrs) {
		......

		//根据IP和端口构造对应SockAddr结构
        const auto addrs = SockAddr::createAll(
            ip, _listenerOptions.port, _listenerOptions.enableIPv6 ? AF_UNSPEC : AF_INET);
        if (addrs.empty()) {
            warning() << "Found no addresses for " << ip;
            continue;
        }
		//遍历addrs信息
        for (const auto& addr : addrs) {
            asio::generic::stream_protocol::endpoint endpoint(addr.raw(), addr.addressSize);

			//_acceptorIOContext和_acceptors关联
            GenericAcceptor acceptor(*_acceptorIOContext);
			//epoll注册，也就是fd和epoll关联
            acceptor.open(endpoint.protocol()); //basic_socket_acceptor::open
			//SO_REUSEADDR配置   
            acceptor.set_option(GenericAcceptor::reuse_address(true));
			//非阻塞设置
            acceptor.non_blocking(true, ec);  //basic_socket_acceptor::non_blocking
            if (ec) {
                return errorCodeToStatus(ec);
            }
			//bind绑定  
            acceptor.bind(endpoint, ec); 
            if (ec) {
                return errorCodeToStatus(ec);
            }

			//socket对应得套接字_acceptors相关处理在后续的TransportLayerASIO::start
			//一个accptor对应一个addr，存入_acceptors vector
            _acceptors.emplace_back(std::make_pair(std::move(addr), std::move(acceptor)));
        }
    }
	
	......

    return Status::OK();
}

//listener线程创建并通过_acceptConnection进行accept异步处理
Status TransportLayerASIO::start() { //listen线程处理
    ......

	//这里专门起一个线程做listen相关的accept事件处理
    _listenerThread = stdx::thread([this] {
		//新线程为listen线程,循环处理accept请求
        setThreadName("listener"); 
        while (_running.load()) {
			//accept处理由一个专门的_acceptorIOContext负责
            asio::io_context::work work(*_acceptorIOContext); 
			//_acceptorIOContext和_acceptors是关联的，见TransportLayerASIO::setup
            try {
				//对应ASIO中得io_context::run，accept异步调度处理
				_acceptorIOContext->run();  
            } catch (...) {
                //异常处理
            }
        }
    }); //创建listener线程

	//遍历acceptor 
    for (auto& acceptor : _acceptors) {  
		//listen backlog设置
        acceptor.second.listen(serverGlobalParams.listenBacklog);
		//异步accept注册在该函数中
        _acceptConnection(acceptor.second);    
    }
    return Status::OK();
}

   TransportLayerASIO::start()启用一个listener线程调度异步accept处理，然后调用_acceptConnection进行异步注册，
_acceptConnection实现如下：
void TransportLayerASIO::_acceptConnection(GenericAcceptor& acceptor) {
	//新链接到来时候的回调函数，也就是ASIO底层accept一个新链接的回调处理函数
	auto acceptCb = [this, &acceptor](const std::error_code& ec, GenericSocket peerSocket) mutable {
		//底层accept异常
        if (ec) {
            log() << "Error accepting new connection on "
                  << endpointToHostAndPort(acceptor.local_endpoint()) << ": " << ec.message();
			//继续递归调用
            _acceptConnection(acceptor);
            return;
        }

		//每个新的链接都会new一个新的ASIOSession
        std::shared_ptr<ASIOSession> session(new ASIOSession(this, std::move(peerSocket)));

		//新的链接处理ServiceEntryPointImpl::startSession
        _sep->startSession(std::move(session));
		//递归
        _acceptConnection(acceptor); 
    };

	//通过asio库实现accept异步事件注册，新连接获取到后回调函数为acceptCb
    acceptor.async_accept(*_workerIOContext, std::move(acceptCb)); 
}

    通过上面几个接口的分析，我们了解了套接字相关处理、accept异步处理的实现流程。下面的几个接口则是针对
新链接上的数据接收和发送的接口，实现如下：
//构造数据接收相关类:ASIOSourceTicket
Ticket TransportLayerASIO::sourceMessage(const SessionHandle& session,
                                         Message* message,
                                         Date_t expiration) {
	//构造asioSession
    auto asioSession = checked_pointer_cast<ASIOSession>(session);
	//构造数据接收相关ASIOSourceTicket类
    auto ticket = stdx::make_unique<ASIOSourceTicket>(asioSession, expiration, message);
    return {this, std::move(ticket)};
}

//构造数据接收相关类:ASIOSinkTicket
Ticket TransportLayerASIO::sinkMessage(const SessionHandle& session,
                                       const Message& message,
                                       Date_t expiration) {
	//构造asioSession
    auto asioSession = checked_pointer_cast<ASIOSession>(session);
	//构造数据接收相关ASIOSinkTicket类
    auto ticket = stdx::make_unique<ASIOSinkTicket>(asioSession, expiration, message);
    return {this, std::move(ticket)};
}

//asio数据收发同步处理
Status TransportLayerASIO::wait(Ticket&& ticket) {
	//接收对应ASIOSourceTicket，发送对应ASIOSinkTicket
    auto ownedASIOTicket = getOwnedTicketImpl(std::move(ticket));
    auto asioTicket = checked_cast<ASIOTicket*>(ownedASIOTicket.get());

    Status waitStatus = Status::OK();
	//调用对应fill接口
    asioTicket->fill(true, [&waitStatus](Status result) { waitStatus = result; });

    return waitStatus;
}

//asio数据收发异步处理
void TransportLayerASIO::asyncWait(Ticket&& ticket, TicketCallback callback) {
	//接收对应ASIOSourceTicket，发送对应ASIOSinkTicket
	auto ownedASIOTicket = std::shared_ptr<TicketImpl>(getOwnedTicketImpl(std::move(ticket)));
    auto asioTicket = checked_cast<ASIOTicket*>(ownedASIOTicket.get());

	//调用对应ASIOTicket::fill
    asioTicket->fill(
        false,
        [ callback = std::move(callback),
          ownedASIOTicket = std::move(ownedASIOTicket) ](Status status) { callback(status); });
}

2.3.3 总结
    从上面的分析可以把TransportLayerASIO接口分为两类：
1. 针对套接字、accept异步处理相关接口，主要是setup()、start()、_acceptConnection()接口scheduler
2. 针对数据接收和发送相关的接口，主要是sourceMessage()、sinkMessage()、wait()、asyncWait()，这几个接口最终会调用ASIOTicket类的接口来完成数据。

3. Ticket数据收发回调处理子模块
3.1 ASIOTicket类实现
    Ticket模块把数据接收和发送过程分开处理，发送由ASIOSinkTicket类实现，接收过程由ASIOSourceTicket类实现，这两个类的基类为ASIOTicket，该类实现如下：
//下面的ASIOSinkTicket和ASIOSourceTicket继承该类,用于控制数据的发送和接收
class TransportLayerASIO::ASIOTicket : public TicketImpl {
    MONGO_DISALLOW_COPYING(ASIOTicket);

public:
    //初始化构造
    explicit ASIOTicket(const ASIOSessionHandle& session, Date_t expiration);
    //获取sessionId
    SessionId sessionId() const final {
        return _sessionId;
    }
    //asio模式没用，针对legacy模型
    Date_t expiration() const final {
        return _expiration;
    }
    void fill(bool sync, TicketCallback&& cb);

protected:
    void finishFill(Status status);
    std::shared_ptr<ASIOSession> getSession();
    bool isSync() const;
    virtual void fillImpl() = 0;

private:
    //会话信息
    std::weak_ptr<ASIOSession> _session;
    //每个session有一个唯一id
    const SessionId _sessionId;
    //asio模型没用，针对legacy生效
    const Date_t _expiration;
    //接收到数据或者发送数据成功对应的回调函数
    TicketCallback _fillCallback;
    //同步方式还是异步方式进行数据处理，默认异步
    bool _fillSync;
};
    每个基类都包含会话信息，同时都会有一个_fillCallback回调函数，该函数在对应的继承类中实现。

//数据接收的ticket类
class TransportLayerASIO::ASIOSourceTicket : public TransportLayerASIO::ASIOTicket {
public:
    //初始化构造
    ASIOSourceTicket(const ASIOSessionHandle& session, Date_t expiration, Message* msg);

protected:
    void fillImpl() final;
private:
    void _headerCallback(const std::error_code& ec, size_t size);
    void _bodyCallback(const std::error_code& ec, size_t size);

    //存储数据的buffer，最后数据获取完毕后，会转存到_target中
    SharedBuffer _buffer;
    //数据赋值见TransportLayerASIO::ASIOSourceTicket::_bodyCallback
    //初始空间赋值见ServiceStateMachine::_sourceMessage->Session::sourceMessage->TransportLayerASIO::sourceMessage
    Message* _target;
};

//TransportLayerASIO类的相关接口使用
//数据发送的ticket类
class TransportLayerASIO::ASIOSinkTicket : public TransportLayerASIO::ASIOTicket {
public:
    //初始化构造
    ASIOSinkTicket(const ASIOSessionHandle& session, Date_t expiration, const Message& msg);
protected:
    void fillImpl() final;

private:
    void _sinkCallback(const std::error_code& ec, size_t size);
    //需要发送的数据message信息
    Message _msgToSend;
};
    从上面的继承类可以看出，mongodb在实现ticket的时候，使用两个ASIOSinkTicket、ASIOSourceTicket类分别实现数据发送和数据接收处理。
3.2 主要接口实现
    在TransportLayerASIO子模块分析中已经知道TransportLayerASIO::asyncWait最终会调用ASIOTicket::fill进行底层数据接收或者发送处理，该函数实现如下：
void TransportLayerASIO::ASIOTicket::fill(bool sync, TicketCallback&& cb) {
	//同步还是异步，ASIO对应异步
    _fillSync = sync;
    dassert(!_fillCallback);
	//cb赋值给fill回调，cb接收数据过程对应ServiceStateMachine::_sourceCallback
	//，cb发送数据过程对应ServiceStateMachine::_sinkCallback
    _fillCallback = std::move(cb);
	//接收对应TransportLayerASIO::ASIOSourceTicket::fillImpl
	//发送对应TransportLayerASIO::ASIOSinkTicket::fillImpl
    fillImpl();
}
   fill()接口最终根据本ASIOTicket所属类型，如果是接收则调用TransportLayerASIO::ASIOSourceTicket::fillImpl进行数据接收处理，
如果是发送则调用TransportLayerASIO::ASIOSinkTicket::fillImpl接口处理。
3.2.1 数据接收处理过程
//数据接收及其回调
void TransportLayerASIO::ASIOSourceTicket::fillImpl() {  
	//获取session信息
    auto session = getSession();
    if (!session)
        return;
	//mongodb头部结构长度
    const auto initBufSize = kHeaderSize;
	//分配这么多空间准备接收数据
    _buffer = SharedBuffer::allocate(initBufSize);

	//读取数据 TransportLayerASIO::ASIOSession::read，头部接收到后执行_headerCallback回调
    session->read(isSync(),
			  //先读取头部字段出来
              asio::buffer(_buffer.get(), initBufSize), 
                [this](const std::error_code& ec, size_t size) { _headerCallback(ec, size); });
}

//读取到mongodb header头部信息后的回调处理
void TransportLayerASIO::ASIOSourceTicket::_headerCallback(const std::error_code& ec, size_t size) {
    ......
	//获取session信息
    auto session = getSession();
    if (!session)
        return;
	//从_buffer中获取头部信息
    MSGHEADER::View headerView(_buffer.get());
	//获取message长度
    auto msgLen = static_cast<size_t>(headerView.getMessageLength());
	//长度太小或者太大，直接报错
    if (msgLen < kHeaderSize || msgLen > MaxMessageSizeBytes) {
        ......
        return;
    }

	//说明数据部分也读取出来了，一个完整的mongo报文读取完毕,也就是报文只带有头部，没有包体的协议请求
    if (msgLen == size) {
        finishFill(Status::OK());
        return;
    }

	//内容还不够一个mongo协议报文，继续读取body长度字节的数据，读取完毕后开始body处理
    _buffer.realloc(msgLen); //注意这里是realloc，保证头部和body在同一个buffer中
    MsgData::View msgView(_buffer.get());


	//读取数据body TransportLayerASIO::ASIOSession::read
    session->read(isSync(),
      //数据读取到该buffer				
      asio::buffer(msgView.data(), msgView.dataLen()),
      //读取成功后的回调处理，接收完成调用_bodyCallback
      [this](const std::error_code& ec, size_t size) { _bodyCallback(ec, size); });
}
    _headerCallback中接收到包体部分后，工作线程会调用_bodyCallback执行，该函数接口实现如下:
void TransportLayerASIO::ASIOSourceTicket::_bodyCallback(const std::error_code& ec, size_t size) {
    ......

	//buffer转存到_target中
    _target->setData(std::move(_buffer));
	//流量统计
    networkCounter.hitPhysicalIn(_target->size());
	//TransportLayerASIO::ASIOTicket::finishFill执行_fillCallback
    finishFill(Status::OK()); //包体内容读完后，开始下一阶段的处理  
}
    继续查看finishFill实现：
void TransportLayerASIO::ASIOTicket::finishFill(Status status) { //每一个阶段处理完后都通过这里执行相应的回调
    ......
    auto fillCallback = std::move(_fillCallback);
	//数据接收对应ServiceStateMachine::_sourceCallback
	//数据发送对应ServiceStateMachine::_sinkCallback
    fillCallback(status); 
}
    至此，一个完整的mongodb报文内容接收完毕了，开始进入状态机处理流程。
3.2.2 数据发送处理过程
	数据发送由ASIOSinkTicket类fillImpl实现，如下
void TransportLayerASIO::ASIOSinkTicket::fillImpl() {
	//获取对应session
    auto session = getSession();
    if (!session)
        return;

	//发送数据 TransportLayerASIO::ASIOSession::write
    session->write(isSync(),
	   asio::buffer(_msgToSend.buf(), _msgToSend.size()),
	   //发送数据成功后的callback回调
	   [this](const std::error_code& ec, size_t size) { _sinkCallback(ec, size); });
}
    当发送msg成功后会执行_sinkCallback，该函数接口实现如下：
void TransportLayerASIO::ASIOSinkTicket::_sinkCallback(const std::error_code& ec, size_t size) {
	//发送的网络字节数统计
    networkCounter.hitPhysicalOut(_msgToSend.size()); 
	//执行对应fillCallback，即状态机调度中的ServiceStateMachine::_sinkCallback
    finishFill(ec ? errorCodeToStatus(ec) : Status::OK());
}

3.3 总结
1. 数据收发过程由ASIOTicket完成，mongodb内核在实现的时候，针对发送和接收分开实现，接收处理由ASIOSourceTicket类实现，
发送处理由ASIOSinkTicket实现。
2. 接收处理过程分为两步完成：1. 头部接收即处理。2. body包体部分接收及处理。
3. 一个完整的mongod报文发送或者接收完毕后，会进入状态机模块中执行，分别对应状态机模块的ServiceStateMachine::_sourceCallback和ServiceStateMachine::_sinkCallback接口。
4. Ticket模块调用boost-asio库进行真正的网络IO数据读写，实际上由session会话模块完成。

4. session会话子模块
    上面的ASIOTicket分析可以看出，最终的boost-asio异步数据读写操作由session会话模块处理，该模块主要负责调用boost-asio库来实现
底层数据处理。
4.1 session实现
asio模式的session会话模块代码实现主要由ASIOSession类完成，如下：
//调用boost-asio库接口实现数据收发
class TransportLayerASIO::ASIOSession : public Session {
public:
    //初始化构造 TransportLayerASIO::_acceptConnection调用
    ASIOSession(TransportLayerASIO* tl, GenericSocket socket)
		//fd描述符及TL初始化赋值
        : _socket(std::move(socket)), _tl(tl) {
        std::error_code ec;

        //异步方式设置为非阻塞读
        _socket.non_blocking(_tl->_listenerOptions.transportMode == Mode::kAsynchronous, ec);
        fassert(40490, ec.value() == 0);

        //获取套接字的family
        auto family = endpointToSockAddr(_socket.local_endpoint()).getType();
        //
        if (family == AF_INET || family == AF_INET6) {
            //no_delay keep_alive套接字系统参数设置
            _socket.set_option(asio::ip::tcp::no_delay(true));
            _socket.set_option(asio::socket_base::keep_alive(true));
            //KeepAlive系统参数设置
            setSocketKeepAliveParams(_socket.native_handle());
        }

        //获取本端和对端地址
        _local = endpointToHostAndPort(_socket.local_endpoint());
        _remote = endpointToHostAndPort(_socket.remote_endpoint(ec));
        if (ec) {
            LOG(3) << "Unable to get remote endpoint address: " << ec.message();
        }
    }
    //获取该session对应的tl
    TransportLayer* getTransportLayer() const override {
        return _tl;
    }
    //获取远端地址
    const HostAndPort& remote() const override {
        return _remote;
    }
    //获取本端地址
    const HostAndPort& local() const override {
        return _local;
    }
    //获取链接对应描述符信息
    GenericSocket& getSocket() {
        return _socket;
    }

    //描述符回收处理
    void shutdown() {
        ......
    }

    //ASIOTicket::getSession中调用，标识该描述符是否已注册
    bool isOpen() const {
        return _socket.is_open();
    }   
    
    //TransportLayerASIO::ASIOSourceTicket::fillImpl调用
    template <typename MutableBufferSequence, typename CompleteHandler>
    //buffers参数里面携带有buffer的size长度
	//读取整个buffers数据后，执行handler回调
    void read(bool sync, const MutableBufferSequence& buffers, CompleteHandler&& handler) {
		......
		//真正的底层读取操作
		opportunisticRead(sync, _socket, buffers, std::forward<CompleteHandler>(handler)); 
    }

    template <typename ConstBufferSequence, typename CompleteHandler>
	//buffers参数里面携带有buffer的size长度
	//发送整个buffers数据后，执行handler回调
    void write(bool sync, const ConstBufferSequence& buffers, CompleteHandler&& handler) {
		//真正的写入操作
		opportunisticWrite(sync, _socket, buffers, std::forward<CompleteHandler>(handler));
    }

//TransportLayerASIO::ASIOSourceTicket::fillImpl调用
private:
    //从stream对应fd读取数据   read和write指定长度的数据后，执行对应的回调handler
    template <typename Stream, typename MutableBufferSequence, typename CompleteHandler>
    //一次性读size字节如果成功，则直接执行handler，否则没写完的数据通过异步方式继续读，写完后执行对应handler
    void opportunisticRead(bool sync,
                           Stream& stream,
                           const MutableBufferSequence& buffers, //buffers有大小size，实际读最多读size字节
                           CompleteHandler&& handler) {
        std::error_code ec;
        //先直接同步方式从协议栈读取数据，直到读取到数据并且把协议栈数据读完
        auto size = asio::read(stream, buffers, ec);
        //协议栈内容已经读完了，但是还不够size字节，则继续异步读取
        if ((ec == asio::error::would_block || ec == asio::error::try_again) && !sync) {
            // asio::read is a loop internally, so some of buffers may have been read into already.
            // So we need to adjust the buffers passed into async_read to be offset by size, if
            // size is > 0.
            //buffers有大小size，实际读最多读size字节
            MutableBufferSequence asyncBuffers(buffers);
            if (size > 0) {
                asyncBuffers += size; //buffer offset向后移动
            }

            //数据得读取及handler回调执行见asio库得read_op::operator
            asio::async_read(stream, asyncBuffers, std::forward<CompleteHandler>(handler));
        } else { 
            //直接read获取到size字节数据，则直接执行handler 
            handler(ec, size);
        }
    }

    template <typename Stream, typename ConstBufferSequence, typename CompleteHandler>
    void opportunisticWrite(bool sync,
                            Stream& stream,
                            const ConstBufferSequence& buffers, //buffers有大小size，实际读最多读size字节
                            CompleteHandler&& handler) {
        std::error_code ec;
        //先直接写
        auto size = asio::write(stream, buffers, ec); 
        //一次性写size字节如果成功，则直接执行handler，否则没写完的数据通过异步方式继续写，写完后执行对应handler
        if ((ec == asio::error::would_block || ec == asio::error::try_again) && !sync) {
        //一般当内核协议栈buffer写满后，会返回try_again
            // asio::write is a loop internally, so some of buffers may have been read into already.
            // So we need to adjust the buffers passed into async_write to be offset by size, if
            // size is > 0.
            ConstBufferSequence asyncBuffers(buffers);
            if (size > 0) {
                asyncBuffers += size;
            }

            //数据得读取及handler回调执行见asio库得write_op::operator
            asio::async_write(stream, asyncBuffers, std::forward<CompleteHandler>(handler));
        } else {
            handler(ec, size);
        }
    }

    //远端地址信息
    HostAndPort _remote;
    //本段地址信息
    HostAndPort _local;
    //赋值见TransportLayerASIO::_acceptConnection
    //也就是fd，链接描述符
    GenericSocket _socket;
    //该session对应tl
    TransportLayerASIO* const _tl;
};

class Session {
public:
	......
	
private:
    //session唯一id
    const Id _id;
};
//sessionid自增实现
Session::Session() : _id(sessionIdCounter.addAndFetch(1)), _tags(kPending) {}
	
     上面的ASIOSession类包含有完整链接session信息，包括链接描述符socket、两端地址信息及TL信息，一个session对应一个链接，对session的读写操作
针对的也就是该链接上的读写操作，该类是真正调用boost-asio网络库的实现类。

4.2 总结
1. 一个session对应一个链接，该链接上的读写操作都是通过该ASIOSession类实现
2. ASIOSession类最终直接调用boost-asio网络库实现底层数据收发。
3. 每个session都会有一个全局唯一ID标识

5. 总结
    transport网络传输实现由传输层、Ticket、会话、状态机、服务人口、服务运行、网络消息压缩七个子模块组成，各个子模块功能总结如下：
1. ransport_layer传输层子模块进行套接字初始化及accept异步链接处理
2. Ticket数据收发回调处理子模块主要负责接收或者发送一个完整mongodb报文的回调处理
3. Session会话子模块负责底层boost-asio库的底层数据收发及回调实现
   以下四个模块，鉴于篇幅，下期分享：

4. service_state_machine状态机子模块负责mongodb网络处理状态调度 
5. service_executor服务运行子模块负责工作线程模型处理，包括sync(一个链接一个线程)和adaptive(动态线程池模式)两种模式  
6. service_entry_point_impl服务入口子模块负责报文解析及后续处理 
7. networkMessageCompressors消息压缩子模块用于消息压缩处理，减少网络IO消耗 


6. mongodb网络状态机处理实现
    mongodb服务状态机，用于处理网络处理过程中的各种状态转换，通过状态机调度统一管理，从而使网络处理过程中的各种状态正常运行。
mongodb服务状态机主要由service_state_machine.cpp、service_state_machine.h处理，这两个文件完成状态机ServiceStateMachine类结构及
接口定义实现。
6.1 ServiceStateMachine类结构实现
    具体实现如下:
class ServiceStateMachine : public std::enable_shared_from_this<ServiceStateMachine> {

public:
	......
	//该类注意几个接口实现如下
    static std::shared_ptr<ServiceStateMachine> create(ServiceContext* svcContext,
                                                       transport::SessionHandle session,
                                                       transport::Mode transportMode);

    ServiceStateMachine(ServiceContext* svcContext,
                        transport::SessionHandle session,
                        transport::Mode transportMode);

    /*
     * Any state may transition to EndSession in case of an error, otherwise the valid state
     * transitions are:
     * Source -> SourceWait -> Process -> SinkWait -> Source (standard RPC)
     * Source -> SourceWait -> Process -> SinkWait -> Process -> SinkWait ... (exhaust)
     * Source -> SourceWait -> Process -> Source (fire-and-forget)
     */

    enum class State {
        //ServiceStateMachine::ServiceStateMachine构造函数初始状态
        Created,    
		//ServiceStateMachine::_runNextInGuard开始进入接收网络数据状态
        Source,      
		//等待获取数据
        SourceWait,  
		//处理接收到的数据
        Process,     
		//等待数据发送成功
        SinkWait,    
		//接收或者发送数据异常，则进入该状态
        EndSession,  
		//session回收处理进入该状态
        Ended        
    };

    /*
     * When start() is called with Ownership::kOwned, the SSM will swap the Client/thread name
     * whenever it runs a stage of the state machine, and then unswap them out when leaving the SSM.
     * 当使用Ownership::kOwned调用start()时，SSM将在运行状态机的某个阶段时交换客户机/线程名称，
     * 然后在离开SSM时取消交换。
     *
     * With Ownership::kStatic, it will assume that the SSM will only ever be run from one thread,
     * and that thread will not be used for other SSM's. It will swap in the Client/thread name
     * for the first run and leave them in place.
     *  Ownership::kStatic标识本SSM只会在本线程中运行，同时本线程不会运行其他SSM。
     *
     * kUnowned is used internally to mark that the SSM is inactive.
     * kUnowned在内部用于标记SSM处于非活动状态。
     */
    //所有权
    enum class Ownership { 
		//该状态表示本状态机SSM处于非活跃状态
		kUnowned,  
		//如果是transport::Mode::kSynchronous一个链接一个线程模式，则整个过程中都是同一个线程处理，所以不需要更改线程名
		//如果是async异步线程池模式，则处理链接的过程中会从conn线程变为worker线程
		//该状态标识本状态机SSM归属于某个工作worker线程，处于活跃调度运行状态
		kOwned, 
		//kSynchronous线程模型，一个链接对应一个SSM状态机，因此本状态机SSM始终由固定线程运行
		//表示SSM固定归属于某个线程
		kStatic 
    };

	//该类主要几个public接口
    void runNext();
    void start(Ownership ownershipModel);
    State state();
    void terminate();
    void terminateIfTagsDontMatch(transport::Session::TagMask tags);
    void setCleanupHook(stdx::function<void()> hook);

private:
	//该类主要几个private接口
    void _terminateAndLogIfError(Status status);
    void _scheduleNextWithGuard(ThreadGuard guard,
                                transport::ServiceExecutor::ScheduleFlags flags,
                                Ownership ownershipModel = Ownership::kOwned);
    const transport::SessionHandle& _session() const;
    void _runNextInGuard(ThreadGuard guard);
    inline void _processMessage(ThreadGuard guard);
    void _sourceCallback(Status status);
    void _sinkCallback(Status status);
    void _sourceMessage(ThreadGuard guard);
    void _sinkMessage(ThreadGuard guard, Message toSink);
    void _cleanupSession(ThreadGuard guard);
	
	//状态机所处状态，初始化会Created状态
    AtomicWord<State> _state{State::Created};

    //ServiceEntryPointMongod ServiceEntryPointMongos mongod及mongos入口点
    ServiceEntryPoint* _sep;
    //synchronous及adaptive模式
    transport::Mode _transportMode;
    //ServiceContextMongoD(mongod)或者ServiceContextNoop(mongos)服务上下文
    ServiceContext* const _serviceContext;
    
    //TransportLayerASIO::_acceptConnection->ServiceEntryPointImpl::startSession->ServiceStateMachine::create 
    //记录对端信息、同时负责数据相关得读写
    transport::SessionHandle _sessionHandle; //默认对应ASIOSession 
    //根据session构造对应client信息,ServiceStateMachine::ServiceStateMachine赋值
    ServiceContext::UniqueClient _dbClient;
    //指向上面的_dbClient
    const Client* _dbClientPtr;
    //由于worker线程会从ASIO的任务队列获取operation执行，存在一会儿做网络IO处理，一会儿做
    //业务逻辑处理，所以由两种线程名:conn-xx、worker-x,同一个线程需要做2种处理，因此需要记录旧的线程名
    //状态机线程名:conn-x
    const std::string _threadName;
    //之前的线程名，
    std::string _oldThreadName;

    //ServiceEntryPointImpl::startSession->ServiceStateMachine::setCleanupHook中设置赋值
    //session链接回收处理
    stdx::function<void()> _cleanupHook;

    //exhaust cursor是否启用
    //使用Exhaust类型的cursor，这样可以让mongo一批一批的返回查询结果，并且
    //在client请求之前把数据stream过来。
    bool _inExhaust = false;
    //如果启用了网络压缩，对应有一个compressorId
    boost::optional<MessageCompressorId> _compressorId;
    //接收处理的message信息
    Message _inMessage; //赋值见ServiceStateMachine::_sourceMessage

    //默认初始化kUnowned,标识本SSM状态机处于非活跃状态
    AtomicWord<Ownership> _owned{Ownership::kUnowned};
};
    状态机的几个状态由结构State管理，每个状态的意义及作用如下表所示：
TODO XXXX 需要补一个表上来
	状态机所有权由Ownership类管理，一个链接对应一个ssm，该ssm负责整个链接数据状态转换处理，
在adaptive动态线程池模式下，状态机ssm执行过程的不同阶段可能由不用的线程运行，因此需要标识ssm所属权归于那个线程，也就是当前ssm由那个线程运行




//命令模块基类基础接口初始化实现
class Command : public CommandInterface {
public:
    //获取集合名collection
    static std::string parseNsFullyQualified(...);
    //获取DB.COLLECTION
    static NamespaceString parseNsCollectionRequired(...);
    //map表结构
    using CommandMap = StringMap<Command*>;
    ......
    //获取命令名
    const std::string& getName() const final {
        return _name;
    }
    ......
    //应答保留填充字段长度
    std::size_t reserveBytesForReply() const override {
        return 0u;
    }
    //该命令是否只能在admin库执行，默认不可以
    bool adminOnly() const override {
        return false;
    }
    //该命令是否需要权限认证检查？默认不需要
    bool localHostOnlyIfNoAuth() override {
        return false;
    }
    //该命令执行后是否进行command操作计数
    bool shouldAffectCommandCounter() const override {
        return true;
    }
    //该命令是否需要认证
    bool requiresAuth() const override {
        return true;
    }
    //help帮助信息
    void help(std::stringstream& help) const override;
    //执行计划信息
    Status explain(...) const override;
    //日志信息相关
    void redactForLogging(mutablebson::Document* cmdObj) override;
    BSONObj getRedactedCopyForLogging(const BSONObj& cmdObj) override;
    //该命令是否为maintenance模式，默认false
    bool maintenanceMode() const override {
        return false;
    }
    //maintenance是否支持，默认支持
    bool maintenanceOk() const override {
        return true; 
    }
    //本地是否支持非本地ReadConcern,默认不支持
    bool supportsNonLocalReadConcern(...) const override {
        return false;
    }
    //是否允许AfterClusterTime，默认允许
    bool allowsAfterClusterTime(const BSONObj& cmdObj) const override {
        return true;
    }
    //3.6版本默认opCode=OP_MSG,所以对应逻辑操作op为LogicalOp::opCommand
    LogicalOp getLogicalOp() const override {
        return LogicalOp::opCommand;
    }
    //例如find就是kRead，update  delete insert就是kWrite，非读写操作就是kCommand
    ReadWriteType getReadWriteType() const override {
        return ReadWriteType::kCommand;
    }
    //该命令执行成功统计
    void incrementCommandsExecuted() final {
        _commandsExecuted.increment();
    }

    //该命令执行失败统计
    void incrementCommandsFailed() final {
        _commandsFailed.increment();
    }

    //真正得命令运行
    bool publicRun(OperationContext* opCtx, const OpMsgRequest& request, BSONObjBuilder& result);

    //获取支持的所有命令信息  ListCommandsCmd获取所有支持的命令 db.listCommands()
    static const CommandMap& allCommands() {
        return *_commands;
    }

    //没用
    static const CommandMap& allCommandsByBestName() {
        return *_commandsByBestName;
    }

    //收到不支持命令的统计，例如mongo shell敲一个mongodb无法识别得命令，这里就会统计出来
    static Counter64 unknownCommands;
    //根据命令字符串名查找对应命令
    static Command* findCommand(StringData name);
    //执行结果
    static void appendCommandStatus(...);
    //是否启用了command test功能
    static bool testCommandsEnabled;
    //help帮助信息
    static bool isHelpRequest(const BSONElement& helpElem);
    static const char kHelpFieldName[];
    //认证检查，检查是否有执行该命令得权限
    static Status checkAuthorization(Command* c,
                                     OperationContext* opCtx,
                                     const OpMsgRequest& request);
    ......
private:
    //添加地方见Command::Command(  
    //所有的command都在_commands中保存
    static CommandMap* _commands;
    //暂时没用
    static CommandMap* _commandsByBestName;
    //执行对应命令run接口
    virtual bool enhancedRun(OperationContext* opCtx,
                             const OpMsgRequest& request,
                             BSONObjBuilder& result) = 0;
    //db.serverStatus().metrics.commands命令查看，本命令的执行统计,包括执行成功和执行失败的
    Counter64 _commandsExecuted; 
    Counter64 _commandsFailed;
   //命令名，如"find" "insert" "update" "createIndexes" "deleteIndexes"
    const std::string _name;

    //每个命令执行是否成功通过MetricTree管理起来，也就是db.serverStatus().metrics.commands统计信息
    //通过MetricTree特殊二叉树管理起来
    ServerStatusMetricField<Counter64> _commandsExecutedMetric;
    ServerStatusMetricField<Counter64> _commandsFailedMetric;
};


class WriteCommandBase {  
public:
   //基类接口
   ......
   //mongodb字段验证规则（schema validation）
   bool _bypassDocumentValidation{false};
   //一次对多条数据进行插入或者删除或者更新的时候，前面的数据操作失败，是否继续后面的操作
   bool _ordered{true};
   //事务相关，等4.2版本回头分析
   boost::optional<std::vector<std::int32_t>> _stmtIds;
}

class Insert {  
public:
    ......
    //也就是db.collection
    NamespaceString _nss;
    //公共结构信息
    WriteCommandBase _writeCommandBase;
    //真正的文档在这里documents
    std::vector<mongo::BSONObj> _documents;
    //库信息
    std::string _dbName;
    //是否有documents
}

class Update { 
public:
      ......
    //db.collection信息，也就是库.表信息
    NamespaceString _nss;
    WriteCommandBase _writeCommandBase;
    //需要更新的具体内容在该成员中  一个OpMsgRequest中可以携带多个update
    std::vector<UpdateOpEntry> _updates;
}

class Delete {  
public:
    ......
    //DB.COLLECTION信息
    NamespaceString _nss;
    WriteCommandBase _writeCommandBase;
    //具体的delete内容在这里
    std::vector<DeleteOpEntry> _deletes;
}

Insert Insert::parse(const IDLParserErrorContext& ctxt, const BSONObj& bsonObject) {
    ......
    //调用Insert::parseProtected
    object.parseProtected(ctxt, bsonObject);
    return object;
}

void Insert::parseProtected(...)
{
    //解析出insert类的对应成员信息
    for (const auto& element :request.body) {
        const auto fieldName = element.fieldNameStringData();

        //解析bypassDocumentValidation信息
        if (fieldName == kBypassDocumentValidationFieldName) {
              ......
        }
        //解析ordered信息
        else if (fieldName == kOrderedFieldName) {
             ......
        }
        //解析stmtIds信息
        else if (fieldName == kStmtIdsFieldName) {
             ......
        }
        //解析需要插入的文档信息
        else if (fieldName == kDocumentsFieldName) {
           //解析的文档保持到_documents数组
            _documents = std::move(values);
        }
        //解析db名
        else if (fieldName == kDbNameFieldName) {
            ......
        }
        ......
    }
    //从request中解析出_writeCommandBase基础成员内容
    _writeCommandBase = WriteCommandBase::parse(ctxt, request.body);

    ......
    //根据db+collection构造出db.collection字符串
    _nss = ctxt.parseNSCollectionRequired(_dbName, commandElement);
}


//插入文档会走这里面  CmdInsert::runImpl
void runImpl(OperationContext* opCtx,
			 const OpMsgRequest& request,
			 BSONObjBuilder& result) final {
	//从request中解析出write_ops::Insert类成员信息
	const auto batch = InsertOp::parse(request);
    const auto reply = performInserts(opCtx, batch);
	......
}

WriteResult performInserts(OperationContext* opCtx, const write_ops::Insert& wholeOp) {
    .......
    //写入数据成功后的会掉处理
    //主要完成表级tps及时延统计
    ON_BLOCK_EXIT([&] {
	//performInserts执行完成后调用，记录执行结束时间 
        curOp.done();    
        //表级tps及时延统计
        Top::get(opCtx->getServiceContext())
            .record(...);

    });

    ......
    size_t bytesInBatch = 0;
    //batch数组
    std::vector<InsertStatement> batch; 
    //默认64,可以通过db.adminCommand( { setParameter: 1, internalInsertMaxBatchSize:xx } )配置
    const size_t maxBatchSize = internalInsertMaxBatchSize.load();
    //当写入的数据小于64时，也就是一个batch即可一起处理
    //batch最大限制为写入数据大于64或者batch中总字节数超过256K
    batch.reserve(std::min(wholeOp.getDocuments().size(), maxBatchSize));
    for (auto&& doc : wholeOp.getDocuments()) {
	......
	//doc检查，例如是否嵌套过多，是否一个doc带有多个_id等
        auto fixedDoc = fixDocumentForInsert(opCtx->getServiceContext(), doc);
	//如果这个文档检测有异常，则跳过这个文档，进行下一个文档操作
        if (!fixedDoc.isOK()) { 
            //啥也不做，直接忽略该doc
        } else {
            //事务相关，先忽略，以后会回头专门分析事务
            const auto stmtId = getStmtIdForWriteOp(opCtx, wholeOp, stmtIdIndex++);
            ......
	    //把文档插入到batch数组
            BSONObj toInsert = fixedDoc.getValue().isEmpty() ? doc : std::move(fixedDoc.getValue());
            batch.emplace_back(stmtId, toInsert);
            bytesInBatch += batch.back().doc.objsize();
	    //这里continue，就是为了把批量插入的文档组成到一个batch数组中，到达一定量一次性插入
	    //batch里面一次最多插入64个文档或者总字节数256K，则后续的数据拆分到下一个batch
            if (!isLastDoc && batch.size() < maxBatchSize && bytesInBatch < insertVectorMaxBytes)
                continue;  // Add more to batch before inserting.
        }

	//把本batch中的数据交由该接口统一处理
        bool canContinue = insertBatchAndHandleErrors(opCtx, wholeOp, batch, &lastOpFixer, &out);
	//清空batch，开始下一轮处理
        batch.clear();   
        bytesInBatch = 0;
	......
}



WriteResult performInserts(OperationContext* opCtx, const write_ops::Insert& wholeOp) {
    .......
	//写入数据成功后的会掉处理
	//主要完成表级tps及时延统计
    ON_BLOCK_EXIT([&] {
		//performInserts执行完成后调用，记录执行结束时间 
        curOp.done();    
        //表级tps及时延统计
        Top::get(opCtx->getServiceContext())
            .record(...);

    });

    ......
    size_t bytesInBatch = 0;
	//batch数组
    std::vector<InsertStatement> batch; 
    //默认64,可以通过db.adminCommand( { setParameter: 1, internalInsertMaxBatchSize:xx } )配置
    const size_t maxBatchSize = internalInsertMaxBatchSize.load();
	//当写入的数据小于64时，也就是一个batch即可一起处理
	//batch最大限制为写入数据大于64或者batch中总字节数超过256K
    batch.reserve(std::min(wholeOp.getDocuments().size(), maxBatchSize));
    for (auto&& doc : wholeOp.getDocuments()) {
		......
		//doc检查，例如是否嵌套过多，是否一个doc带有多个_id等
        auto fixedDoc = fixDocumentForInsert(opCtx->getServiceContext(), doc);
		//如果这个文档检测有异常，则跳过这个文档，进行下一个文档操作
        if (!fixedDoc.isOK()) { 
            //啥也不做，直接忽略该doc
        } else {
        	//事务相关，先忽略，以后会回头专门分析事务
            const auto stmtId = getStmtIdForWriteOp(opCtx, wholeOp, stmtIdIndex++);
            ......
			//把文档插入到batch数组
            BSONObj toInsert = fixedDoc.getValue().isEmpty() ? doc : std::move(fixedDoc.getValue());
            batch.emplace_back(stmtId, toInsert);
            bytesInBatch += batch.back().doc.objsize();
			//这里continue，就是为了把批量插入的文档组成到一个batch数组中，到达一定量一次性插入

			//batch里面一次最多插入64个文档或者总字节数256K，则后续的数据拆分到下一个batch
            if (!isLastDoc && batch.size() < maxBatchSize && bytesInBatch < insertVectorMaxBytes)
                continue;  // Add more to batch before inserting.
        }

		//把本batch中的数据交由该接口统一处理
        bool canContinue = insertBatchAndHandleErrors(opCtx, wholeOp, batch, &lastOpFixer, &out);
		//清空batch，开始下一轮处理
        batch.clear();   
        bytesInBatch = 0;
		......
}



bool insertBatchAndHandleErrors(...) {
    ......
    try {
	    //如果对应collection不存在则创建
        acquireCollection(); //执行上面定义的函数
        //如果collection不是固定capped集合，并且batch中数据大于一条
		//则试着在一个事务中一次性写入所有的数据
		if (!collection->getCollection()->isCapped() && batch.size() > 1) {  
			......
			//为什么这里没有检查返回值？默认全部成功？ 实际上通过try catch获取到异常后，再后续改为一条一条插入
            insertDocuments(opCtx, collection->getCollection(), batch.begin(), batch.end());
            //insert统计计数及返回值赋值
            globalOpCounters.gotInserts(batch.size());
            ......
            std::fill_n(std::back_inserter(out->results), batch.size(), std::move(result));
            curOp.debug().ninserted += batch.size();
			//一个事务写入多个doc成功，直接返回
            return true;
        }
    } catch (const DBException&) { //批量写入失败，则后面一条一条的写
        collection.reset();
		//注意这里没有return，在后续一条一个事务写入
    }

	//这里循环解析batch，实现一条数据一个在一个事务中处理
    for (auto it = batch.begin(); it != batch.end(); ++it) {
        globalOpCounters.gotInsert(); //insert操作计数
        try {
			//log() << "yang test ............getNamespace().ns():" << wholeOp.getNamespace().ns();
			//writeConflictRetry里面会执行{}中的函数体 
            writeConflictRetry(opCtx, "insert", wholeOp.getNamespace().ns(), [&] {
                try {
                    ......
                    //把该条文档插入  
                    insertDocuments(opCtx, collection->getCollection(), it, it + 1);
                    //统计计数处理
                    SingleWriteResult result;
                    result.setN(1);
                    out->results.emplace_back(std::move(result));
                    curOp.debug().ninserted++;
                } catch (...) {
                    // Release the lock following any error. Among other things, this ensures that
                    // we don't sleep in the WCE retry loop with the lock held.
                    collection.reset();
                    throw;
                }
            });
        } catch (const DBException& ex) {//写入异常
        	//注意这里，如果失败是否还可以继续后续数据的写入
            bool canContinue =
                handleError(opCtx, ex, wholeOp.getNamespace(), wholeOp.getWriteCommandBase(), out);
            if (!canContinue)
                return false; //注意这里直接退出循环，也就是本批次数据后续数据没有写入了
        }
    }

    return true;
}


void insertDocuments(OperationContext* opCtx,
                     Collection* collection,
                     std::vector<InsertStatement>::iterator begin,
                     std::vector<InsertStatement>::iterator end)
    //事务开始
    WriteUnitOfWork wuow(opCtx);
    ......
	//把数组begin到end之间的所有doc文档数据放入该事务中
    uassertStatusOK(collection->insertDocuments(
        opCtx, begin, end, &CurOp::get(opCtx)->debug(), /*enforceQuota*/ true));
	//事务结束
    wuow.commit(); //WriteUnitOfWork::commit
}


bool handleError(...) {
    ......

	//判断是什么原因引起的异常，从而返回不同的值
	//如果是isInterruption错误，直接返回true,意思是不需要后续数据写入
    if (ErrorCodes::isInterruption(ex.code())) {
		//如果是interrupt异常，则整批数据写失败，也就是不进行后续数据写入
        throw;  // These have always failed the whole batch.
    }

    ......
    //如果ordered为false则忽略这条写入失败的数据，继续后续数据写入
    return !wholeOp.getOrdered();
}


WriteResult performDeletes(...)
{
    ......

	//singleOp类型为DeleteOpEntry     write_ops::Delete::getDeletes
    for (auto&& singleOp : wholeOp.getDeletes()) {
		//事务相关，先跳过，以后相关章节专门分析
        const auto stmtId = getStmtIdForWriteOp(opCtx, wholeOp, stmtIdIndex++);
        if (opCtx->getTxnNumber()) {
            auto session = OperationContextSession::get(opCtx);
            if (session->checkStatementExecutedNoOplogEntryFetch(*opCtx->getTxnNumber(), stmtId)) {
                out.results.emplace_back(makeWriteResultForInsertOrDeleteRetry());
                continue;
            }
        }

        ......

		//该函数接口执行完后执行该finishCurOp
		//finishCurOp实现表级QPS及时延统计 本op操作的慢日志记录等
        ON_BLOCK_EXIT([&] { finishCurOp(opCtx, &curOp); });
        try {
            lastOpFixer.startingOp();
            out.results.emplace_back(
				//该delete op操作真正执行在这里，singleOp类型为DeleteOpEntry
                performSingleDeleteOp(opCtx, wholeOp.getNamespace(), stmtId, singleOp));
            lastOpFixer.finishedOpSuccessfully();
        } catch (const DBException& ex) {
            ......
    }

    return out;
}

static SingleWriteResult performSingleDeleteOp(...) {
    ......

	//根据ns构造DeleteReques
	//根据请求相关信息初始化赋值DeleteRequest
    DeleteRequest request(ns);
    request.setQuery(op.getQ());
    request.setCollation(write_ops::collationOf(op));
    request.setMulti(op.getMulti());
    request.setYieldPolicy(PlanExecutor::YIELD_AUTO);  // ParsedDelete overrides this for $isolated.
    request.setStmtId(stmtId);

	//根据DeleteRequest构造ParsedDelete
    ParsedDelete parsedDelete(opCtx, &request);
	//从request解析出对应成员存入parsedDelete
    uassertStatusOK(parsedDelete.parseRequest());
    //检查该请求是否已经被kill掉了
    opCtx->checkForInterrupt();

    ......
	//写必须走主节点判断及版本判断
    assertCanWrite_inlock(opCtx, ns);

	//从查询引擎中获取delete执行器
    auto exec = uassertStatusOK(
        getExecutorDelete(opCtx, &curOp.debug(), collection.getCollection(), &parsedDelete));

    {
        stdx::lock_guard<Client> lk(*opCtx->getClient());
        CurOp::get(opCtx)->setPlanSummary_inlock(Explain::getPlanSummary(exec.get()));
    }

	//运行该执行器
    uassertStatusOK(exec->executePlan());

	//下面流程是记录各种统计信息
    long long n = DeleteStage::getNumDeleted(*exec);
    curOp.debug().ndeleted = n;

    PlanSummaryStats summary;
	//获取执行器运行过程中的各种统计信息
    Explain::getSummaryStats(*exec, &summary);
    if (collection.getCollection()) {
        collection.getCollection()->infoCache()->notifyOfQuery(opCtx, summary.indexesUsed);
    }
    curOp.debug().setPlanSummaryMetrics(summary);
	//统计信息序列化
    if (curOp.shouldDBProfile()) {
        BSONObjBuilder execStatsBob;
        Explain::getWinningPlanStats(exec.get(), &execStatsBob);
        curOp.debug().execStats = execStatsBob.obj();
    }
    
	......
    return result;
}





ServiceContextMongoD::initializeGlobalStorageEngine() {
	//获取存储引擎，默认的WiredTiger存储引擎对应WiredTigerFactory
    const StorageEngine::Factory* factory = _storageFactories[storageGlobalParams.engine];
		//WiredTigerFactory::create  //根据params参数构造KVStorageEngine类
    _storageEngine = factory->create(storageGlobalParams, _lockFile.get());
    _storageEngine->finishInit(); //void KVStorageEngine::finishInit() {}
}



//定义一个全局的DatabaseHolder  
DatabaseHolder& dbHolderImpl() {
    static DatabaseHolder _dbHolder;
    return _dbHolder;
}

class DatabaseHolderImpl : public DatabaseHolder::Impl {
public:
	......
	
	//对应DatabaseImpl map表
    typedef StringMap<Database*> DBs;	
	//所有db保存到这里，通过DatabaseHolderImpl::openDb创建后保存到这里
    DBs _dbs; 
}




class CollectionImpl {
	......
	//表名
    const NamespaceString _ns;
    //表对应uuid，一个表对应一个唯一uuid
    OptionalCollectionUUID _uuid;
	
	......
	//数据行KV操作接口
	RecordStore* const _recordStore; 
	//索引行KV操作接口实现
	IndexCatalog _indexCatalog;
	
	//对应CollectionInfoCacheImpl，缓存查询计划信息
    CollectionInfoCache _infoCache;
}



class io_context
  : public execution_context
{
    typedef detail::io_context_impl impl_type;
	
	//和mongodb相关的核心接口如下
	std::size_t run_for(const chrono::duration<Rep, Period>& rel_time);
	ASIO_DECL count_type run(asio::error_code& ec);
	std::size_t run_one_for(const chrono::duration<Rep, Period>& rel_time);
	ASIO_DECL count_type poll(asio::error_code& ec);
	io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
	io_context::post(ASIO_MOVE_ARG(CompletionHandler) handler)
	
  // The implementation.
  //以上接口的具体实现实际上在该类中完成，也就是scheduler类
  impl_type& impl_;
}

io_context类中包含一个impl_成员，该成员类型impl_type实际上就是detail::io_context_impl，io_context_impl的来源定义如下：
typedef class scheduler io_context_impl;io_context.impl_对应scheduler类，类成员及其核心接口实现在scheduler.hpp和scheduler.ipp文件完成。

_acceptorIOContext这个io上下文结构用于处理accept相关请求，该上下文在mongodb服务层调用asio方式为_acceptorIOContext->run()和_acceptorIOContext->stop();;
   _acceptorIOContext->run();这两个接口的实现如下：
io_context::count_type io_context::run()
{
  asio::error_code ec;
  //scheduler::run调度处理
  count_type s = impl_.run(ec);
  asio::detail::throw_error(ec);
  return s;
}

_acceptorIOContext->stop()具体实现如下:
//io调度结束
void io_context::stop()
{
  //scheduler::stop
  impl_.stop();
}

_workerIOContext用于处理accept返回的新链接fd上面的数据读写服务，同时处理网络状态机的调度。asio实际上在实现的时候，把各种读写操作及其数据收发后的回调组装成各种不同的operation handler放入队列，然后由mongodb中创建的各种
工作线程来从队列中取出opration handler执行，见后面opration及调度相关实现。和_workerIOContext关联的几个主要opration出队相关操作接口如下：
std::size_t io_context::run_for(
    const chrono::duration<Rep, Period>& rel_time)
{
  //直接调用io_context::run_until，时间转换为当前时间之后rel_time这个时间点
  return this->run_until(chrono::steady_clock::now() + rel_time);
}


std::size_t io_context::run_until(
    const chrono::time_point<Clock, Duration>& abs_time)
{
  std::size_t n = 0;
  //注意这里是循环掉用run_one_until接口
  while (this->run_one_until(abs_time))
    if (n != (std::numeric_limits<std::size_t>::max)())
      ++n;
  return n;
}

std::size_t io_context::run_one_for(
    const chrono::duration<Rep, Period>& rel_time)
{ 
  //只调用一次
  return this->run_one_until(chrono::steady_clock::now() + rel_time);
}


std::size_t io_context::run_one_until(
    const chrono::time_point<Clock, Duration>& abs_time) //abs_time表示运行该函数到期的绝对事件点
{
  typename Clock::time_point now = Clock::now();
  while (now < abs_time)
  {
    //在本循环中还需要运行多久
    typename Clock::duration rel_time = abs_time - now;
	//最多一次循环执行1s，也就是schedule::wait_one最多单次wait 1s
    if (rel_time > chrono::seconds(1))
      rel_time = chrono::seconds(1);

    asio::error_code ec;
	//schedule::wait_one 从操作队列中获取对应的operation执行，如果获取到operation并执行成功则返回1，否则返回0
    std::size_t s = impl_.wait_one(
        static_cast<long>(chrono::duration_cast<
          chrono::microseconds>(rel_time).count()), ec);
    asio::detail::throw_error(ec);

	//已经获取执行完一个operation，则直接返回，否则继续循环调度schedule::wait_one获取operation执行
    if (s || impl_.stopped())
      return s;

	//重新计算当前时间
    now = Clock::now();
  }

  return 0;
}
	从上面的代码分析可以看出io_context::run_one_for()和io_context::run_for()两个接口最终都掉调用schedule类的wait_one接口。
schedule::wait_one接口的实现细节请参考下一节的schedule调度设计与实现
    io_context::run_one_for()和io_context::run_for()区别主要是run_one_for只调用run_one_until一次，而run_for会持续循环调用run_one_until处理operation操作。


和_workerIOContext关联的几个主要入队相关操作接口如下：
io_context::post(ASIO_MOVE_ARG(CompletionHandler) handler)
{
  //构造async_completion类，handler回调初始化到该类相应成员
  async_completion<CompletionHandler, void ()> init(handler);
  
  //mongodb  asio::async_read(), asio::async_write(), asio::async_connect(), 默认返回true。其他handler返回false
  bool is_continuation =   
    asio_handler_cont_helpers::is_continuation(init.completion_handler);

  // Allocate and construct an operation to wrap the handler.
  //构造complete handler,completion_handler继承基础operation 
  typedef detail::completion_handler<
    typename handler_type<CompletionHandler, void ()>::type> op;
  typename op::ptr p = { detail::addressof(init.completion_handler),
      op::ptr::allocate(init.completion_handler), 0 };
  p.p = new (p.v) op(init.completion_handler);

  //调用scheduler::post_immediate_completion，completion_handler传递给该函数
  impl_.post_immediate_completion(p.p, is_continuation);
  
}
另一个和mongodb入队相关的接口如下：
io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
{
  async_completion<CompletionHandler, void ()> init(handler);

  //scheduler::can_dispatch  本线程已经加入到了工作线程队列，则本handler直接由本线程运行
  if (impl_.can_dispatch())
  {
    detail::fenced_block b(detail::fenced_block::full);
	//直接运行handler
    asio_handler_invoke_helpers::invoke(
        init.completion_handler, init.completion_handler); 
  }
  else
  {
    // Allocate and construct an operation to wrap the handler.
    //构造complete handler,completion_handler继承基础operation 
    typedef detail::completion_handler<
      typename handler_type<CompletionHandler, void ()>::type> op;
    typename op::ptr p = { detail::addressof(init.completion_handler),
      op::ptr::allocate(init.completion_handler), 0 };
    p.p = new (p.v) op(init.completion_handler); //获取op操作，也就是handler回调
    ......
	
	//mongodb的ServiceExecutorAdaptive::schedule调用->io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
	//->scheduler::do_dispatch
	//scheduler::do_dispatch 
    impl_.do_dispatch(p.p); 
    p.v = p.p = 0;
  }
  ......
}


	从上面的分析可以看出，和mongodb直接相关的几个接口最终都是调用schedule类的相关接口，整个实现过程参考下一节scheduler调度实现模块。
总结： 
1. asio::io_context类中和mongodb直接相关的几个接口主要是run_for、run、run_one_for、poll、dispatch
2. 这些接口按照功能不同，可以分为入队型接口(poll、dispatch)和出队型接口(run_for、run、run_one_for)
3. 按照和io_context的关联性不同，可以分为accept相关io(_acceptorIOContext)处理的接口(run、stop)和新链接fd对应Io(_workerIOContext)数据收发相关处理及回调的接口(run_for、run_one_for、poll、dispatch)
4. io_context上下文的上述接口，除了dispatch在某些情况下直接运行handler外，其他接口最终都会间接调用scheduler调度类接口

scheduler多线程并发调度实现过程
    上一节的io_context上下文中提到mongodb操作的io上下文最终都会调用scheduler的几个核心接口。scheduler类主要工作在于完成任务调度，该类和mongodb相关的几个主要成员变量及接口如下：
class scheduler {
public:
  //reactor_op  completion_handler继承该类    
  //mongodb中operation分为两种，一种是completion_handler，另一种是reactor_op
  typedef scheduler_operation operation;
  
  //listen线程做accept异步处理的循环调度过程
  std::size_t scheduler::run(asio::error_code& ec)
  
  //全局任务入队相关的接口
  void scheduler::do_dispatch(scheduler::operation* op)
  
  //任务出队执行的，如果队列为空则获取epoll事件集对应的网络IO任务放入全局op_queue_队列
  std::size_t scheduler::wait_one(long usec, asio::error_code& ec)
  std::size_t scheduler::do_wait_one(mutex::scoped_lock& lock,
    scheduler::thread_info& this_thread, long usec, const asio::error_code& ec)
	
  //任务调度启停相关接口
  void scheduler::restart()
  void scheduler::stop_all_threads(mutex::scoped_lock& lock)
  
private:
  //全局锁，多线程互斥，对全局op_queue_队列做互斥
  mutable mutex mutex_;
  //全局任务队列，全局任务和网络事件相关任务都添加到该队列
  op_queue<operation> op_queue_;
  
  
  struct task_operation : operation //特殊operation
  {
	//这个特殊op的作用是保证从epoll_wait返回值中获取到一批网络事件对应的IO回调op接入全局队列op_queue_后，都加上该特殊op到队列中
	//下次如果从队列头部获取到该特殊Op操作，会继续获取epoll网络事件任务，避免网络IO长时间不被处理引起的"饥饿"状态
    task_operation() : operation(0) {}
  } task_operation_; 
  
  //stop_all_threads中置为true, 为true后，将不再处理epoll相关事件，参考scheduler::do_run_one
  bool stopped_;  
  
  //唤醒等待锁得线程(实际event由信号量封装)
  event wakeup_event_; 
  
  //也就是epoll_reactor，借助epoll实现网络事件异步处理
  reactor* task_; //epoll_reactor
  
  //套接字描述符个数：accept获取到的链接数fd个数+1(定时器fd)
  atomic_count outstanding_work_;
}


ASIO调度的任务(注: 每个任务对应一个operation类，operation类实现讲在后面章节详述)包括以下两种：1. 全局task任务  2. 网络IO事件处理相关任务(新链接、数据读取及其对应回调)，所有的任务通过一个全局队列链接在一起，并有序的调度执行。
1. 任务入队流程
    所有的任务都通过scheduler.op_queue_统一管理，和op_queue_全局队列任务入队相关的函数接口如下：
//全局状态任务入队
void scheduler::do_dispatch(
    scheduler::operation* op)
{
  work_started();
  //默认多线程，所以这里需要对队列加锁
  mutex::scoped_lock lock(mutex_);
  op_queue_.push(op);
  wake_one_thread_and_unlock(lock);
}

除了全局状态任务外，Asio还需要处理网络IO相关的任务，网络IO相关的任务入队相关接口如下：
struct scheduler::task_cleanup
{
  //scheduler::do_wait_one中获取到的epoll网络IO任务先入队到线程私有队列，在task_cleanup析构函数中一次性入队到全局队列
  ////将本线程的私有队列放入全局队列中，然后用task_operation_来标记一个线程私有队列的结束。
  //每次执行epoll_reactor::run去获取epoll对应的事件列表的时候都会执行该cleanup
  ~task_cleanup()
  {
    if (this_thread_->private_outstanding_work > 0)
    { //本线程上的私有任务后面会全部入队到全局op_queue_队列，全局任务数增加
      asio::detail::increment( 
          scheduler_->outstanding_work_,
          this_thread_->private_outstanding_work);
    }
    this_thread_->private_outstanding_work = 0;

    // Enqueue the completed operations and reinsert the task at the end of
    // the operation queue.
    lock_->lock();
	//将本线程的私有队列放入全局队列中，然后用task_operation_来标记一个线程私有队列的结束。
	//task_operation_标记一个线程私有队列的结束。
    scheduler_->task_interrupted_ = true;
    scheduler_->op_queue_.push(this_thread_->private_op_queue);

	//注意这里，加了一个特殊的op task_operation_到全局队列
    scheduler_->op_queue_.push(&scheduler_->task_operation_);
  }

  scheduler* scheduler_;
  //本作用域的锁
  mutex::scoped_lock* lock_;
  //线程私有信息
  thread_info* this_thread_;
};
    从上面的代码分析可以看出，当释放task_cleanup资源的时候，会把本线程私有private_op_queue队列上的所有网络IO相关任务一次性入队到全局op_queue_队列。

	
2. 任务出队执行
    空闲worker线程从op_queue_全局队列中获取任务执行，任务从全局队列出队流程相关接口如下:
std::size_t scheduler::wait_one(long usec, asio::error_code& ec)
{
  ec = asio::error_code();
  if (outstanding_work_ == 0) //如果连工作线程都没有，则说明没有调度的意义，停止所有调度
  {
    stop();
    return 0;
  }

  thread_info this_thread;
  this_thread.private_outstanding_work = 0;
  //线程入队到call_stack.top_链表
  thread_call_stack::context ctx(this, this_thread);
  //上锁
  mutex::scoped_lock lock(mutex_);

  //从操作队列中获取对应的operation执行，如果获取到operation并执行成功则返回1，否则返回0
  return do_wait_one(lock, this_thread, usec, ec);
}
    接口scheduler::wait_one最终调用scheduler::do_wait_one接口，该接口实现如下：
std::size_t scheduler::do_wait_one(...) //获取全局队列首的一个op执行
{
  //stop_all_threads中置为true, 为true后，将不再处理epoll相关事件，参考scheduler::do_run_one
  if (stopped_)
    return 0;
  //去队列头部任务
  operation* o = op_queue_.front();
  if (o == 0) //如果队列为空，则等待usec
  {
    //等待被唤醒
    wakeup_event_.clear(lock);
    wakeup_event_.wait_for_usec(lock, usec);
    usec = 0; // Wait at most once.
    //等一会儿后我们继续判断队列中是否有可执行的op
    o = op_queue_.front();
  }
  
  //该op是一个特殊的op, 说明需求去获取epoll上面的网络时间任务来处理，避免网络IO长期不被处理而处于饥饿状态
  if (o == &task_operation_) 
  {
    op_queue_.pop();
	//队列上还有其他等待执行的任务
    bool more_handlers = (!op_queue_.empty());

    task_interrupted_ = more_handlers;
    //既然队列上还有任务需要被调度执行，则通知其他线程可以继续获取队列任务执行了，这样可以提高并发
    if (more_handlers && !one_thread_)//默认mongodb多线程
      wakeup_event_.unlock_and_signal_one(lock); //唤醒其他工作线程
    else
      lock.unlock();

    {
	  //该类析构函数中会把下面从epoll获取的网络事件任务入队到全局scheduler.op_queue_队列
      task_cleanup on_exit = { this, &lock, &this_thread };
	  //函数exit的时候执行前面的on_exit析构函数从而把this_thread.private_op_queue入队到scheduler.op_queue_
      (void)on_exit;
      //scheduler::do_run_one->epoll_reactor::run
      //通过epoll获取所有得网络事件op入队到private_op_queue, 最终再通过scheduler::poll_one scheduler::poll入队到op_queue_
      //this_thread.private_op_queue队列成员的op类型为descriptor_state
	  task_->run(more_handlers ? 0 : usec, this_thread.private_op_queue);
    }
  }
  
  //没有任务可执行，直接返回
  if (o == 0)
    return 0;

  //先把该op任务出队
  op_queue_.pop();
  
  ......
  
  //执行取出的这个任务
  o->complete(this, ec, task_result);

  return 1;  
}

3. 任务队列
    从上面的分析可以看出，asio管理任务调度的队列包含两种：1. 全局任务队列op_queue_ 2. 工作线程私有队列private_op_queue
全局任务队列op_queue_结构参考前面的scheduler类，工作线程私有队列存放于scheduler_thread_info私有结构中，结构成员如下：
struct scheduler_thread_info : public thread_info_base
{
  //scheduler::do_wait_one->epoll_reactor::run中通过epoll获取对应网络事件任务
  //工作线程私有任务队列
  op_queue<scheduler_operation> private_op_queue; 
  //本线程私有private_op_queue队列中op任务数
  long private_outstanding_work;
};
在scheduler::do_wait_one接口中，工作线程遍历全局任务队列，如果队列头部的任务类型为task_operation特殊任务，则会调用epoll_reactor::run进行epoll
调度，通过epoll_wait获取到所有的网络IO事件任务，同时全部一次性入队到本线程的私有队列this_thread.private_op_queue。最后在task_cleanup析构函数中再一次性
从线程私有队列入队到全局队列，这样做的目的可以保证锁的粒度最小，同时性能最好，这也是设计一个线程私有队列的原因。

operation
    从前面的分析可以看出，一个任务对应一个operation类结构，asio中schduler调度的任务分为全局状态机任务和网络IO处理回调任务，这两种任务都有对应的回调函数。
此外，asio还有一种特殊的operation，该Operastion什么也不做，只是一个特殊标记。全局状态机任务、网络IO处理任务、特殊任务这三类任务分别对应、
reactor_op、task_operation_三个类实现，这三个类都会继承operation。
1. operation基类
    operation基类实际上就是scheduler_operation类，通过typedef scheduler_operation operation指定，是其他三个任务的父类，其主要实现接口如下：
class scheduler_operation  
{
public:
  typedef scheduler_operation operation_type;

  //执行func回调
  void complete(void* owner, const asio::error_code& ec,
      std::size_t bytes_transferred)
  {
    func_(owner, this, ec, bytes_transferred);
  }

protected:
  //初始化
  scheduler_operation(func_type func)
    : next_(0),
      func_(func),
      task_result_(0)
  {}

private:
  //reactor_op类(对应网络事件处理任务):epoll_reactor::descriptor_state::do_complete
  //completion_handler类(对应全局状态机任务):对应completion_handler::do_complete
  func_type func_;
protected:
  //本opration对应的scheduler
  friend class scheduler;
  //获取epoll_wait返回的event信息，赋值见set_ready_events add_ready_events
  //所有的网络事件通过task_result_位图记录，生效见epoll_reactor::descriptor_state::do_complete
  unsigned int task_result_; // Passed into bytes transferred.
};

//统一用scheduler_operation类型定义opration，外部使用Opration都用scheduler_operation统一代替
typedef scheduler_operation operation;


2. 全局状态机任务(completion_handler)
     当mongodb通过listener线程接受到一个新链接后，会生成一个状态机调度任务，然后入队到全局队列op_queue_，worker线程从全局队列获取到该任务后调度执行，从而
进入状态机调度流程，在该流程中会触发epoll相关得网络IO注册及异步IO处理。一个全局状态机任务对应一个completion_handler类，该类实现如下：
class completion_handler : public operation  
{
public:
  ASIO_DEFINE_HANDLER_PTR(completion_handler);
  //初始化
  completion_handler(Handler& h)
    //completion_handler对应的func
    : operation(&completion_handler::do_complete),  
	  //handler_赋值
      handler_(ASIO_MOVE_CAST(Handler)(h)) 
  {
    handler_work<Handler>::start(handler_); //标记任务开始执行
  }

  //真正执行在scheduler::do_wait_one->operation::complete->completion_handler::do_complete
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
      ......
      //获取对应handler执行
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN(());
      w.complete(handler, handler);
      ASIO_HANDLER_INVOCATION_END;
  }

private:
  //任务对应的回调函数
  Handler handler_;
}
    全局任务入队的时候有两种方式，一种是io_context::dispatch方式，另一种是io_context::post。从前面章节对这两个接口的代码分析可以看出，任务直接入队到全局队列
op_queue_中，然后工作线程通过scheduler::do_wait_one从队列获取该任务执行。
注意：任务入队由Listener线程完成，任务出队调度执行由mongodb工作线程执行。
	
3. 网络IO事件处理任务
    网络IO事件对应的Opration任务最终由reactor_op类实现，该类主要成员及接口如下：
class reactor_op   
  : public operation //也就是scheduler_operation
{
public:
  // The error code to be passed to the completion handler.
  asio::error_code ec_;

  // The number of bytes transferred, to be passed to the completion handler.
  //发送或者接收fd的数据量
  std::size_t bytes_transferred_;

  //发送或者接收fd数据的状态
  enum status { not_done, done, done_and_exhausted };

  // Perform the operation. Returns true if it is finished.
  status perform() //执行perform_func_
  { 
    //执行func，网络IO对应的func见epoll_reactor::descriptor_state::perform_io中调度执行
    return perform_func_(this);
  }

protected:
  typedef status (*perform_func_type)(reactor_op*);
  //例如accept操作过程回调过程，一个执行accept操作，一个执行后续的complete_func操作(mongodb中的task)
  //例如recvmsg操作过程，一个执行reactive_socket_recv_op_base::do_perform(最终recvmsg)，一个执行后续complete_func操作(mongodb中的task)
  reactor_op(perform_func_type perform_func, func_type complete_func)
  //例如接受数据的complete_func为reactive_socket_recv_op::do_complete
    : operation(complete_func),  //complete_func赋值给operation，在operation中执行
      bytes_transferred_(0),
      perform_func_(perform_func)
  {
  }

private:
  //perform_func也就是fd数据收发底层实现底层实现，赋值给reactor_op.perform_func_, 
  //complete_func赋值给父类operation的func,见reactor_op构造函数
  perform_func_type perform_func_;
};	
    从reactor_op类可以看出，该类的主要两个函数成员：perform_func_和complete_func。其中perform_func_函数主要负责异步网络IO底层处理，complete_func用于数据接收或者发送后的后续处理逻辑。
perform_func_具体功能如下：
1. 通过epoll事件集处理底层accept获取新连接fd。
2. fd上的数据异步接收
3. fd上的数据异步发送
	针对上面的三个网络IO处理功能，ASIO在实现的时候，分别通过三个不同的继承类(reactive_socket_accept_op_base、reactive_socket_recv_op_base、
reactive_socket_send_op_base)实现。
1. accept异步处理实现
    accept的异步接收过程，perform_func_底层IO实现主要通过reactive_socket_accept_op_base实现，具体实现方式如下：

//reactive_socket_service::async_accept中注册
class reactive_socket_accept_op_base : public reactor_op
{
public:
  //reactive_socket_accept_op中构造赋值
  reactive_socket_accept_op_base(socket_type socket,
      socket_ops::state_type state, Socket& peer, const Protocol& protocol,
      typename Protocol::endpoint* peer_endpoint, func_type complete_func)
      //初始化accept的底层IO处理接口及接收到新链接的后续处理接口
    : reactor_op(&reactive_socket_accept_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      peer_(peer),
      protocol_(protocol),
      peer_endpoint_(peer_endpoint),
      addrlen_(peer_endpoint ? peer_endpoint->capacity() : 0)
  {
  }

  //这里是底层的accept接收处理
  static status do_perform(reactor_op* base)
  {
    ......
    status result = socket_ops::non_blocking_accept(o->socket_,
        o->state_, o->peer_endpoint_ ? o->peer_endpoint_->data() : 0,
        o->peer_endpoint_ ? &o->addrlen_ : 0, o->ec_, new_socket)
    ? done : not_done;
    ......
	
    return result;
  }

  //在外层继承类reactive_socket_accept_op中的do_complete中执行
  //把接收到的新链接new_socket_注册到epoll事件集中，这样就可以异步收发该fd的数据
  void do_assign()  
  {
       ......
	  //reactive_socket_service_base::do_assign,这里面把new_socket_注册到epoll事件集
      peer_.assign(protocol_, new_socket_.get(), ec_);
	  ......
  }

private:
  //sock套接字sd
  socket_type socket_; 
  //accept获取到的新链接fd，见reactive_socket_accept_op_base::do_perform
  socket_holder new_socket_;
  //客户端地址信息记录在这里面
  Socket& peer_; 
  typename Protocol::endpoint* peer_endpoint_;
};
    complete_func用于处理accept获取到新链接new_socket_后的后续处理，后续处理通过reactive_socket_accept_op继承类实现，如下：
class reactive_socket_accept_op :
  public reactive_socket_accept_op_base<Socket, Protocol>
{
public:
  //初始化构造
  reactive_socket_accept_op(socket_type socket,
      socket_ops::state_type state, Socket& peer, const Protocol& protocol,
      typename Protocol::endpoint* peer_endpoint, Handler& handler)
    : reactive_socket_accept_op_base<Socket, Protocol>(socket, state, peer,
        protocol, peer_endpoint, &reactive_socket_accept_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }

  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
    ......
	//执行前面的新连接epoll事件集注册reactive_socket_accept_op_base::do_assign
    if (owner)
	//reactive_socket_accept_op::do_assign
      o->do_assign();

    .......
	//执行accept后回调，也就是mongodb中的ServiceEntryPointImpl::startSession
    w.complete(handler, handler.handler_);
    ASIO_HANDLER_INVOCATION_END;
	......
  }

private:
  //接收到新链接的回调，mongodb中对应ServiceEntryPointImpl::startSession
  Handler handler_;
};
    complete_func函数接口主要事accept新链接后的处理，包括新连接new_sock注册到epoll事件集，这样该新链接的数据收发就可以异步实现。此外，
还会执行mongodb服务层的handler回调，也就是ServiceEntryPointImpl::startSession。

2. 网络数据接收异步处理实现
    网络数据异步接收过程，perform_func_底层IO实现主要通过reactive_socket_recv_op_base实现，具体实现方式如下：    
class reactive_socket_recv_op_base : public reactor_op
{
public:
  //初始化构造
  reactive_socket_recv_op_base(socket_type socket,
      socket_ops::state_type state, const MutableBufferSequence& buffers,
      socket_base::message_flags flags, func_type complete_func)
	//网络数据接收及其后续处理
    : reactor_op(&reactive_socket_recv_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      buffers_(buffers),
      flags_(flags)
  {
  }

  static status do_perform(reactor_op* base)
  {
    reactive_socket_recv_op_base* o(
        static_cast<reactive_socket_recv_op_base*>(base));
	......
	//异步接收数据存入bufs中
    status result = socket_ops::non_blocking_recv(o->socket_,
        bufs.buffers(), bufs.count(), o->flags_,
        (o->state_ & socket_ops::stream_oriented) != 0,
        o->ec_, o->bytes_transferred_) ? done : not_done;
	......
	
    return result;
  }

private:
  //套接字
  socket_type socket_;
  //存储读取的数据的buf,里面需要读取数据的长度
  MutableBufferSequence buffers_;
};
    接收到数据后的handler回调在reactive_socket_recv_op类中实现，该类继承reactive_socket_recv_op_base，其具体实现如下:
class reactive_socket_recv_op :
  public reactive_socket_recv_op_base<MutableBufferSequence>
{
public:
  //初始化构造
  reactive_socket_recv_op(socket_type socket,
      socket_ops::state_type state, const MutableBufferSequence& buffers,
      socket_base::message_flags flags, Handler& handler)
	//handler及do_complete赋值
    : reactive_socket_recv_op_base<MutableBufferSequence>(socket, state,
        buffers, flags, &reactive_socket_recv_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }

  //读取到一个完整的mongo数据后，回调在这里执行，实际上是工作线程从队列获取op执行的，见
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
	  ......
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN((handler.arg1_, handler.arg2_));
	  //执行对应handler
      w.complete(handler, handler.handler_);
      ASIO_HANDLER_INVOCATION_END;
	  ......
  }

private:
  //mongodb接收数据的handler对应TransportLayerASIO::ASIOSourceTicket::_headerCallback(接收到mongo协议头部对应的handler)、
  //TransportLayerASIO::ASIOSourceTicket::_bodyCallback(接收到包体部分的handler)
  Handler handler_;
};
3. 网络数据异步发送处理实现
	网络数据异步发送过程和接收过程类似，perform_func_底层IO数据发送实现主要通过reactive_socket_send_op_base实现，具体实现方式如下：   
class reactive_socket_send_op_base : public reactor_op
{
public:
  //初始化构造
  reactive_socket_send_op_base(socket_type socket,
      socket_ops::state_type state, const ConstBufferSequence& buffers,
      socket_base::message_flags flags, func_type complete_func)
	//初始化do_perform和complete_func
    : reactor_op(&reactive_socket_send_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      buffers_(buffers),
      flags_(flags)
  {
  }
  
  //数据异步发送的底层实现
  static status do_perform(reactor_op* base)
  {
	......
    status result = socket_ops::non_blocking_send(o->socket_,
          bufs.buffers(), bufs.count(), o->flags_,
          o->ec_, o->bytes_transferred_) ? done : not_done;
	......

    return result;
  }

private:
  //fd
  socket_type socket_;
  //发送的数据在该buffer中
  ConstBufferSequence buffers_;
}
    发送完buffers_数据后的handler回调在reactive_socket_send_op类中实现，该类继承reactive_socket_send_op_base，其具体实现如下:
class reactive_socket_send_op :
  public reactive_socket_send_op_base<ConstBufferSequence>
{
public:
  //初始化构造
  reactive_socket_send_op(socket_type socket,
      socket_ops::state_type state, const ConstBufferSequence& buffers,
      socket_base::message_flags flags, Handler& handler)
    : reactive_socket_send_op_base<ConstBufferSequence>(socket,
        state, buffers, flags, &reactive_socket_send_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }
  
  //数据发送完成后的handler回调处理
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
	  ......
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN((handler.arg1_, handler.arg2_));
      w.complete(handler, handler.handler_);
      ASIO_HANDLER_INVOCATION_END;
      ......
  }

private:
  //mongodb异步数据发送成功后的handler回调对应TransportLayerASIO::ASIOSinkTicket::_sinkCallback
  Handler handler_;
};
4. 总结
    从上面的网络IO任务的实现可以看出，整个实现过程都是大同小异的，都有底层fd的处理及其对应的handler回调处理，整个逻辑比较清晰。

epoll_reactor模式底层实现
    mongodb默认使用epoll方式来实现异步网络IO事件处理，epoll网上资料比较多，可以参考：https://en.wikipedia.org/wiki/Epoll。
本文只分析和ASIO实现密切关联的一些接口。
1. epoll_reactor类实现
class epoll_reactor 
  : public execution_context_service_base<epoll_reactor>
{
public:
  //任务操作类型
  enum op_types {  
  	read_op = 0,    //对应读事件
    write_op = 1,   //对应写事件
    connect_op = 1, //对应connect事件
    except_op = 2,  //异常事件
    max_ops = 3     //类型最大值
  };

  //新链接new_sock对应的私有描述符信息，用于
  class descriptor_state : operation  
  {
    //并发锁
    mutex mutex_;
	//本descriptor_state所属的epoll reactor
    epoll_reactor* reactor_;
	//新链接对应的套接字描述符
    int descriptor_;  
	//epoll_wait获取到的事件集位图
    uint32_t registered_events_;
	
	//套接字描述符对应的读、写、accept及异常事件对应的Opration任务分别入队到各自的数组队列中
	//op_queue_[I],i也就是对应上面的op_types
    op_queue<reactor_op> op_queue_[max_ops]; 
    bool try_speculative_[max_ops];
	//epoll_reactor::deregister_descriptor置为true，套接字描述符注册掉
    bool shutdown_;

	//descriptor_state初始化构造
	ASIO_DECL descriptor_state(bool locking);
	//epoll事件集对应的位图信息，每个位置1表示对应网络事件发生
	//这些位图上的事件处理在epoll_reactor::descriptor_state::do_complete
    void set_ready_events(uint32_t events) { task_result_ = events; }
    void add_ready_events(uint32_t events) { task_result_ |= events; }

	//对应accept、读、写事件的底层Io处理，实现见reactive_socket_accept_op_base	
	//reactive_socket_recv_op_base reactive_socket_send_op_base
	ASIO_DECL operation* perform_io(uint32_t events);
	//descriptor_state对应的do_complete为epoll_reactor::descriptor_state::do_complete
    ASIO_DECL static void do_complete(
        void* owner, operation* base,
        const asio::error_code& ec, std::size_t bytes_transferred);
  }
  
  //给descriptor_state起一个别名per_descriptor_data
  typedef descriptor_state* per_descriptor_data;
  //构造epoll reactor
  ASIO_DECL epoll_reactor(asio::execution_context& ctx);
  //新链接描述符fd关注的所有读、写、异常等事件注册到epoll事件集中，当对应事件到达通过epoll_wait返回回去即可
  ASIO_DECL int register_descriptor(socket_type descriptor,
      per_descriptor_data& descriptor_data);
  //把读、写操作的回调注册到描述符对应私有信息队列descriptor_data.op_queue_[]中
  void epoll_reactor::start_op(int op_type, socket_type descriptor,
  epoll_reactor::per_descriptor_data& descriptor_data, reactor_op* op,
  bool is_continuation, bool allow_speculative)
    
  //epoll_reactr对应的scheduler_
  scheduler& scheduler_;
  //事件私有信息保护锁
  mutex mutex_;
  //timer fd，定时器对应的fd，本章不做定时器实现描述
  int timer_fd_;
  //timer队列集
  timer_queue_set timer_queues_;
  
  //所有链接描述符私有信息都存入该poll池中，用于资源统一的分配和释放
  object_pool<descriptor_state> registered_descriptors_;
  //registered_descriptors_并发锁控制
  mutex registered_descriptors_mutex_;
}
   从上面对epoll_reactor类的分析，可以看出，所有链接通过descriptor_state管理私有结构，该结构
负责链接上所有的底层epoll事件收集与处理。descriptor_state通过一个op_queue_数组队列来把不同reactor类型的操作记录
到各自不同的队列中，这样可以更好的管理。
   
2. 主要接口成员实现
   epoll reactor类接口实现在epoll_reactor.ipp文件中实现，主要接口包括：初始化、描述符注册、各自事件回调注册、描述符注销、
epoll调度执行、底层IO处理、IO处理后的回调实现等
2.1 epoll相关实现
epoll_reactor::epoll_reactor(asio::execution_context& ctx) 
{
   //初始化构造，代码比较简单
}  

//把链接描述符关注的几个常用事件注册到epoll
int epoll_reactor::register_descriptor(socket_type descriptor,
    epoll_reactor::per_descriptor_data& descriptor_data) //套接字 回调等相关信息
{
  //获取一个描述符descriptor_state信息，分配对应空间
  descriptor_data = allocate_descriptor_state();

  ASIO_HANDLER_REACTOR_REGISTRATION((
        context(), static_cast<uintmax_t>(descriptor),
        reinterpret_cast<uintmax_t>(descriptor_data)));

  {
    mutex::scoped_lock descriptor_lock(descriptor_data->mutex_);

	//下面对descriptor_data进行相应的赋值
    descriptor_data->reactor_ = this;
    descriptor_data->descriptor_ = descriptor;
    descriptor_data->shutdown_ = false;
    for (int i = 0; i < max_ops; ++i)
      descriptor_data->try_speculative_[i] = true;
  }

  epoll_event ev = { 0, { 0 } };
  //同时把这些事件添加到epoll事件集，表示关注这些事件，注意这里是边沿触发
  ev.events = EPOLLIN | EPOLLERR | EPOLLHUP | EPOLLPRI | EPOLLET; 
  descriptor_data->registered_events_ = ev.events; 
  //赋值记录到ev.data.ptr中，当对应网络事件到底执行回调的时候可以通过该指针获取descriptor_data
  //descriptor_data记录到该指针，epoll_reactor::run中通过对应事件获取该私有信息
  ev.data.ptr = descriptor_data; 
  //通过epoll_ctl把events添加到事件集，当对应事件发生，epoll_wait可以获取对应事件
  int result = epoll_ctl(epoll_fd_, EPOLL_CTL_ADD, descriptor, &ev);
  ......

  return 0;
}

//1. 对描述符关注的epoll事件做跟新
//2. 把reactor_op对应的网络IO相关opration任务入队到descriptor_data对应的私有队列信息
void epoll_reactor::start_op(int op_type, socket_type descriptor,
    epoll_reactor::per_descriptor_data& descriptor_data, reactor_op* op,
    bool is_continuation, bool allow_speculative)
{
  ......
  
  //如果reactor_op还没有加入对应的op_queue_[i]队列，则EPOLL_CTL_MOD跟新epoll对应事件信息
  if (descriptor_data->op_queue_[op_type].empty())
  {
    ......
    {
	  //如果是写类型，添加EPOLLOUT进去
      if (op_type == write_op)
      {
        descriptor_data->registered_events_ |= EPOLLOUT;
      }
 
      epoll_event ev = { 0, { 0 } };
      ev.events = descriptor_data->registered_events_;
      ev.data.ptr = descriptor_data;
	  //再次跟新一下事件
      epoll_ctl(epoll_fd_, EPOLL_CTL_MOD, descriptor, &ev);
    }
  }
  //把任务回调opration入队到descriptor_data的对应队列
  descriptor_data->op_queue_[op_type].push(op);
  //scheduler::work_started  实际上就是链接数
  scheduler_.work_started();
}

//epoll异步IO事件处理流程
void epoll_reactor::run(long usec, op_queue<operation>& ops) //ops队列内容为descriptor_state
{
  ......
  epoll_event events[128];
  //epoll_wait获取到IO事件后返回，或者超时事件内没有对应网络IO事件，也返回
  int num_events = epoll_wait(epoll_fd_, events, 128, timeout);
  ......
  //遍历获取对应的事件信息
  for (int i = 0; i < num_events; ++i)
  {
      void* ptr = events[i].data.ptr;

	  //等待完成之后，我们开始分发事件:

      unsigned event_mask = 0;
	  //accept事件、网络数据到达事件
      if ((events[i].events & EPOLLIN) != 0)
        event_mask |= ASIO_HANDLER_REACTOR_READ_EVENT;
	  //写事件
      if ((events[i].events & EPOLLOUT))
        event_mask |= ASIO_HANDLER_REACTOR_WRITE_EVENT;
	  //异常事件
      if ((events[i].events & (EPOLLERR | EPOLLHUP)) != 0)
        event_mask |= ASIO_HANDLER_REACTOR_ERROR_EVENT;
      ASIO_HANDLER_REACTOR_EVENTS((context(),
            reinterpret_cast<uintmax_t>(ptr), event_mask));

  }

  // Dispatch the waiting events.
  //IO事件类型有三种:interrupt,timer和普通的IO事件
  for (int i = 0; i < num_events; ++i)
  {
      //该事件对应的私有信息指针，通过该指针就可以获取到对应的descriptor_data，该结构记录了对应的读写react_op
      void* ptr = events[i].data.ptr;

      //通过ptr获取对应descriptor_data信息
      descriptor_state* descriptor_data = static_cast<descriptor_state*>(ptr);
      if (!ops.is_enqueued(descriptor_data))  //不在队列中，则添加
      {
        //对应事件位图置位
        descriptor_data->set_ready_events(events[i].events);
		//epoll operation对应的回调函数是epoll_reactor::descriptor_state::do_complete
        ops.push(descriptor_data); 
      }
      else  //descriptor_data已经在ops的队列中了，对应事件位图置位
      {
        descriptor_data->add_ready_events(events[i].events);
      }
    }
  }
  ......
}

2.2 react_op对应网络IO事件处理流程
    前面章节已经知道，ASIO的网络IO处理及其回调都是通过reactor_op的几个基础类中赋值，其真正的调度执行通过
以下几个接口实现：
//初始化一个描述符信息descriptor_state
epoll_reactor::descriptor_state::descriptor_state(bool locking)
  //这里可以看出opration对应的func为descriptor_state::do_complete
  : operation(&epoll_reactor::descriptor_state::do_complete),
    mutex_(locking)
{
}
当scheduler::do_wait_one中获取operation执行complete的时候，最终会调用该descriptor_state::do_complete执行，该接口实现如下：
void epoll_reactor::descriptor_state::do_complete(
    void* owner, operation* base,
    const asio::error_code& ec, std::size_t bytes_transferred)
{
  if (owner)
  {
    descriptor_state* descriptor_data = static_cast<descriptor_state*>(base);
	//事件位图
    uint32_t events = static_cast<uint32_t>(bytes_transferred); 
	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
    if (operation* op = descriptor_data->perform_io(events)) 
    {
      //执行complete_func, 也就是reactive_socket_accept_op_base  reactive_socket_recv_op_base reactive_socket_send_op_base  
      //这三个IO操作对应的complete_func回调

	  //注意这里只执行了网络IO事件任务中的一个，其他的在perform_io中入队到全局队列中了，等待其他线程执行
      op->complete(owner, ec, 0);
    }
  }
}
    descriptor_state::do_complete最终会调用descriptor_state::perform_io执行，perform_io实现如下：
//网络IO相关的任务处理，如accept接收新链接、接收数据、发送数据
operation* epoll_reactor::descriptor_state::perform_io(uint32_t events)
{
  //上锁
  mutex_.lock();
  //这里构造一个perform_io_cleanup_on_block_exit类，用于后续析构时候的收尾处理
  perform_io_cleanup_on_block_exit io_cleanup(reactor_);
  mutex::scoped_lock descriptor_lock(mutex_, mutex::scoped_lock::adopt_lock);


  //分别对应新链接到达或者数据来临、可以写数据、有紧急的数据可读(这里应该表示有带外数据到来)
  static const int flag[max_ops] = { EPOLLIN, EPOLLOUT, EPOLLPRI };
  //循环处理如下三种类型的reactor_op:
  // 1. reactive_socket_accept_op_base
  // 2. reactive_socket_recv_op_base
  // 3. reactive_socket_send_op_base    
  for (int j = max_ops - 1; j >= 0; --j)
  {
    //有读写事件、或者epoll_wait有获取到异常，如链接断开等
    if (events & (flag[j] | EPOLLERR | EPOLLHUP)) 
    {
      try_speculative_[j] = true;
	  //遍历对应数组成员的整个队列
      while (reactor_op* op = op_queue_[j].front())
      {
       //reactive_socket_accept_op_base::do_perform  reactive_socket_recv_op_base::do_perform
  //reactive_socket_send_op_base::do_perform  分别对应新链接，读取数据，发送数据的底层实现
  	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
        if (reactor_op::status status = op->perform()) 
		//status为true表示成功，false表示底层处理失败
        {
          op_queue_[j].pop();
          io_cleanup.ops_.push(op);
          if (status == reactor_op::done_and_exhausted)
          {
            try_speculative_[j] = false;
            break;
          }
        }
		//例如如果没读到一个完整的数据，则这里直接退出，继续循环处理其他网络数据，下次继续读取。
        else
          break;
      }
    }
  }

  // The first operation will be returned for completion now. The others will
  // be posted for later by the io_cleanup object's destructor.
   //这里只返回了第一个op，其他的op在~perform_io_cleanup_on_block_exit处理
  io_cleanup.first_op_ = io_cleanup.ops_.front();
  //把返回的第一个从队列中清除，剩余的op还在队列中
  io_cleanup.ops_.pop(); 

  //该逻辑的总体思路：队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行

  //只返回第一个op,外层的epoll_reactor::descriptor_state::do_complete中执行该op对应的complete
  return io_cleanup.first_op_;
}

//epoll_reactor::descriptor_state::do_complete执行
//网络IO相关的任务处理，如accept接收新链接、接收数据、发送数据
operation* epoll_reactor::descriptor_state::perform_io(uint32_t events)
{
  ......
  
  //分别对应链接到达或者数据来临、可以写数据、有紧急的数据可读(这里应该表示有带外数据到来)
  static const int flag[max_ops] = { EPOLLIN, EPOLLOUT, EPOLLPRI };
  //循环处理各自不同的reactor_op(reactive_socket_accept_op_base   reactive_socket_recv_op_base reactive_socket_send_op_base)
  for (int j = max_ops - 1; j >= 0; --j)
  {
    if (events & (flag[j] | EPOLLERR | EPOLLHUP)) //有读写事件、或者epoll_wait有获取到异常，如链接断开等
    {
      try_speculative_[j] = true;
      while (reactor_op* op = op_queue_[j].front())
      {
       //reactive_socket_accept_op_base::do_perform  reactive_socket_recv_op_base::do_perform
    //reactive_socket_send_op_base::do_perform  分别对应新链接，读取数据，发送数据的底层实现
  	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
        if (reactor_op::status status = op->perform()) 
		//status为true表示成功，false表示底层处理失败
        {
          //取出对应的op，入队到临时队列io_cleanup.ops_
          op_queue_[j].pop();
          io_cleanup.ops_.push(op);
          if (status == reactor_op::done_and_exhausted)
          {
            try_speculative_[j] = false;
            break;
          }
        }
		//如果有异常
        else
          break;
      }
    }
  }

   //这里只返回了第一个op，其他的op在~perform_io_cleanup_on_block_exit处理
  io_cleanup.first_op_ = io_cleanup.ops_.front();
  //把返回的第一个从队列中清除，剩余的op还在队列中
  io_cleanup.ops_.pop(); 

  //该逻辑的总体思路：队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行

  //只返回第一个op,外层的epoll_reactor::descriptor_state::do_complete中执行该op对应的complete
  return io_cleanup.first_op_;
}
    从上面的分析可以看出，op_queue_队列上的所有reactor_op的底层IO处理成功后，会把该reactor_op入队到临时io_cleanup.first_op_队列中，
函数结束的时候把队首的reactor_op取出然后返回，在外层的do_complete中直接执行op->complete(也就是reactor_op中的complete_func，分别对应accetp、读、写中的
reactive_socket_accept_op_base::do_perform、reactive_socket_accept_op_base::do_perform、reactive_socket_recv_op_base reactive_socket_send_op_base::do_perform  )。
    那么这里有个问题？临时队列的其他reactor_op是如何处理的呢？其他reactor_op的处理实际上由perform_io_cleanup_on_block_exit析构函数处理，下面看下该析构函数
实现：

struct epoll_reactor::perform_io_cleanup_on_block_exit
{
 //初始化构造
  explicit perform_io_cleanup_on_block_exit(epoll_reactor* r)
    : reactor_(r), first_op_(0)
  {
  }
  
  //epoll_reactor::descriptor_state::perform_io中后续收尾处理
  ~perform_io_cleanup_on_block_exit()
  { 
    //配合epoll_reactor::descriptor_state::perform_io阅读，可以看出第一个op由本线程获取，其他op放入到了ops_队列
    //队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行
	if (first_op_)
    {
      //队列不为空
      if (!ops_.empty())
	  	//scheduler::post_deferred_completions，op任务放入全局队列，延迟执行
        reactor_->scheduler_.post_deferred_completions(ops_);
    }
    else
    {
	  //队首的线程由本线程处理，其他op任务放入全局任务队列, 见epoll_reactor::descriptor_state::do_complete
      reactor_->scheduler_.compensating_work_started();
    }
  }
  
  //所属epoll_reactor
  epoll_reactor* reactor_;
  //临时队列
  op_queue<operation> ops_;
  //标记队首位置
  operation* first_op_;
};








存储引擎注册识别区分流程：
ServiceContextMongoD._storageFactories

void ServiceContextMongoD::registerStorageEngine(const std::string& name,
                                                 const StorageEngine::Factory* factory) {
    // No double-registering.
    invariant(0 == _storageFactories.count(name));

    // Some sanity checks: the factory must exist,
    invariant(factory);

    // and all factories should be added before we pick a storage engine.
    invariant(NULL == _storageEngine);

    _storageFactories[name] = factory;
}


MONGO_INITIALIZER_WITH_PREREQUISITES(DevNullEngineInit, ("SetGlobalEnvironment"))
(InitializerContext* context) {
    getGlobalServiceContext()->registerStorageEngine("devnull", new DevNullStorageEngineFactory());------对应DevNullKVEngine
    return Status::OK();
}
}


MONGO_INITIALIZER_WITH_PREREQUISITES(MMAPV1EngineInit, ("SetGlobalEnvironment"))
(InitializerContext* context) {
    getGlobalServiceContext()->registerStorageEngine("mmapv1", new MMAPV1Factory());   --------对应MMAPV1Engine
    return Status::OK();
}

MONGO_INITIALIZER_WITH_PREREQUISITES(WiredTigerEngineInit, ("SetGlobalEnvironment"))  ---------WiredTigerKVEngine
(InitializerContext* context) {
    getGlobalServiceContext()->registerStorageEngine(kWiredTigerEngineName,
                                                     new WiredTigerFactory());

    return Status::OK();
}

ServiceContextMongoD::initializeGlobalStorageEngine() {
	//获取存储引擎，默认的WiredTiger存储引擎对应WiredTigerFactory
    const StorageEngine::Factory* factory = _storageFactories[storageGlobalParams.engine];
		//WiredTigerFactory::create  //根据params参数构造KVStorageEngine类
    _storageEngine = factory->create(storageGlobalParams, _lockFile.get());
    _storageEngine->finishInit(); //void KVStorageEngine::finishInit() {}
}



//定义一个全局的DatabaseHolder  
DatabaseHolder& dbHolderImpl() {
    static DatabaseHolder _dbHolder;
    return _dbHolder;
}

class DatabaseHolderImpl : public DatabaseHolder::Impl {
public:
	......
	
	//对应DatabaseImpl map表
    typedef StringMap<Database*> DBs;	
	//所有db保存到这里，通过DatabaseHolderImpl::openDb创建后保存到这里
    DBs _dbs; 
}




class CollectionImpl {
	......
	//表名
    const NamespaceString _ns;
    //表对应uuid，一个表对应一个唯一uuid
    OptionalCollectionUUID _uuid;
	
	......
	//数据行KV操作接口
	RecordStore* const _recordStore; 
	//索引行KV操作接口实现
	IndexCatalog _indexCatalog;
	
	//对应CollectionInfoCacheImpl，缓存查询计划信息
    CollectionInfoCache _infoCache;
}



class io_context
  : public execution_context
{
    typedef detail::io_context_impl impl_type;
	
	//和mongodb相关的核心接口如下
	std::size_t run_for(const chrono::duration<Rep, Period>& rel_time);
	ASIO_DECL count_type run(asio::error_code& ec);
	std::size_t run_one_for(const chrono::duration<Rep, Period>& rel_time);
	ASIO_DECL count_type poll(asio::error_code& ec);
	io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
	io_context::post(ASIO_MOVE_ARG(CompletionHandler) handler)
	
  // The implementation.
  //以上接口的具体实现实际上在该类中完成，也就是scheduler类
  impl_type& impl_;
}

io_context类中包含一个impl_成员，该成员类型impl_type实际上就是detail::io_context_impl，io_context_impl的来源定义如下：
typedef class scheduler io_context_impl;io_context.impl_对应scheduler类，类成员及其核心接口实现在scheduler.hpp和scheduler.ipp文件完成。

_acceptorIOContext这个io上下文结构用于处理accept相关请求，该上下文在mongodb服务层调用asio方式为_acceptorIOContext->run()和_acceptorIOContext->stop();;
   _acceptorIOContext->run();这两个接口的实现如下：
io_context::count_type io_context::run()
{
  asio::error_code ec;
  //scheduler::run调度处理
  count_type s = impl_.run(ec);
  asio::detail::throw_error(ec);
  return s;
}

_acceptorIOContext->stop()具体实现如下:
//io调度结束
void io_context::stop()
{
  //scheduler::stop
  impl_.stop();
}

_workerIOContext用于处理accept返回的新链接fd上面的数据读写服务，同时处理网络状态机的调度。asio实际上在实现的时候，把各种读写操作及其数据收发后的回调组装成各种不同的operation handler放入队列，然后由mongodb中创建的各种
工作线程来从队列中取出opration handler执行，见后面opration及调度相关实现。和_workerIOContext关联的几个主要opration出队相关操作接口如下：
std::size_t io_context::run_for(
    const chrono::duration<Rep, Period>& rel_time)
{
  //直接调用io_context::run_until，时间转换为当前时间之后rel_time这个时间点
  return this->run_until(chrono::steady_clock::now() + rel_time);
}


std::size_t io_context::run_until(
    const chrono::time_point<Clock, Duration>& abs_time)
{
  std::size_t n = 0;
  //注意这里是循环掉用run_one_until接口
  while (this->run_one_until(abs_time))
    if (n != (std::numeric_limits<std::size_t>::max)())
      ++n;
  return n;
}

std::size_t io_context::run_one_for(
    const chrono::duration<Rep, Period>& rel_time)
{ 
  //只调用一次
  return this->run_one_until(chrono::steady_clock::now() + rel_time);
}


std::size_t io_context::run_one_until(
    const chrono::time_point<Clock, Duration>& abs_time) //abs_time表示运行该函数到期的绝对事件点
{
  typename Clock::time_point now = Clock::now();
  while (now < abs_time)
  {
    //在本循环中还需要运行多久
    typename Clock::duration rel_time = abs_time - now;
	//最多一次循环执行1s，也就是schedule::wait_one最多单次wait 1s
    if (rel_time > chrono::seconds(1))
      rel_time = chrono::seconds(1);

    asio::error_code ec;
	//schedule::wait_one 从操作队列中获取对应的operation执行，如果获取到operation并执行成功则返回1，否则返回0
    std::size_t s = impl_.wait_one(
        static_cast<long>(chrono::duration_cast<
          chrono::microseconds>(rel_time).count()), ec);
    asio::detail::throw_error(ec);

	//已经获取执行完一个operation，则直接返回，否则继续循环调度schedule::wait_one获取operation执行
    if (s || impl_.stopped())
      return s;

	//重新计算当前时间
    now = Clock::now();
  }

  return 0;
}
	从上面的代码分析可以看出io_context::run_one_for()和io_context::run_for()两个接口最终都掉调用schedule类的wait_one接口。
schedule::wait_one接口的实现细节请参考下一节的schedule调度设计与实现
    io_context::run_one_for()和io_context::run_for()区别主要是run_one_for只调用run_one_until一次，而run_for会持续循环调用run_one_until处理operation操作。


和_workerIOContext关联的几个主要入队相关操作接口如下：
io_context::post(ASIO_MOVE_ARG(CompletionHandler) handler)
{
  //构造async_completion类，handler回调初始化到该类相应成员
  async_completion<CompletionHandler, void ()> init(handler);
  
  //mongodb  asio::async_read(), asio::async_write(), asio::async_connect(), 默认返回true。其他handler返回false
  bool is_continuation =   
    asio_handler_cont_helpers::is_continuation(init.completion_handler);

  // Allocate and construct an operation to wrap the handler.
  //构造complete handler,completion_handler继承基础operation 
  typedef detail::completion_handler<
    typename handler_type<CompletionHandler, void ()>::type> op;
  typename op::ptr p = { detail::addressof(init.completion_handler),
      op::ptr::allocate(init.completion_handler), 0 };
  p.p = new (p.v) op(init.completion_handler);

  //调用scheduler::post_immediate_completion，completion_handler传递给该函数
  impl_.post_immediate_completion(p.p, is_continuation);
  
}
另一个和mongodb入队相关的接口如下：
io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
{
  async_completion<CompletionHandler, void ()> init(handler);

  //scheduler::can_dispatch  本线程已经加入到了工作线程队列，则本handler直接由本线程运行
  if (impl_.can_dispatch())
  {
    detail::fenced_block b(detail::fenced_block::full);
	//直接运行handler
    asio_handler_invoke_helpers::invoke(
        init.completion_handler, init.completion_handler); 
  }
  else
  {
    // Allocate and construct an operation to wrap the handler.
    //构造complete handler,completion_handler继承基础operation 
    typedef detail::completion_handler<
      typename handler_type<CompletionHandler, void ()>::type> op;
    typename op::ptr p = { detail::addressof(init.completion_handler),
      op::ptr::allocate(init.completion_handler), 0 };
    p.p = new (p.v) op(init.completion_handler); //获取op操作，也就是handler回调
    ......
	
	//mongodb的ServiceExecutorAdaptive::schedule调用->io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
	//->scheduler::do_dispatch
	//scheduler::do_dispatch 
    impl_.do_dispatch(p.p); 
    p.v = p.p = 0;
  }
  ......
}


	从上面的分析可以看出，和mongodb直接相关的几个接口最终都是调用schedule类的相关接口，整个实现过程参考下一节scheduler调度实现模块。
总结： 
1. asio::io_context类中和mongodb直接相关的几个接口主要是run_for、run、run_one_for、poll、dispatch
2. 这些接口按照功能不同，可以分为入队型接口(poll、dispatch)和出队型接口(run_for、run、run_one_for)
3. 按照和io_context的关联性不同，可以分为accept相关io(_acceptorIOContext)处理的接口(run、stop)和新链接fd对应Io(_workerIOContext)数据收发相关处理及回调的接口(run_for、run_one_for、poll、dispatch)
4. io_context上下文的上述接口，除了dispatch在某些情况下直接运行handler外，其他接口最终都会间接调用scheduler调度类接口

scheduler多线程并发调度实现过程
    上一节的io_context上下文中提到mongodb操作的io上下文最终都会调用scheduler的几个核心接口。scheduler类主要工作在于完成任务调度，该类和mongodb相关的几个主要成员变量及接口如下：
class scheduler {
public:
  //reactor_op  completion_handler继承该类    
  //mongodb中operation分为两种，一种是completion_handler，另一种是reactor_op
  typedef scheduler_operation operation;
  
  //listen线程做accept异步处理的循环调度过程
  std::size_t scheduler::run(asio::error_code& ec)
  
  //全局任务入队相关的接口
  void scheduler::do_dispatch(scheduler::operation* op)
  
  //任务出队执行的，如果队列为空则获取epoll事件集对应的网络IO任务放入全局op_queue_队列
  std::size_t scheduler::wait_one(long usec, asio::error_code& ec)
  std::size_t scheduler::do_wait_one(mutex::scoped_lock& lock,
    scheduler::thread_info& this_thread, long usec, const asio::error_code& ec)
	
  //任务调度启停相关接口
  void scheduler::restart()
  void scheduler::stop_all_threads(mutex::scoped_lock& lock)
  
private:
  //全局锁，多线程互斥，对全局op_queue_队列做互斥
  mutable mutex mutex_;
  //全局任务队列，全局任务和网络事件相关任务都添加到该队列
  op_queue<operation> op_queue_;
  
  
  struct task_operation : operation //特殊operation
  {
	//这个特殊op的作用是保证从epoll_wait返回值中获取到一批网络事件对应的IO回调op接入全局队列op_queue_后，都加上该特殊op到队列中
	//下次如果从队列头部获取到该特殊Op操作，会继续获取epoll网络事件任务，避免网络IO长时间不被处理引起的"饥饿"状态
    task_operation() : operation(0) {}
  } task_operation_; 
  
  //stop_all_threads中置为true, 为true后，将不再处理epoll相关事件，参考scheduler::do_run_one
  bool stopped_;  
  
  //唤醒等待锁得线程(实际event由信号量封装)
  event wakeup_event_; 
  
  //也就是epoll_reactor，借助epoll实现网络事件异步处理
  reactor* task_; //epoll_reactor
  
  //套接字描述符个数：accept获取到的链接数fd个数+1(定时器fd)
  atomic_count outstanding_work_;
}


ASIO调度的任务(注: 每个任务对应一个operation类，operation类实现讲在后面章节详述)包括以下两种：1. 全局task任务  2. 网络IO事件处理相关任务(新链接、数据读取及其对应回调)，所有的任务通过一个全局队列链接在一起，并有序的调度执行。
1. 任务入队流程
    所有的任务都通过scheduler.op_queue_统一管理，和op_queue_全局队列任务入队相关的函数接口如下：
//全局状态任务入队
void scheduler::do_dispatch(
    scheduler::operation* op)
{
  work_started();
  //默认多线程，所以这里需要对队列加锁
  mutex::scoped_lock lock(mutex_);
  op_queue_.push(op);
  wake_one_thread_and_unlock(lock);
}

除了全局状态任务外，Asio还需要处理网络IO相关的任务，网络IO相关的任务入队相关接口如下：
struct scheduler::task_cleanup
{
  //scheduler::do_wait_one中获取到的epoll网络IO任务先入队到线程私有队列，在task_cleanup析构函数中一次性入队到全局队列
  ////将本线程的私有队列放入全局队列中，然后用task_operation_来标记一个线程私有队列的结束。
  //每次执行epoll_reactor::run去获取epoll对应的事件列表的时候都会执行该cleanup
  ~task_cleanup()
  {
    if (this_thread_->private_outstanding_work > 0)
    { //本线程上的私有任务后面会全部入队到全局op_queue_队列，全局任务数增加
      asio::detail::increment( 
          scheduler_->outstanding_work_,
          this_thread_->private_outstanding_work);
    }
    this_thread_->private_outstanding_work = 0;

    // Enqueue the completed operations and reinsert the task at the end of
    // the operation queue.
    lock_->lock();
	//将本线程的私有队列放入全局队列中，然后用task_operation_来标记一个线程私有队列的结束。
	//task_operation_标记一个线程私有队列的结束。
    scheduler_->task_interrupted_ = true;
    scheduler_->op_queue_.push(this_thread_->private_op_queue);

	//注意这里，加了一个特殊的op task_operation_到全局队列
    scheduler_->op_queue_.push(&scheduler_->task_operation_);
  }

  scheduler* scheduler_;
  //本作用域的锁
  mutex::scoped_lock* lock_;
  //线程私有信息
  thread_info* this_thread_;
};
    从上面的代码分析可以看出，当释放task_cleanup资源的时候，会把本线程私有private_op_queue队列上的所有网络IO相关任务一次性入队到全局op_queue_队列。

	
2. 任务出队执行
    空闲worker线程从op_queue_全局队列中获取任务执行，任务从全局队列出队流程相关接口如下:
std::size_t scheduler::wait_one(long usec, asio::error_code& ec)
{
  ec = asio::error_code();
  if (outstanding_work_ == 0) //如果连工作线程都没有，则说明没有调度的意义，停止所有调度
  {
    stop();
    return 0;
  }

  thread_info this_thread;
  this_thread.private_outstanding_work = 0;
  //线程入队到call_stack.top_链表
  thread_call_stack::context ctx(this, this_thread);
  //上锁
  mutex::scoped_lock lock(mutex_);

  //从操作队列中获取对应的operation执行，如果获取到operation并执行成功则返回1，否则返回0
  return do_wait_one(lock, this_thread, usec, ec);
}
    接口scheduler::wait_one最终调用scheduler::do_wait_one接口，该接口实现如下：
std::size_t scheduler::do_wait_one(...) //获取全局队列首的一个op执行
{
  //stop_all_threads中置为true, 为true后，将不再处理epoll相关事件，参考scheduler::do_run_one
  if (stopped_)
    return 0;
  //去队列头部任务
  operation* o = op_queue_.front();
  if (o == 0) //如果队列为空，则等待usec
  {
    //等待被唤醒
    wakeup_event_.clear(lock);
    wakeup_event_.wait_for_usec(lock, usec);
    usec = 0; // Wait at most once.
    //等一会儿后我们继续判断队列中是否有可执行的op
    o = op_queue_.front();
  }
  
  //该op是一个特殊的op, 说明需求去获取epoll上面的网络时间任务来处理，避免网络IO长期不被处理而处于饥饿状态
  if (o == &task_operation_) 
  {
    op_queue_.pop();
	//队列上还有其他等待执行的任务
    bool more_handlers = (!op_queue_.empty());

    task_interrupted_ = more_handlers;
    //既然队列上还有任务需要被调度执行，则通知其他线程可以继续获取队列任务执行了，这样可以提高并发
    if (more_handlers && !one_thread_)//默认mongodb多线程
      wakeup_event_.unlock_and_signal_one(lock); //唤醒其他工作线程
    else
      lock.unlock();

    {
	  //该类析构函数中会把下面从epoll获取的网络事件任务入队到全局scheduler.op_queue_队列
      task_cleanup on_exit = { this, &lock, &this_thread };
	  //函数exit的时候执行前面的on_exit析构函数从而把this_thread.private_op_queue入队到scheduler.op_queue_
      (void)on_exit;
      //scheduler::do_run_one->epoll_reactor::run
      //通过epoll获取所有得网络事件op入队到private_op_queue, 最终再通过scheduler::poll_one scheduler::poll入队到op_queue_
      //this_thread.private_op_queue队列成员的op类型为descriptor_state
	  task_->run(more_handlers ? 0 : usec, this_thread.private_op_queue);
    }
  }
  
  //没有任务可执行，直接返回
  if (o == 0)
    return 0;

  //先把该op任务出队
  op_queue_.pop();
  
  ......
  
  //执行取出的这个任务
  o->complete(this, ec, task_result);

  return 1;  
}

3. 任务队列
    从上面的分析可以看出，asio管理任务调度的队列包含两种：1. 全局任务队列op_queue_ 2. 工作线程私有队列private_op_queue
全局任务队列op_queue_结构参考前面的scheduler类，工作线程私有队列存放于scheduler_thread_info私有结构中，结构成员如下：
struct scheduler_thread_info : public thread_info_base
{
  //scheduler::do_wait_one->epoll_reactor::run中通过epoll获取对应网络事件任务
  //工作线程私有任务队列
  op_queue<scheduler_operation> private_op_queue; 
  //本线程私有private_op_queue队列中op任务数
  long private_outstanding_work;
};
在scheduler::do_wait_one接口中，工作线程遍历全局任务队列，如果队列头部的任务类型为task_operation特殊任务，则会调用epoll_reactor::run进行epoll
调度，通过epoll_wait获取到所有的网络IO事件任务，同时全部一次性入队到本线程的私有队列this_thread.private_op_queue。最后在task_cleanup析构函数中再一次性
从线程私有队列入队到全局队列，这样做的目的可以保证锁的粒度最小，同时性能最好，这也是设计一个线程私有队列的原因。

operation
    从前面的分析可以看出，一个任务对应一个operation类结构，asio中schduler调度的任务分为全局状态机任务和网络IO处理回调任务，这两种任务都有对应的回调函数。
此外，asio还有一种特殊的operation，该Operastion什么也不做，只是一个特殊标记。全局状态机任务、网络IO处理任务、特殊任务这三类任务分别对应、
reactor_op、task_operation_三个类实现，这三个类都会继承operation。
1. operation基类
    operation基类实际上就是scheduler_operation类，通过typedef scheduler_operation operation指定，是其他三个任务的父类，其主要实现接口如下：
class scheduler_operation  
{
public:
  typedef scheduler_operation operation_type;

  //执行func回调
  void complete(void* owner, const asio::error_code& ec,
      std::size_t bytes_transferred)
  {
    func_(owner, this, ec, bytes_transferred);
  }

protected:
  //初始化
  scheduler_operation(func_type func)
    : next_(0),
      func_(func),
      task_result_(0)
  {}

private:
  //reactor_op类(对应网络事件处理任务):epoll_reactor::descriptor_state::do_complete
  //completion_handler类(对应全局状态机任务):对应completion_handler::do_complete
  func_type func_;
protected:
  //本opration对应的scheduler
  friend class scheduler;
  //获取epoll_wait返回的event信息，赋值见set_ready_events add_ready_events
  //所有的网络事件通过task_result_位图记录，生效见epoll_reactor::descriptor_state::do_complete
  unsigned int task_result_; // Passed into bytes transferred.
};

//统一用scheduler_operation类型定义opration，外部使用Opration都用scheduler_operation统一代替
typedef scheduler_operation operation;


2. 全局状态机任务(completion_handler)
     当mongodb通过listener线程接受到一个新链接后，会生成一个状态机调度任务，然后入队到全局队列op_queue_，worker线程从全局队列获取到该任务后调度执行，从而
进入状态机调度流程，在该流程中会触发epoll相关得网络IO注册及异步IO处理。一个全局状态机任务对应一个completion_handler类，该类实现如下：
class completion_handler : public operation  
{
public:
  ASIO_DEFINE_HANDLER_PTR(completion_handler);
  //初始化
  completion_handler(Handler& h)
    //completion_handler对应的func
    : operation(&completion_handler::do_complete),  
	  //handler_赋值
      handler_(ASIO_MOVE_CAST(Handler)(h)) 
  {
    handler_work<Handler>::start(handler_); //标记任务开始执行
  }

  //真正执行在scheduler::do_wait_one->operation::complete->completion_handler::do_complete
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
      ......
      //获取对应handler执行
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN(());
      w.complete(handler, handler);
      ASIO_HANDLER_INVOCATION_END;
  }

private:
  //任务对应的回调函数
  Handler handler_;
}
    全局任务入队的时候有两种方式，一种是io_context::dispatch方式，另一种是io_context::post。从前面章节对这两个接口的代码分析可以看出，任务直接入队到全局队列
op_queue_中，然后工作线程通过scheduler::do_wait_one从队列获取该任务执行。
注意：任务入队由Listener线程完成，任务出队调度执行由mongodb工作线程执行。
	
3. 网络IO事件处理任务
    网络IO事件对应的Opration任务最终由reactor_op类实现，该类主要成员及接口如下：
class reactor_op   
  : public operation //也就是scheduler_operation
{
public:
  // The error code to be passed to the completion handler.
  asio::error_code ec_;

  // The number of bytes transferred, to be passed to the completion handler.
  //发送或者接收fd的数据量
  std::size_t bytes_transferred_;

  //发送或者接收fd数据的状态
  enum status { not_done, done, done_and_exhausted };

  // Perform the operation. Returns true if it is finished.
  status perform() //执行perform_func_
  { 
    //执行func，网络IO对应的func见epoll_reactor::descriptor_state::perform_io中调度执行
    return perform_func_(this);
  }

protected:
  typedef status (*perform_func_type)(reactor_op*);
  //例如accept操作过程回调过程，一个执行accept操作，一个执行后续的complete_func操作(mongodb中的task)
  //例如recvmsg操作过程，一个执行reactive_socket_recv_op_base::do_perform(最终recvmsg)，一个执行后续complete_func操作(mongodb中的task)
  reactor_op(perform_func_type perform_func, func_type complete_func)
  //例如接受数据的complete_func为reactive_socket_recv_op::do_complete
    : operation(complete_func),  //complete_func赋值给operation，在operation中执行
      bytes_transferred_(0),
      perform_func_(perform_func)
  {
  }

private:
  //perform_func也就是fd数据收发底层实现底层实现，赋值给reactor_op.perform_func_, 
  //complete_func赋值给父类operation的func,见reactor_op构造函数
  perform_func_type perform_func_;
};	
    从reactor_op类可以看出，该类的主要两个函数成员：perform_func_和complete_func。其中perform_func_函数主要负责异步网络IO底层处理，complete_func用于数据接收或者发送后的后续处理逻辑。
perform_func_具体功能如下：
1. 通过epoll事件集处理底层accept获取新连接fd。
2. fd上的数据异步接收
3. fd上的数据异步发送
	针对上面的三个网络IO处理功能，ASIO在实现的时候，分别通过三个不同的继承类(reactive_socket_accept_op_base、reactive_socket_recv_op_base、
reactive_socket_send_op_base)实现。
1. accept异步处理实现
    accept的异步接收过程，perform_func_底层IO实现主要通过reactive_socket_accept_op_base实现，具体实现方式如下：

//reactive_socket_service::async_accept中注册
class reactive_socket_accept_op_base : public reactor_op
{
public:
  //reactive_socket_accept_op中构造赋值
  reactive_socket_accept_op_base(socket_type socket,
      socket_ops::state_type state, Socket& peer, const Protocol& protocol,
      typename Protocol::endpoint* peer_endpoint, func_type complete_func)
      //初始化accept的底层IO处理接口及接收到新链接的后续处理接口
    : reactor_op(&reactive_socket_accept_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      peer_(peer),
      protocol_(protocol),
      peer_endpoint_(peer_endpoint),
      addrlen_(peer_endpoint ? peer_endpoint->capacity() : 0)
  {
  }

  //这里是底层的accept接收处理
  static status do_perform(reactor_op* base)
  {
    ......
    status result = socket_ops::non_blocking_accept(o->socket_,
        o->state_, o->peer_endpoint_ ? o->peer_endpoint_->data() : 0,
        o->peer_endpoint_ ? &o->addrlen_ : 0, o->ec_, new_socket)
    ? done : not_done;
    ......
	
    return result;
  }

  //在外层继承类reactive_socket_accept_op中的do_complete中执行
  //把接收到的新链接new_socket_注册到epoll事件集中，这样就可以异步收发该fd的数据
  void do_assign()  
  {
       ......
	  //reactive_socket_service_base::do_assign,这里面把new_socket_注册到epoll事件集
      peer_.assign(protocol_, new_socket_.get(), ec_);
	  ......
  }

private:
  //sock套接字sd
  socket_type socket_; 
  //accept获取到的新链接fd，见reactive_socket_accept_op_base::do_perform
  socket_holder new_socket_;
  //客户端地址信息记录在这里面
  Socket& peer_; 
  typename Protocol::endpoint* peer_endpoint_;
};
    complete_func用于处理accept获取到新链接new_socket_后的后续处理，后续处理通过reactive_socket_accept_op继承类实现，如下：
class reactive_socket_accept_op :
  public reactive_socket_accept_op_base<Socket, Protocol>
{
public:
  //初始化构造
  reactive_socket_accept_op(socket_type socket,
      socket_ops::state_type state, Socket& peer, const Protocol& protocol,
      typename Protocol::endpoint* peer_endpoint, Handler& handler)
    : reactive_socket_accept_op_base<Socket, Protocol>(socket, state, peer,
        protocol, peer_endpoint, &reactive_socket_accept_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }

  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
    ......
	//执行前面的新连接epoll事件集注册reactive_socket_accept_op_base::do_assign
    if (owner)
	//reactive_socket_accept_op::do_assign
      o->do_assign();

    .......
	//执行accept后回调，也就是mongodb中的ServiceEntryPointImpl::startSession
    w.complete(handler, handler.handler_);
    ASIO_HANDLER_INVOCATION_END;
	......
  }

private:
  //接收到新链接的回调，mongodb中对应ServiceEntryPointImpl::startSession
  Handler handler_;
};
    complete_func函数接口主要事accept新链接后的处理，包括新连接new_sock注册到epoll事件集，这样该新链接的数据收发就可以异步实现。此外，
还会执行mongodb服务层的handler回调，也就是ServiceEntryPointImpl::startSession。

2. 网络数据接收异步处理实现
    网络数据异步接收过程，perform_func_底层IO实现主要通过reactive_socket_recv_op_base实现，具体实现方式如下：    
class reactive_socket_recv_op_base : public reactor_op
{
public:
  //初始化构造
  reactive_socket_recv_op_base(socket_type socket,
      socket_ops::state_type state, const MutableBufferSequence& buffers,
      socket_base::message_flags flags, func_type complete_func)
	//网络数据接收及其后续处理
    : reactor_op(&reactive_socket_recv_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      buffers_(buffers),
      flags_(flags)
  {
  }

  static status do_perform(reactor_op* base)
  {
    reactive_socket_recv_op_base* o(
        static_cast<reactive_socket_recv_op_base*>(base));
	......
	//异步接收数据存入bufs中
    status result = socket_ops::non_blocking_recv(o->socket_,
        bufs.buffers(), bufs.count(), o->flags_,
        (o->state_ & socket_ops::stream_oriented) != 0,
        o->ec_, o->bytes_transferred_) ? done : not_done;
	......
	
    return result;
  }

private:
  //套接字
  socket_type socket_;
  //存储读取的数据的buf,里面需要读取数据的长度
  MutableBufferSequence buffers_;
};
    接收到数据后的handler回调在reactive_socket_recv_op类中实现，该类继承reactive_socket_recv_op_base，其具体实现如下:
class reactive_socket_recv_op :
  public reactive_socket_recv_op_base<MutableBufferSequence>
{
public:
  //初始化构造
  reactive_socket_recv_op(socket_type socket,
      socket_ops::state_type state, const MutableBufferSequence& buffers,
      socket_base::message_flags flags, Handler& handler)
	//handler及do_complete赋值
    : reactive_socket_recv_op_base<MutableBufferSequence>(socket, state,
        buffers, flags, &reactive_socket_recv_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }

  //读取到一个完整的mongo数据后，回调在这里执行，实际上是工作线程从队列获取op执行的，见
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
	  ......
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN((handler.arg1_, handler.arg2_));
	  //执行对应handler
      w.complete(handler, handler.handler_);
      ASIO_HANDLER_INVOCATION_END;
	  ......
  }

private:
  //mongodb接收数据的handler对应TransportLayerASIO::ASIOSourceTicket::_headerCallback(接收到mongo协议头部对应的handler)、
  //TransportLayerASIO::ASIOSourceTicket::_bodyCallback(接收到包体部分的handler)
  Handler handler_;
};
3. 网络数据异步发送处理实现
	网络数据异步发送过程和接收过程类似，perform_func_底层IO数据发送实现主要通过reactive_socket_send_op_base实现，具体实现方式如下：   
class reactive_socket_send_op_base : public reactor_op
{
public:
  //初始化构造
  reactive_socket_send_op_base(socket_type socket,
      socket_ops::state_type state, const ConstBufferSequence& buffers,
      socket_base::message_flags flags, func_type complete_func)
	//初始化do_perform和complete_func
    : reactor_op(&reactive_socket_send_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      buffers_(buffers),
      flags_(flags)
  {
  }
  
  //数据异步发送的底层实现
  static status do_perform(reactor_op* base)
  {
	......
    status result = socket_ops::non_blocking_send(o->socket_,
          bufs.buffers(), bufs.count(), o->flags_,
          o->ec_, o->bytes_transferred_) ? done : not_done;
	......

    return result;
  }

private:
  //fd
  socket_type socket_;
  //发送的数据在该buffer中
  ConstBufferSequence buffers_;
}
    发送完buffers_数据后的handler回调在reactive_socket_send_op类中实现，该类继承reactive_socket_send_op_base，其具体实现如下:
class reactive_socket_send_op :
  public reactive_socket_send_op_base<ConstBufferSequence>
{
public:
  //初始化构造
  reactive_socket_send_op(socket_type socket,
      socket_ops::state_type state, const ConstBufferSequence& buffers,
      socket_base::message_flags flags, Handler& handler)
    : reactive_socket_send_op_base<ConstBufferSequence>(socket,
        state, buffers, flags, &reactive_socket_send_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }
  
  //数据发送完成后的handler回调处理
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
	  ......
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN((handler.arg1_, handler.arg2_));
      w.complete(handler, handler.handler_);
      ASIO_HANDLER_INVOCATION_END;
      ......
  }

private:
  //mongodb异步数据发送成功后的handler回调对应TransportLayerASIO::ASIOSinkTicket::_sinkCallback
  Handler handler_;
};
4. 总结
    从上面的网络IO任务的实现可以看出，整个实现过程都是大同小异的，都有底层fd的处理及其对应的handler回调处理，整个逻辑比较清晰。

epoll_reactor模式底层实现
    mongodb默认使用epoll方式来实现异步网络IO事件处理，epoll网上资料比较多，可以参考：https://en.wikipedia.org/wiki/Epoll。
本文只分析和ASIO实现密切关联的一些接口。
1. epoll_reactor类实现
class epoll_reactor 
  : public execution_context_service_base<epoll_reactor>
{
public:
  //任务操作类型
  enum op_types {  
  	read_op = 0,    //对应读事件
    write_op = 1,   //对应写事件
    connect_op = 1, //对应connect事件
    except_op = 2,  //异常事件
    max_ops = 3     //类型最大值
  };

  //新链接new_sock对应的私有描述符信息，用于
  class descriptor_state : operation  
  {
    //并发锁
    mutex mutex_;
	//本descriptor_state所属的epoll reactor
    epoll_reactor* reactor_;
	//新链接对应的套接字描述符
    int descriptor_;  
	//epoll_wait获取到的事件集位图
    uint32_t registered_events_;
	
	//套接字描述符对应的读、写、accept及异常事件对应的Opration任务分别入队到各自的数组队列中
	//op_queue_[I],i也就是对应上面的op_types
    op_queue<reactor_op> op_queue_[max_ops]; 
    bool try_speculative_[max_ops];
	//epoll_reactor::deregister_descriptor置为true，套接字描述符注册掉
    bool shutdown_;

	//descriptor_state初始化构造
	ASIO_DECL descriptor_state(bool locking);
	//epoll事件集对应的位图信息，每个位置1表示对应网络事件发生
	//这些位图上的事件处理在epoll_reactor::descriptor_state::do_complete
    void set_ready_events(uint32_t events) { task_result_ = events; }
    void add_ready_events(uint32_t events) { task_result_ |= events; }

	//对应accept、读、写事件的底层Io处理，实现见reactive_socket_accept_op_base	
	//reactive_socket_recv_op_base reactive_socket_send_op_base
	ASIO_DECL operation* perform_io(uint32_t events);
	//descriptor_state对应的do_complete为epoll_reactor::descriptor_state::do_complete
    ASIO_DECL static void do_complete(
        void* owner, operation* base,
        const asio::error_code& ec, std::size_t bytes_transferred);
  }
  
  //给descriptor_state起一个别名per_descriptor_data
  typedef descriptor_state* per_descriptor_data;
  //构造epoll reactor
  ASIO_DECL epoll_reactor(asio::execution_context& ctx);
  //新链接描述符fd关注的所有读、写、异常等事件注册到epoll事件集中，当对应事件到达通过epoll_wait返回回去即可
  ASIO_DECL int register_descriptor(socket_type descriptor,
      per_descriptor_data& descriptor_data);
  //把读、写操作的回调注册到描述符对应私有信息队列descriptor_data.op_queue_[]中
  void epoll_reactor::start_op(int op_type, socket_type descriptor,
  epoll_reactor::per_descriptor_data& descriptor_data, reactor_op* op,
  bool is_continuation, bool allow_speculative)
    
  //epoll_reactr对应的scheduler_
  scheduler& scheduler_;
  //事件私有信息保护锁
  mutex mutex_;
  //timer fd，定时器对应的fd，本章不做定时器实现描述
  int timer_fd_;
  //timer队列集
  timer_queue_set timer_queues_;
  
  //所有链接描述符私有信息都存入该poll池中，用于资源统一的分配和释放
  object_pool<descriptor_state> registered_descriptors_;
  //registered_descriptors_并发锁控制
  mutex registered_descriptors_mutex_;
}
   从上面对epoll_reactor类的分析，可以看出，所有链接通过descriptor_state管理私有结构，该结构
负责链接上所有的底层epoll事件收集与处理。descriptor_state通过一个op_queue_数组队列来把不同reactor类型的操作记录
到各自不同的队列中，这样可以更好的管理。
   
2. 主要接口成员实现
   epoll reactor类接口实现在epoll_reactor.ipp文件中实现，主要接口包括：初始化、描述符注册、各自事件回调注册、描述符注销、
epoll调度执行、底层IO处理、IO处理后的回调实现等
2.1 epoll相关实现
epoll_reactor::epoll_reactor(asio::execution_context& ctx) 
{
   //初始化构造，代码比较简单
}  

//把链接描述符关注的几个常用事件注册到epoll
int epoll_reactor::register_descriptor(socket_type descriptor,
    epoll_reactor::per_descriptor_data& descriptor_data) //套接字 回调等相关信息
{
  //获取一个描述符descriptor_state信息，分配对应空间
  descriptor_data = allocate_descriptor_state();

  ASIO_HANDLER_REACTOR_REGISTRATION((
        context(), static_cast<uintmax_t>(descriptor),
        reinterpret_cast<uintmax_t>(descriptor_data)));

  {
    mutex::scoped_lock descriptor_lock(descriptor_data->mutex_);

	//下面对descriptor_data进行相应的赋值
    descriptor_data->reactor_ = this;
    descriptor_data->descriptor_ = descriptor;
    descriptor_data->shutdown_ = false;
    for (int i = 0; i < max_ops; ++i)
      descriptor_data->try_speculative_[i] = true;
  }

  epoll_event ev = { 0, { 0 } };
  //同时把这些事件添加到epoll事件集，表示关注这些事件，注意这里是边沿触发
  ev.events = EPOLLIN | EPOLLERR | EPOLLHUP | EPOLLPRI | EPOLLET; 
  descriptor_data->registered_events_ = ev.events; 
  //赋值记录到ev.data.ptr中，当对应网络事件到底执行回调的时候可以通过该指针获取descriptor_data
  //descriptor_data记录到该指针，epoll_reactor::run中通过对应事件获取该私有信息
  ev.data.ptr = descriptor_data; 
  //通过epoll_ctl把events添加到事件集，当对应事件发生，epoll_wait可以获取对应事件
  int result = epoll_ctl(epoll_fd_, EPOLL_CTL_ADD, descriptor, &ev);
  ......

  return 0;
}

//1. 对描述符关注的epoll事件做跟新
//2. 把reactor_op对应的网络IO相关opration任务入队到descriptor_data对应的私有队列信息
void epoll_reactor::start_op(int op_type, socket_type descriptor,
    epoll_reactor::per_descriptor_data& descriptor_data, reactor_op* op,
    bool is_continuation, bool allow_speculative)
{
  ......
  
  //如果reactor_op还没有加入对应的op_queue_[i]队列，则EPOLL_CTL_MOD跟新epoll对应事件信息
  if (descriptor_data->op_queue_[op_type].empty())
  {
    ......
    {
	  //如果是写类型，添加EPOLLOUT进去
      if (op_type == write_op)
      {
        descriptor_data->registered_events_ |= EPOLLOUT;
      }
 
      epoll_event ev = { 0, { 0 } };
      ev.events = descriptor_data->registered_events_;
      ev.data.ptr = descriptor_data;
	  //再次跟新一下事件
      epoll_ctl(epoll_fd_, EPOLL_CTL_MOD, descriptor, &ev);
    }
  }
  //把任务回调opration入队到descriptor_data的对应队列
  descriptor_data->op_queue_[op_type].push(op);
  //scheduler::work_started  实际上就是链接数
  scheduler_.work_started();
}

//epoll异步IO事件处理流程
void epoll_reactor::run(long usec, op_queue<operation>& ops) //ops队列内容为descriptor_state
{
  ......
  epoll_event events[128];
  //epoll_wait获取到IO事件后返回，或者超时事件内没有对应网络IO事件，也返回
  int num_events = epoll_wait(epoll_fd_, events, 128, timeout);
  ......
  //遍历获取对应的事件信息
  for (int i = 0; i < num_events; ++i)
  {
      void* ptr = events[i].data.ptr;

	  //等待完成之后，我们开始分发事件:

      unsigned event_mask = 0;
	  //accept事件、网络数据到达事件
      if ((events[i].events & EPOLLIN) != 0)
        event_mask |= ASIO_HANDLER_REACTOR_READ_EVENT;
	  //写事件
      if ((events[i].events & EPOLLOUT))
        event_mask |= ASIO_HANDLER_REACTOR_WRITE_EVENT;
	  //异常事件
      if ((events[i].events & (EPOLLERR | EPOLLHUP)) != 0)
        event_mask |= ASIO_HANDLER_REACTOR_ERROR_EVENT;
      ASIO_HANDLER_REACTOR_EVENTS((context(),
            reinterpret_cast<uintmax_t>(ptr), event_mask));

  }

  // Dispatch the waiting events.
  //IO事件类型有三种:interrupt,timer和普通的IO事件
  for (int i = 0; i < num_events; ++i)
  {
      //该事件对应的私有信息指针，通过该指针就可以获取到对应的descriptor_data，该结构记录了对应的读写react_op
      void* ptr = events[i].data.ptr;

      //通过ptr获取对应descriptor_data信息
      descriptor_state* descriptor_data = static_cast<descriptor_state*>(ptr);
      if (!ops.is_enqueued(descriptor_data))  //不在队列中，则添加
      {
        //对应事件位图置位
        descriptor_data->set_ready_events(events[i].events);
		//epoll operation对应的回调函数是epoll_reactor::descriptor_state::do_complete
        ops.push(descriptor_data); 
      }
      else  //descriptor_data已经在ops的队列中了，对应事件位图置位
      {
        descriptor_data->add_ready_events(events[i].events);
      }
    }
  }
  ......
}

2.2 react_op对应网络IO事件处理流程
    前面章节已经知道，ASIO的网络IO处理及其回调都是通过reactor_op的几个基础类中赋值，其真正的调度执行通过
以下几个接口实现：
//初始化一个描述符信息descriptor_state
epoll_reactor::descriptor_state::descriptor_state(bool locking)
  //这里可以看出opration对应的func为descriptor_state::do_complete
  : operation(&epoll_reactor::descriptor_state::do_complete),
    mutex_(locking)
{
}
当scheduler::do_wait_one中获取operation执行complete的时候，最终会调用该descriptor_state::do_complete执行，该接口实现如下：
void epoll_reactor::descriptor_state::do_complete(
    void* owner, operation* base,
    const asio::error_code& ec, std::size_t bytes_transferred)
{
  if (owner)
  {
    descriptor_state* descriptor_data = static_cast<descriptor_state*>(base);
	//事件位图
    uint32_t events = static_cast<uint32_t>(bytes_transferred); 
	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
    if (operation* op = descriptor_data->perform_io(events)) 
    {
      //执行complete_func, 也就是reactive_socket_accept_op_base  reactive_socket_recv_op_base reactive_socket_send_op_base  
      //这三个IO操作对应的complete_func回调

	  //注意这里只执行了网络IO事件任务中的一个，其他的在perform_io中入队到全局队列中了，等待其他线程执行
      op->complete(owner, ec, 0);
    }
  }
}
    descriptor_state::do_complete最终会调用descriptor_state::perform_io执行，perform_io实现如下：
//网络IO相关的任务处理，如accept接收新链接、接收数据、发送数据
operation* epoll_reactor::descriptor_state::perform_io(uint32_t events)
{
  //上锁
  mutex_.lock();
  //这里构造一个perform_io_cleanup_on_block_exit类，用于后续析构时候的收尾处理
  perform_io_cleanup_on_block_exit io_cleanup(reactor_);
  mutex::scoped_lock descriptor_lock(mutex_, mutex::scoped_lock::adopt_lock);


  //分别对应新链接到达或者数据来临、可以写数据、有紧急的数据可读(这里应该表示有带外数据到来)
  static const int flag[max_ops] = { EPOLLIN, EPOLLOUT, EPOLLPRI };
  //循环处理如下三种类型的reactor_op:
  // 1. reactive_socket_accept_op_base
  // 2. reactive_socket_recv_op_base
  // 3. reactive_socket_send_op_base    
  for (int j = max_ops - 1; j >= 0; --j)
  {
    //有读写事件、或者epoll_wait有获取到异常，如链接断开等
    if (events & (flag[j] | EPOLLERR | EPOLLHUP)) 
    {
      try_speculative_[j] = true;
	  //遍历对应数组成员的整个队列
      while (reactor_op* op = op_queue_[j].front())
      {
       //reactive_socket_accept_op_base::do_perform  reactive_socket_recv_op_base::do_perform
  //reactive_socket_send_op_base::do_perform  分别对应新链接，读取数据，发送数据的底层实现
  	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
        if (reactor_op::status status = op->perform()) 
		//status为true表示成功，false表示底层处理失败
        {
          op_queue_[j].pop();
          io_cleanup.ops_.push(op);
          if (status == reactor_op::done_and_exhausted)
          {
            try_speculative_[j] = false;
            break;
          }
        }
		//例如如果没读到一个完整的数据，则这里直接退出，继续循环处理其他网络数据，下次继续读取。
        else
          break;
      }
    }
  }

  // The first operation will be returned for completion now. The others will
  // be posted for later by the io_cleanup object's destructor.
   //这里只返回了第一个op，其他的op在~perform_io_cleanup_on_block_exit处理
  io_cleanup.first_op_ = io_cleanup.ops_.front();
  //把返回的第一个从队列中清除，剩余的op还在队列中
  io_cleanup.ops_.pop(); 

  //该逻辑的总体思路：队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行

  //只返回第一个op,外层的epoll_reactor::descriptor_state::do_complete中执行该op对应的complete
  return io_cleanup.first_op_;
}

//epoll_reactor::descriptor_state::do_complete执行
//网络IO相关的任务处理，如accept接收新链接、接收数据、发送数据
operation* epoll_reactor::descriptor_state::perform_io(uint32_t events)
{
  ......
  
  //分别对应链接到达或者数据来临、可以写数据、有紧急的数据可读(这里应该表示有带外数据到来)
  static const int flag[max_ops] = { EPOLLIN, EPOLLOUT, EPOLLPRI };
  //循环处理各自不同的reactor_op(reactive_socket_accept_op_base   reactive_socket_recv_op_base reactive_socket_send_op_base)
  for (int j = max_ops - 1; j >= 0; --j)
  {
    if (events & (flag[j] | EPOLLERR | EPOLLHUP)) //有读写事件、或者epoll_wait有获取到异常，如链接断开等
    {
      try_speculative_[j] = true;
      while (reactor_op* op = op_queue_[j].front())
      {
       //reactive_socket_accept_op_base::do_perform  reactive_socket_recv_op_base::do_perform
    //reactive_socket_send_op_base::do_perform  分别对应新链接，读取数据，发送数据的底层实现
  	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
        if (reactor_op::status status = op->perform()) 
		//status为true表示成功，false表示底层处理失败
        {
          //取出对应的op，入队到临时队列io_cleanup.ops_
          op_queue_[j].pop();
          io_cleanup.ops_.push(op);
          if (status == reactor_op::done_and_exhausted)
          {
            try_speculative_[j] = false;
            break;
          }
        }
		//如果有异常
        else
          break;
      }
    }
  }

   //这里只返回了第一个op，其他的op在~perform_io_cleanup_on_block_exit处理
  io_cleanup.first_op_ = io_cleanup.ops_.front();
  //把返回的第一个从队列中清除，剩余的op还在队列中
  io_cleanup.ops_.pop(); 

  //该逻辑的总体思路：队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行

  //只返回第一个op,外层的epoll_reactor::descriptor_state::do_complete中执行该op对应的complete
  return io_cleanup.first_op_;
}
    从上面的分析可以看出，op_queue_队列上的所有reactor_op的底层IO处理成功后，会把该reactor_op入队到临时io_cleanup.first_op_队列中，
函数结束的时候把队首的reactor_op取出然后返回，在外层的do_complete中直接执行op->complete(也就是reactor_op中的complete_func，分别对应accetp、读、写中的
reactive_socket_accept_op_base::do_perform、reactive_socket_accept_op_base::do_perform、reactive_socket_recv_op_base reactive_socket_send_op_base::do_perform  )。
    那么这里有个问题？临时队列的其他reactor_op是如何处理的呢？其他reactor_op的处理实际上由perform_io_cleanup_on_block_exit析构函数处理，下面看下该析构函数
实现：

struct epoll_reactor::perform_io_cleanup_on_block_exit
{
 //初始化构造
  explicit perform_io_cleanup_on_block_exit(epoll_reactor* r)
    : reactor_(r), first_op_(0)
  {
  }
  
  //epoll_reactor::descriptor_state::perform_io中后续收尾处理
  ~perform_io_cleanup_on_block_exit()
  { 
    //配合epoll_reactor::descriptor_state::perform_io阅读，可以看出第一个op由本线程获取，其他op放入到了ops_队列
    //队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行
	if (first_op_)
    {
      //队列不为空
      if (!ops_.empty())
	  	//scheduler::post_deferred_completions，op任务放入全局队列，延迟执行
        reactor_->scheduler_.post_deferred_completions(ops_);
    }
    else
    {
	  //队首的线程由本线程处理，其他op任务放入全局任务队列, 见epoll_reactor::descriptor_state::do_complete
      reactor_->scheduler_.compensating_work_started();
    }
  }
  
  //所属epoll_reactor
  epoll_reactor* reactor_;
  //临时队列
  op_queue<operation> ops_;
  //标记队首位置
  operation* first_op_;
};





线程队列：thread_context

除了全局任务队列op_queue_外，每个工作线程还有各自的私有线程队列，该私有队列由scheduler_thread_info结构管理，如下所示：

全局任务队列，为啥还要设计一个私有任务队列，因为可以一次性入队
	completion_handler
	
如何保证读写某个fd的数据有序，如果第一个线程度一部分数据，第二个线程又读取一部分数据，无法连贯
并发如何控制

asio
1. 并发控制
2. 队列组成
3. 调度实现











存储引擎注册识别区分流程：
ServiceContextMongoD._storageFactories

void ServiceContextMongoD::registerStorageEngine(const std::string& name,
                                                 const StorageEngine::Factory* factory) {
    // No double-registering.
    invariant(0 == _storageFactories.count(name));

    // Some sanity checks: the factory must exist,
    invariant(factory);

    // and all factories should be added before we pick a storage engine.
    invariant(NULL == _storageEngine);

    _storageFactories[name] = factory;
}


MONGO_INITIALIZER_WITH_PREREQUISITES(DevNullEngineInit, ("SetGlobalEnvironment"))
(InitializerContext* context) {
    getGlobalServiceContext()->registerStorageEngine("devnull", new DevNullStorageEngineFactory());------对应DevNullKVEngine
    return Status::OK();
}
}


MONGO_INITIALIZER_WITH_PREREQUISITES(MMAPV1EngineInit, ("SetGlobalEnvironment"))
(InitializerContext* context) {
    getGlobalServiceContext()->registerStorageEngine("mmapv1", new MMAPV1Factory());   --------对应MMAPV1Engine
    return Status::OK();
}

MONGO_INITIALIZER_WITH_PREREQUISITES(WiredTigerEngineInit, ("SetGlobalEnvironment"))  ---------WiredTigerKVEngine
(InitializerContext* context) {
    getGlobalServiceContext()->registerStorageEngine(kWiredTigerEngineName,
                                                     new WiredTigerFactory());

    return Status::OK();
}

ServiceContextMongoD::initializeGlobalStorageEngine() {
	//获取存储引擎，默认的WiredTiger存储引擎对应WiredTigerFactory
    const StorageEngine::Factory* factory = _storageFactories[storageGlobalParams.engine];
		//WiredTigerFactory::create  //根据params参数构造KVStorageEngine类
    _storageEngine = factory->create(storageGlobalParams, _lockFile.get());
    _storageEngine->finishInit(); //void KVStorageEngine::finishInit() {}
}



//定义一个全局的DatabaseHolder  
DatabaseHolder& dbHolderImpl() {
    static DatabaseHolder _dbHolder;
    return _dbHolder;
}

class DatabaseHolderImpl : public DatabaseHolder::Impl {
public:
	......
	
	//对应DatabaseImpl map表
    typedef StringMap<Database*> DBs;	
	//所有db保存到这里，通过DatabaseHolderImpl::openDb创建后保存到这里
    DBs _dbs; 
}




class CollectionImpl {
	......
	//表名
    const NamespaceString _ns;
    //表对应uuid，一个表对应一个唯一uuid
    OptionalCollectionUUID _uuid;
	
	......
	//数据行KV操作接口
	RecordStore* const _recordStore; 
	//索引行KV操作接口实现
	IndexCatalog _indexCatalog;
	
	//对应CollectionInfoCacheImpl，缓存查询计划信息
    CollectionInfoCache _infoCache;
}



class io_context
  : public execution_context
{
    typedef detail::io_context_impl impl_type;
	
	//和mongodb相关的核心接口如下
	std::size_t run_for(const chrono::duration<Rep, Period>& rel_time);
	ASIO_DECL count_type run(asio::error_code& ec);
	std::size_t run_one_for(const chrono::duration<Rep, Period>& rel_time);
	ASIO_DECL count_type poll(asio::error_code& ec);
	io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
	io_context::post(ASIO_MOVE_ARG(CompletionHandler) handler)
	
  // The implementation.
  //以上接口的具体实现实际上在该类中完成，也就是scheduler类
  impl_type& impl_;
}

io_context类中包含一个impl_成员，该成员类型impl_type实际上就是detail::io_context_impl，io_context_impl的来源定义如下：
typedef class scheduler io_context_impl;io_context.impl_对应scheduler类，类成员及其核心接口实现在scheduler.hpp和scheduler.ipp文件完成。

_acceptorIOContext这个io上下文结构用于处理accept相关请求，该上下文在mongodb服务层调用asio方式为_acceptorIOContext->run()和_acceptorIOContext->stop();;
   _acceptorIOContext->run();这两个接口的实现如下：
io_context::count_type io_context::run()
{
  asio::error_code ec;
  //scheduler::run调度处理
  count_type s = impl_.run(ec);
  asio::detail::throw_error(ec);
  return s;
}

_acceptorIOContext->stop()具体实现如下:
//io调度结束
void io_context::stop()
{
  //scheduler::stop
  impl_.stop();
}

_workerIOContext用于处理accept返回的新链接fd上面的数据读写服务，同时处理网络状态机的调度。asio实际上在实现的时候，把各种读写操作及其数据收发后的回调组装成各种不同的operation handler放入队列，然后由mongodb中创建的各种
工作线程来从队列中取出opration handler执行，见后面opration及调度相关实现。和_workerIOContext关联的几个主要opration出队相关操作接口如下：
std::size_t io_context::run_for(
    const chrono::duration<Rep, Period>& rel_time)
{
  //直接调用io_context::run_until，时间转换为当前时间之后rel_time这个时间点
  return this->run_until(chrono::steady_clock::now() + rel_time);
}


std::size_t io_context::run_until(
    const chrono::time_point<Clock, Duration>& abs_time)
{
  std::size_t n = 0;
  //注意这里是循环掉用run_one_until接口
  while (this->run_one_until(abs_time))
    if (n != (std::numeric_limits<std::size_t>::max)())
      ++n;
  return n;
}

std::size_t io_context::run_one_for(
    const chrono::duration<Rep, Period>& rel_time)
{ 
  //只调用一次
  return this->run_one_until(chrono::steady_clock::now() + rel_time);
}


std::size_t io_context::run_one_until(
    const chrono::time_point<Clock, Duration>& abs_time) //abs_time表示运行该函数到期的绝对事件点
{
  typename Clock::time_point now = Clock::now();
  while (now < abs_time)
  {
    //在本循环中还需要运行多久
    typename Clock::duration rel_time = abs_time - now;
	//最多一次循环执行1s，也就是schedule::wait_one最多单次wait 1s
    if (rel_time > chrono::seconds(1))
      rel_time = chrono::seconds(1);

    asio::error_code ec;
	//schedule::wait_one 从操作队列中获取对应的operation执行，如果获取到operation并执行成功则返回1，否则返回0
    std::size_t s = impl_.wait_one(
        static_cast<long>(chrono::duration_cast<
          chrono::microseconds>(rel_time).count()), ec);
    asio::detail::throw_error(ec);

	//已经获取执行完一个operation，则直接返回，否则继续循环调度schedule::wait_one获取operation执行
    if (s || impl_.stopped())
      return s;

	//重新计算当前时间
    now = Clock::now();
  }

  return 0;
}
	从上面的代码分析可以看出io_context::run_one_for()和io_context::run_for()两个接口最终都掉调用schedule类的wait_one接口。
schedule::wait_one接口的实现细节请参考下一节的schedule调度设计与实现
    io_context::run_one_for()和io_context::run_for()区别主要是run_one_for只调用run_one_until一次，而run_for会持续循环调用run_one_until处理operation操作。


和_workerIOContext关联的几个主要入队相关操作接口如下：
io_context::post(ASIO_MOVE_ARG(CompletionHandler) handler)
{
  //构造async_completion类，handler回调初始化到该类相应成员
  async_completion<CompletionHandler, void ()> init(handler);
  
  //mongodb  asio::async_read(), asio::async_write(), asio::async_connect(), 默认返回true。其他handler返回false
  bool is_continuation =   
    asio_handler_cont_helpers::is_continuation(init.completion_handler);

  // Allocate and construct an operation to wrap the handler.
  //构造complete handler,completion_handler继承基础operation 
  typedef detail::completion_handler<
    typename handler_type<CompletionHandler, void ()>::type> op;
  typename op::ptr p = { detail::addressof(init.completion_handler),
      op::ptr::allocate(init.completion_handler), 0 };
  p.p = new (p.v) op(init.completion_handler);

  //调用scheduler::post_immediate_completion，completion_handler传递给该函数
  impl_.post_immediate_completion(p.p, is_continuation);
  
}
另一个和mongodb入队相关的接口如下：
io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
{
  async_completion<CompletionHandler, void ()> init(handler);

  //scheduler::can_dispatch  本线程已经加入到了工作线程队列，则本handler直接由本线程运行
  if (impl_.can_dispatch())
  {
    detail::fenced_block b(detail::fenced_block::full);
	//直接运行handler
    asio_handler_invoke_helpers::invoke(
        init.completion_handler, init.completion_handler); 
  }
  else
  {
    // Allocate and construct an operation to wrap the handler.
    //构造complete handler,completion_handler继承基础operation 
    typedef detail::completion_handler<
      typename handler_type<CompletionHandler, void ()>::type> op;
    typename op::ptr p = { detail::addressof(init.completion_handler),
      op::ptr::allocate(init.completion_handler), 0 };
    p.p = new (p.v) op(init.completion_handler); //获取op操作，也就是handler回调
    ......
	
	//mongodb的ServiceExecutorAdaptive::schedule调用->io_context::dispatch(ASIO_MOVE_ARG(CompletionHandler) handler)
	//->scheduler::do_dispatch
	//scheduler::do_dispatch 
    impl_.do_dispatch(p.p); 
    p.v = p.p = 0;
  }
  ......
}


	从上面的分析可以看出，和mongodb直接相关的几个接口最终都是调用schedule类的相关接口，整个实现过程参考下一节scheduler调度实现模块。
总结： 
1. asio::io_context类中和mongodb直接相关的几个接口主要是run_for、run、run_one_for、poll、dispatch
2. 这些接口按照功能不同，可以分为入队型接口(poll、dispatch)和出队型接口(run_for、run、run_one_for)
3. 按照和io_context的关联性不同，可以分为accept相关io(_acceptorIOContext)处理的接口(run、stop)和新链接fd对应Io(_workerIOContext)数据收发相关处理及回调的接口(run_for、run_one_for、poll、dispatch)
4. io_context上下文的上述接口，除了dispatch在某些情况下直接运行handler外，其他接口最终都会间接调用scheduler调度类接口

scheduler多线程并发调度实现过程
    上一节的io_context上下文中提到mongodb操作的io上下文最终都会调用scheduler的几个核心接口。scheduler类主要工作在于完成任务调度，该类和mongodb相关的几个主要成员变量及接口如下：
class scheduler {
public:
  //reactor_op  completion_handler继承该类    
  //mongodb中operation分为两种，一种是completion_handler，另一种是reactor_op
  typedef scheduler_operation operation;
  
  //listen线程做accept异步处理的循环调度过程
  std::size_t scheduler::run(asio::error_code& ec)
  
  //全局任务入队相关的接口
  void scheduler::do_dispatch(scheduler::operation* op)
  
  //任务出队执行的，如果队列为空则获取epoll事件集对应的网络IO任务放入全局op_queue_队列
  std::size_t scheduler::wait_one(long usec, asio::error_code& ec)
  std::size_t scheduler::do_wait_one(mutex::scoped_lock& lock,
    scheduler::thread_info& this_thread, long usec, const asio::error_code& ec)
	
  //任务调度启停相关接口
  void scheduler::restart()
  void scheduler::stop_all_threads(mutex::scoped_lock& lock)
  
private:
  //全局锁，多线程互斥，对全局op_queue_队列做互斥
  mutable mutex mutex_;
  //全局任务队列，全局任务和网络事件相关任务都添加到该队列
  op_queue<operation> op_queue_;
  
  
  struct task_operation : operation //特殊operation
  {
	//这个特殊op的作用是保证从epoll_wait返回值中获取到一批网络事件对应的IO回调op接入全局队列op_queue_后，都加上该特殊op到队列中
	//下次如果从队列头部获取到该特殊Op操作，会继续获取epoll网络事件任务，避免网络IO长时间不被处理引起的"饥饿"状态
    task_operation() : operation(0) {}
  } task_operation_; 
  
  //stop_all_threads中置为true, 为true后，将不再处理epoll相关事件，参考scheduler::do_run_one
  bool stopped_;  
  
  //唤醒等待锁得线程(实际event由信号量封装)
  event wakeup_event_; 
  
  //也就是epoll_reactor，借助epoll实现网络事件异步处理
  reactor* task_; //epoll_reactor
  
  //套接字描述符个数：accept获取到的链接数fd个数+1(定时器fd)
  atomic_count outstanding_work_;
}


ASIO调度的任务(注: 每个任务对应一个operation类，operation类实现讲在后面章节详述)包括以下两种：1. 全局task任务  2. 网络IO事件处理相关任务(新链接、数据读取及其对应回调)，所有的任务通过一个全局队列链接在一起，并有序的调度执行。
1. 任务入队流程
    所有的任务都通过scheduler.op_queue_统一管理，和op_queue_全局队列任务入队相关的函数接口如下：
//全局状态任务入队
void scheduler::do_dispatch(
    scheduler::operation* op)
{
  work_started();
  //默认多线程，所以这里需要对队列加锁
  mutex::scoped_lock lock(mutex_);
  op_queue_.push(op);
  wake_one_thread_and_unlock(lock);
}

除了全局状态任务外，Asio还需要处理网络IO相关的任务，网络IO相关的任务入队相关接口如下：
struct scheduler::task_cleanup
{
  //scheduler::do_wait_one中获取到的epoll网络IO任务先入队到线程私有队列，在task_cleanup析构函数中一次性入队到全局队列
  ////将本线程的私有队列放入全局队列中，然后用task_operation_来标记一个线程私有队列的结束。
  //每次执行epoll_reactor::run去获取epoll对应的事件列表的时候都会执行该cleanup
  ~task_cleanup()
  {
    if (this_thread_->private_outstanding_work > 0)
    { //本线程上的私有任务后面会全部入队到全局op_queue_队列，全局任务数增加
      asio::detail::increment( 
          scheduler_->outstanding_work_,
          this_thread_->private_outstanding_work);
    }
    this_thread_->private_outstanding_work = 0;

    // Enqueue the completed operations and reinsert the task at the end of
    // the operation queue.
    lock_->lock();
	//将本线程的私有队列放入全局队列中，然后用task_operation_来标记一个线程私有队列的结束。
	//task_operation_标记一个线程私有队列的结束。
    scheduler_->task_interrupted_ = true;
    scheduler_->op_queue_.push(this_thread_->private_op_queue);

	//注意这里，加了一个特殊的op task_operation_到全局队列
    scheduler_->op_queue_.push(&scheduler_->task_operation_);
  }

  scheduler* scheduler_;
  //本作用域的锁
  mutex::scoped_lock* lock_;
  //线程私有信息
  thread_info* this_thread_;
};
    从上面的代码分析可以看出，当释放task_cleanup资源的时候，会把本线程私有private_op_queue队列上的所有网络IO相关任务一次性入队到全局op_queue_队列。

	
2. 任务出队执行
    空闲worker线程从op_queue_全局队列中获取任务执行，任务从全局队列出队流程相关接口如下:
std::size_t scheduler::wait_one(long usec, asio::error_code& ec)
{
  ec = asio::error_code();
  if (outstanding_work_ == 0) //如果连工作线程都没有，则说明没有调度的意义，停止所有调度
  {
    stop();
    return 0;
  }

  thread_info this_thread;
  this_thread.private_outstanding_work = 0;
  //线程入队到call_stack.top_链表
  thread_call_stack::context ctx(this, this_thread);
  //上锁
  mutex::scoped_lock lock(mutex_);

  //从操作队列中获取对应的operation执行，如果获取到operation并执行成功则返回1，否则返回0
  return do_wait_one(lock, this_thread, usec, ec);
}
    接口scheduler::wait_one最终调用scheduler::do_wait_one接口，该接口实现如下：
std::size_t scheduler::do_wait_one(...) //获取全局队列首的一个op执行
{
  //stop_all_threads中置为true, 为true后，将不再处理epoll相关事件，参考scheduler::do_run_one
  if (stopped_)
    return 0;
  //去队列头部任务
  operation* o = op_queue_.front();
  if (o == 0) //如果队列为空，则等待usec
  {
    //等待被唤醒
    wakeup_event_.clear(lock);
    wakeup_event_.wait_for_usec(lock, usec);
    usec = 0; // Wait at most once.
    //等一会儿后我们继续判断队列中是否有可执行的op
    o = op_queue_.front();
  }
  
  //该op是一个特殊的op, 说明需求去获取epoll上面的网络时间任务来处理，避免网络IO长期不被处理而处于饥饿状态
  if (o == &task_operation_) 
  {
    op_queue_.pop();
	//队列上还有其他等待执行的任务
    bool more_handlers = (!op_queue_.empty());

    task_interrupted_ = more_handlers;
    //既然队列上还有任务需要被调度执行，则通知其他线程可以继续获取队列任务执行了，这样可以提高并发
    if (more_handlers && !one_thread_)//默认mongodb多线程
      wakeup_event_.unlock_and_signal_one(lock); //唤醒其他工作线程
    else
      lock.unlock();

    {
	  //该类析构函数中会把下面从epoll获取的网络事件任务入队到全局scheduler.op_queue_队列
      task_cleanup on_exit = { this, &lock, &this_thread };
	  //函数exit的时候执行前面的on_exit析构函数从而把this_thread.private_op_queue入队到scheduler.op_queue_
      (void)on_exit;
      //scheduler::do_run_one->epoll_reactor::run
      //通过epoll获取所有得网络事件op入队到private_op_queue, 最终再通过scheduler::poll_one scheduler::poll入队到op_queue_
      //this_thread.private_op_queue队列成员的op类型为descriptor_state
	  task_->run(more_handlers ? 0 : usec, this_thread.private_op_queue);
    }
  }
  
  //没有任务可执行，直接返回
  if (o == 0)
    return 0;

  //先把该op任务出队
  op_queue_.pop();
  
  ......
  
  //执行取出的这个任务
  o->complete(this, ec, task_result);

  return 1;  
}

3. 任务队列
    从上面的分析可以看出，asio管理任务调度的队列包含两种：1. 全局任务队列op_queue_ 2. 工作线程私有队列private_op_queue
全局任务队列op_queue_结构参考前面的scheduler类，工作线程私有队列存放于scheduler_thread_info私有结构中，结构成员如下：
struct scheduler_thread_info : public thread_info_base
{
  //scheduler::do_wait_one->epoll_reactor::run中通过epoll获取对应网络事件任务
  //工作线程私有任务队列
  op_queue<scheduler_operation> private_op_queue; 
  //本线程私有private_op_queue队列中op任务数
  long private_outstanding_work;
};
在scheduler::do_wait_one接口中，工作线程遍历全局任务队列，如果队列头部的任务类型为task_operation特殊任务，则会调用epoll_reactor::run进行epoll
调度，通过epoll_wait获取到所有的网络IO事件任务，同时全部一次性入队到本线程的私有队列this_thread.private_op_queue。最后在task_cleanup析构函数中再一次性
从线程私有队列入队到全局队列，这样做的目的可以保证锁的粒度最小，同时性能最好，这也是设计一个线程私有队列的原因。

operation
    从前面的分析可以看出，一个任务对应一个operation类结构，asio中schduler调度的任务分为全局状态机任务和网络IO处理回调任务，这两种任务都有对应的回调函数。
此外，asio还有一种特殊的operation，该Operastion什么也不做，只是一个特殊标记。全局状态机任务、网络IO处理任务、特殊任务这三类任务分别对应、
reactor_op、task_operation_三个类实现，这三个类都会继承operation。
1. operation基类
    operation基类实际上就是scheduler_operation类，通过typedef scheduler_operation operation指定，是其他三个任务的父类，其主要实现接口如下：
class scheduler_operation  
{
public:
  typedef scheduler_operation operation_type;

  //执行func回调
  void complete(void* owner, const asio::error_code& ec,
      std::size_t bytes_transferred)
  {
    func_(owner, this, ec, bytes_transferred);
  }

protected:
  //初始化
  scheduler_operation(func_type func)
    : next_(0),
      func_(func),
      task_result_(0)
  {}

private:
  //reactor_op类(对应网络事件处理任务):epoll_reactor::descriptor_state::do_complete
  //completion_handler类(对应全局状态机任务):对应completion_handler::do_complete
  func_type func_;
protected:
  //本opration对应的scheduler
  friend class scheduler;
  //获取epoll_wait返回的event信息，赋值见set_ready_events add_ready_events
  //所有的网络事件通过task_result_位图记录，生效见epoll_reactor::descriptor_state::do_complete
  unsigned int task_result_; // Passed into bytes transferred.
};

//统一用scheduler_operation类型定义opration，外部使用Opration都用scheduler_operation统一代替
typedef scheduler_operation operation;


2. 全局状态机任务(completion_handler)
     当mongodb通过listener线程接受到一个新链接后，会生成一个状态机调度任务，然后入队到全局队列op_queue_，worker线程从全局队列获取到该任务后调度执行，从而
进入状态机调度流程，在该流程中会触发epoll相关得网络IO注册及异步IO处理。一个全局状态机任务对应一个completion_handler类，该类实现如下：
class completion_handler : public operation  
{
public:
  ASIO_DEFINE_HANDLER_PTR(completion_handler);
  //初始化
  completion_handler(Handler& h)
    //completion_handler对应的func
    : operation(&completion_handler::do_complete),  
	  //handler_赋值
      handler_(ASIO_MOVE_CAST(Handler)(h)) 
  {
    handler_work<Handler>::start(handler_); //标记任务开始执行
  }

  //真正执行在scheduler::do_wait_one->operation::complete->completion_handler::do_complete
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
      ......
      //获取对应handler执行
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN(());
      w.complete(handler, handler);
      ASIO_HANDLER_INVOCATION_END;
  }

private:
  //任务对应的回调函数
  Handler handler_;
}
    全局任务入队的时候有两种方式，一种是io_context::dispatch方式，另一种是io_context::post。从前面章节对这两个接口的代码分析可以看出，任务直接入队到全局队列
op_queue_中，然后工作线程通过scheduler::do_wait_one从队列获取该任务执行。
注意：任务入队由Listener线程完成，任务出队调度执行由mongodb工作线程执行。
	
3. 网络IO事件处理任务
    网络IO事件对应的Opration任务最终由reactor_op类实现，该类主要成员及接口如下：
class reactor_op   
  : public operation //也就是scheduler_operation
{
public:
  // The error code to be passed to the completion handler.
  asio::error_code ec_;

  // The number of bytes transferred, to be passed to the completion handler.
  //发送或者接收fd的数据量
  std::size_t bytes_transferred_;

  //发送或者接收fd数据的状态
  enum status { not_done, done, done_and_exhausted };

  // Perform the operation. Returns true if it is finished.
  status perform() //执行perform_func_
  { 
    //执行func，网络IO对应的func见epoll_reactor::descriptor_state::perform_io中调度执行
    return perform_func_(this);
  }

protected:
  typedef status (*perform_func_type)(reactor_op*);
  //例如accept操作过程回调过程，一个执行accept操作，一个执行后续的complete_func操作(mongodb中的task)
  //例如recvmsg操作过程，一个执行reactive_socket_recv_op_base::do_perform(最终recvmsg)，一个执行后续complete_func操作(mongodb中的task)
  reactor_op(perform_func_type perform_func, func_type complete_func)
  //例如接受数据的complete_func为reactive_socket_recv_op::do_complete
    : operation(complete_func),  //complete_func赋值给operation，在operation中执行
      bytes_transferred_(0),
      perform_func_(perform_func)
  {
  }

private:
  //perform_func也就是fd数据收发底层实现底层实现，赋值给reactor_op.perform_func_, 
  //complete_func赋值给父类operation的func,见reactor_op构造函数
  perform_func_type perform_func_;
};	
    从reactor_op类可以看出，该类的主要两个函数成员：perform_func_和complete_func。其中perform_func_函数主要负责异步网络IO底层处理，complete_func用于数据接收或者发送后的后续处理逻辑。
perform_func_具体功能如下：
1. 通过epoll事件集处理底层accept获取新连接fd。
2. fd上的数据异步接收
3. fd上的数据异步发送
	针对上面的三个网络IO处理功能，ASIO在实现的时候，分别通过三个不同的继承类(reactive_socket_accept_op_base、reactive_socket_recv_op_base、
reactive_socket_send_op_base)实现。
1. accept异步处理实现
    accept的异步接收过程，perform_func_底层IO实现主要通过reactive_socket_accept_op_base实现，具体实现方式如下：

//reactive_socket_service::async_accept中注册
class reactive_socket_accept_op_base : public reactor_op
{
public:
  //reactive_socket_accept_op中构造赋值
  reactive_socket_accept_op_base(socket_type socket,
      socket_ops::state_type state, Socket& peer, const Protocol& protocol,
      typename Protocol::endpoint* peer_endpoint, func_type complete_func)
      //初始化accept的底层IO处理接口及接收到新链接的后续处理接口
    : reactor_op(&reactive_socket_accept_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      peer_(peer),
      protocol_(protocol),
      peer_endpoint_(peer_endpoint),
      addrlen_(peer_endpoint ? peer_endpoint->capacity() : 0)
  {
  }

  //这里是底层的accept接收处理
  static status do_perform(reactor_op* base)
  {
    ......
    status result = socket_ops::non_blocking_accept(o->socket_,
        o->state_, o->peer_endpoint_ ? o->peer_endpoint_->data() : 0,
        o->peer_endpoint_ ? &o->addrlen_ : 0, o->ec_, new_socket)
    ? done : not_done;
    ......
	
    return result;
  }

  //在外层继承类reactive_socket_accept_op中的do_complete中执行
  //把接收到的新链接new_socket_注册到epoll事件集中，这样就可以异步收发该fd的数据
  void do_assign()  
  {
       ......
	  //reactive_socket_service_base::do_assign,这里面把new_socket_注册到epoll事件集
      peer_.assign(protocol_, new_socket_.get(), ec_);
	  ......
  }

private:
  //sock套接字sd
  socket_type socket_; 
  //accept获取到的新链接fd，见reactive_socket_accept_op_base::do_perform
  socket_holder new_socket_;
  //客户端地址信息记录在这里面
  Socket& peer_; 
  typename Protocol::endpoint* peer_endpoint_;
};
    complete_func用于处理accept获取到新链接new_socket_后的后续处理，后续处理通过reactive_socket_accept_op继承类实现，如下：
class reactive_socket_accept_op :
  public reactive_socket_accept_op_base<Socket, Protocol>
{
public:
  //初始化构造
  reactive_socket_accept_op(socket_type socket,
      socket_ops::state_type state, Socket& peer, const Protocol& protocol,
      typename Protocol::endpoint* peer_endpoint, Handler& handler)
    : reactive_socket_accept_op_base<Socket, Protocol>(socket, state, peer,
        protocol, peer_endpoint, &reactive_socket_accept_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }

  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
    ......
	//执行前面的新连接epoll事件集注册reactive_socket_accept_op_base::do_assign
    if (owner)
	//reactive_socket_accept_op::do_assign
      o->do_assign();

    .......
	//执行accept后回调，也就是mongodb中的ServiceEntryPointImpl::startSession
    w.complete(handler, handler.handler_);
    ASIO_HANDLER_INVOCATION_END;
	......
  }

private:
  //接收到新链接的回调，mongodb中对应ServiceEntryPointImpl::startSession
  Handler handler_;
};
    complete_func函数接口主要事accept新链接后的处理，包括新连接new_sock注册到epoll事件集，这样该新链接的数据收发就可以异步实现。此外，
还会执行mongodb服务层的handler回调，也就是ServiceEntryPointImpl::startSession。

2. 网络数据接收异步处理实现
    网络数据异步接收过程，perform_func_底层IO实现主要通过reactive_socket_recv_op_base实现，具体实现方式如下：    
class reactive_socket_recv_op_base : public reactor_op
{
public:
  //初始化构造
  reactive_socket_recv_op_base(socket_type socket,
      socket_ops::state_type state, const MutableBufferSequence& buffers,
      socket_base::message_flags flags, func_type complete_func)
	//网络数据接收及其后续处理
    : reactor_op(&reactive_socket_recv_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      buffers_(buffers),
      flags_(flags)
  {
  }

  static status do_perform(reactor_op* base)
  {
    reactive_socket_recv_op_base* o(
        static_cast<reactive_socket_recv_op_base*>(base));
	......
	//异步接收数据存入bufs中
    status result = socket_ops::non_blocking_recv(o->socket_,
        bufs.buffers(), bufs.count(), o->flags_,
        (o->state_ & socket_ops::stream_oriented) != 0,
        o->ec_, o->bytes_transferred_) ? done : not_done;
	......
	
    return result;
  }

private:
  //套接字
  socket_type socket_;
  //存储读取的数据的buf,里面需要读取数据的长度
  MutableBufferSequence buffers_;
};
    接收到数据后的handler回调在reactive_socket_recv_op类中实现，该类继承reactive_socket_recv_op_base，其具体实现如下:
class reactive_socket_recv_op :
  public reactive_socket_recv_op_base<MutableBufferSequence>
{
public:
  //初始化构造
  reactive_socket_recv_op(socket_type socket,
      socket_ops::state_type state, const MutableBufferSequence& buffers,
      socket_base::message_flags flags, Handler& handler)
	//handler及do_complete赋值
    : reactive_socket_recv_op_base<MutableBufferSequence>(socket, state,
        buffers, flags, &reactive_socket_recv_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }

  //读取到一个完整的mongo数据后，回调在这里执行，实际上是工作线程从队列获取op执行的，见
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
	  ......
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN((handler.arg1_, handler.arg2_));
	  //执行对应handler
      w.complete(handler, handler.handler_);
      ASIO_HANDLER_INVOCATION_END;
	  ......
  }

private:
  //mongodb接收数据的handler对应TransportLayerASIO::ASIOSourceTicket::_headerCallback(接收到mongo协议头部对应的handler)、
  //TransportLayerASIO::ASIOSourceTicket::_bodyCallback(接收到包体部分的handler)
  Handler handler_;
};
3. 网络数据异步发送处理实现
	网络数据异步发送过程和接收过程类似，perform_func_底层IO数据发送实现主要通过reactive_socket_send_op_base实现，具体实现方式如下：   
class reactive_socket_send_op_base : public reactor_op
{
public:
  //初始化构造
  reactive_socket_send_op_base(socket_type socket,
      socket_ops::state_type state, const ConstBufferSequence& buffers,
      socket_base::message_flags flags, func_type complete_func)
	//初始化do_perform和complete_func
    : reactor_op(&reactive_socket_send_op_base::do_perform, complete_func),
      socket_(socket),
      state_(state),
      buffers_(buffers),
      flags_(flags)
  {
  }
  
  //数据异步发送的底层实现
  static status do_perform(reactor_op* base)
  {
	......
    status result = socket_ops::non_blocking_send(o->socket_,
          bufs.buffers(), bufs.count(), o->flags_,
          o->ec_, o->bytes_transferred_) ? done : not_done;
	......

    return result;
  }

private:
  //fd
  socket_type socket_;
  //发送的数据在该buffer中
  ConstBufferSequence buffers_;
}
    发送完buffers_数据后的handler回调在reactive_socket_send_op类中实现，该类继承reactive_socket_send_op_base，其具体实现如下:
class reactive_socket_send_op :
  public reactive_socket_send_op_base<ConstBufferSequence>
{
public:
  //初始化构造
  reactive_socket_send_op(socket_type socket,
      socket_ops::state_type state, const ConstBufferSequence& buffers,
      socket_base::message_flags flags, Handler& handler)
    : reactive_socket_send_op_base<ConstBufferSequence>(socket,
        state, buffers, flags, &reactive_socket_send_op::do_complete),
      handler_(ASIO_MOVE_CAST(Handler)(handler))
  {
    handler_work<Handler>::start(handler_);
  }
  
  //数据发送完成后的handler回调处理
  static void do_complete(void* owner, operation* base,
      const asio::error_code& /*ec*/,
      std::size_t /*bytes_transferred*/)
  {
	  ......
      fenced_block b(fenced_block::half);
      ASIO_HANDLER_INVOCATION_BEGIN((handler.arg1_, handler.arg2_));
      w.complete(handler, handler.handler_);
      ASIO_HANDLER_INVOCATION_END;
      ......
  }

private:
  //mongodb异步数据发送成功后的handler回调对应TransportLayerASIO::ASIOSinkTicket::_sinkCallback
  Handler handler_;
};
4. 总结
    从上面的网络IO任务的实现可以看出，整个实现过程都是大同小异的，都有底层fd的处理及其对应的handler回调处理，整个逻辑比较清晰。

epoll_reactor模式底层实现
    mongodb默认使用epoll方式来实现异步网络IO事件处理，epoll网上资料比较多，可以参考：https://en.wikipedia.org/wiki/Epoll。
本文只分析和ASIO实现密切关联的一些接口。
1. epoll_reactor类实现
class epoll_reactor 
  : public execution_context_service_base<epoll_reactor>
{
public:
  //任务操作类型
  enum op_types {  
  	read_op = 0,    //对应读事件
    write_op = 1,   //对应写事件
    connect_op = 1, //对应connect事件
    except_op = 2,  //异常事件
    max_ops = 3     //类型最大值
  };

  //新链接new_sock对应的私有描述符信息，用于
  class descriptor_state : operation  
  {
    //并发锁
    mutex mutex_;
	//本descriptor_state所属的epoll reactor
    epoll_reactor* reactor_;
	//新链接对应的套接字描述符
    int descriptor_;  
	//epoll_wait获取到的事件集位图
    uint32_t registered_events_;
	
	//套接字描述符对应的读、写、accept及异常事件对应的Opration任务分别入队到各自的数组队列中
	//op_queue_[I],i也就是对应上面的op_types
    op_queue<reactor_op> op_queue_[max_ops]; 
    bool try_speculative_[max_ops];
	//epoll_reactor::deregister_descriptor置为true，套接字描述符注册掉
    bool shutdown_;

	//descriptor_state初始化构造
	ASIO_DECL descriptor_state(bool locking);
	//epoll事件集对应的位图信息，每个位置1表示对应网络事件发生
	//这些位图上的事件处理在epoll_reactor::descriptor_state::do_complete
    void set_ready_events(uint32_t events) { task_result_ = events; }
    void add_ready_events(uint32_t events) { task_result_ |= events; }

	//对应accept、读、写事件的底层Io处理，实现见reactive_socket_accept_op_base	
	//reactive_socket_recv_op_base reactive_socket_send_op_base
	ASIO_DECL operation* perform_io(uint32_t events);
	//descriptor_state对应的do_complete为epoll_reactor::descriptor_state::do_complete
    ASIO_DECL static void do_complete(
        void* owner, operation* base,
        const asio::error_code& ec, std::size_t bytes_transferred);
  }
  
  //给descriptor_state起一个别名per_descriptor_data
  typedef descriptor_state* per_descriptor_data;
  //构造epoll reactor
  ASIO_DECL epoll_reactor(asio::execution_context& ctx);
  //新链接描述符fd关注的所有读、写、异常等事件注册到epoll事件集中，当对应事件到达通过epoll_wait返回回去即可
  ASIO_DECL int register_descriptor(socket_type descriptor,
      per_descriptor_data& descriptor_data);
  //把读、写操作的回调注册到描述符对应私有信息队列descriptor_data.op_queue_[]中
  void epoll_reactor::start_op(int op_type, socket_type descriptor,
  epoll_reactor::per_descriptor_data& descriptor_data, reactor_op* op,
  bool is_continuation, bool allow_speculative)
    
  //epoll_reactr对应的scheduler_
  scheduler& scheduler_;
  //事件私有信息保护锁
  mutex mutex_;
  //timer fd，定时器对应的fd，本章不做定时器实现描述
  int timer_fd_;
  //timer队列集
  timer_queue_set timer_queues_;
  
  //所有链接描述符私有信息都存入该poll池中，用于资源统一的分配和释放
  object_pool<descriptor_state> registered_descriptors_;
  //registered_descriptors_并发锁控制
  mutex registered_descriptors_mutex_;
}
   从上面对epoll_reactor类的分析，可以看出，所有链接通过descriptor_state管理私有结构，该结构
负责链接上所有的底层epoll事件收集与处理。descriptor_state通过一个op_queue_数组队列来把不同reactor类型的操作记录
到各自不同的队列中，这样可以更好的管理。
   
2. 主要接口成员实现
   epoll reactor类接口实现在epoll_reactor.ipp文件中实现，主要接口包括：初始化、描述符注册、各自事件回调注册、描述符注销、
epoll调度执行、底层IO处理、IO处理后的回调实现等
2.1 epoll相关实现
epoll_reactor::epoll_reactor(asio::execution_context& ctx) 
{
   //初始化构造，代码比较简单
}  

//把链接描述符关注的几个常用事件注册到epoll
int epoll_reactor::register_descriptor(socket_type descriptor,
    epoll_reactor::per_descriptor_data& descriptor_data) //套接字 回调等相关信息
{
  //获取一个描述符descriptor_state信息，分配对应空间
  descriptor_data = allocate_descriptor_state();

  ASIO_HANDLER_REACTOR_REGISTRATION((
        context(), static_cast<uintmax_t>(descriptor),
        reinterpret_cast<uintmax_t>(descriptor_data)));

  {
    mutex::scoped_lock descriptor_lock(descriptor_data->mutex_);

	//下面对descriptor_data进行相应的赋值
    descriptor_data->reactor_ = this;
    descriptor_data->descriptor_ = descriptor;
    descriptor_data->shutdown_ = false;
    for (int i = 0; i < max_ops; ++i)
      descriptor_data->try_speculative_[i] = true;
  }

  epoll_event ev = { 0, { 0 } };
  //同时把这些事件添加到epoll事件集，表示关注这些事件，注意这里是边沿触发
  ev.events = EPOLLIN | EPOLLERR | EPOLLHUP | EPOLLPRI | EPOLLET; 
  descriptor_data->registered_events_ = ev.events; 
  //赋值记录到ev.data.ptr中，当对应网络事件到底执行回调的时候可以通过该指针获取descriptor_data
  //descriptor_data记录到该指针，epoll_reactor::run中通过对应事件获取该私有信息
  ev.data.ptr = descriptor_data; 
  //通过epoll_ctl把events添加到事件集，当对应事件发生，epoll_wait可以获取对应事件
  int result = epoll_ctl(epoll_fd_, EPOLL_CTL_ADD, descriptor, &ev);
  ......

  return 0;
}

//1. 对描述符关注的epoll事件做跟新
//2. 把reactor_op对应的网络IO相关opration任务入队到descriptor_data对应的私有队列信息
void epoll_reactor::start_op(int op_type, socket_type descriptor,
    epoll_reactor::per_descriptor_data& descriptor_data, reactor_op* op,
    bool is_continuation, bool allow_speculative)
{
  ......
  
  //如果reactor_op还没有加入对应的op_queue_[i]队列，则EPOLL_CTL_MOD跟新epoll对应事件信息
  if (descriptor_data->op_queue_[op_type].empty())
  {
    ......
    {
	  //如果是写类型，添加EPOLLOUT进去
      if (op_type == write_op)
      {
        descriptor_data->registered_events_ |= EPOLLOUT;
      }
 
      epoll_event ev = { 0, { 0 } };
      ev.events = descriptor_data->registered_events_;
      ev.data.ptr = descriptor_data;
	  //再次跟新一下事件
      epoll_ctl(epoll_fd_, EPOLL_CTL_MOD, descriptor, &ev);
    }
  }
  //把任务回调opration入队到descriptor_data的对应队列
  descriptor_data->op_queue_[op_type].push(op);
  //scheduler::work_started  实际上就是链接数
  scheduler_.work_started();
}

//epoll异步IO事件处理流程
void epoll_reactor::run(long usec, op_queue<operation>& ops) //ops队列内容为descriptor_state
{
  ......
  epoll_event events[128];
  //epoll_wait获取到IO事件后返回，或者超时事件内没有对应网络IO事件，也返回
  int num_events = epoll_wait(epoll_fd_, events, 128, timeout);
  ......
  //遍历获取对应的事件信息
  for (int i = 0; i < num_events; ++i)
  {
      void* ptr = events[i].data.ptr;

	  //等待完成之后，我们开始分发事件:

      unsigned event_mask = 0;
	  //accept事件、网络数据到达事件
      if ((events[i].events & EPOLLIN) != 0)
        event_mask |= ASIO_HANDLER_REACTOR_READ_EVENT;
	  //写事件
      if ((events[i].events & EPOLLOUT))
        event_mask |= ASIO_HANDLER_REACTOR_WRITE_EVENT;
	  //异常事件
      if ((events[i].events & (EPOLLERR | EPOLLHUP)) != 0)
        event_mask |= ASIO_HANDLER_REACTOR_ERROR_EVENT;
      ASIO_HANDLER_REACTOR_EVENTS((context(),
            reinterpret_cast<uintmax_t>(ptr), event_mask));

  }

  // Dispatch the waiting events.
  //IO事件类型有三种:interrupt,timer和普通的IO事件
  for (int i = 0; i < num_events; ++i)
  {
      //该事件对应的私有信息指针，通过该指针就可以获取到对应的descriptor_data，该结构记录了对应的读写react_op
      void* ptr = events[i].data.ptr;

      //通过ptr获取对应descriptor_data信息
      descriptor_state* descriptor_data = static_cast<descriptor_state*>(ptr);
      if (!ops.is_enqueued(descriptor_data))  //不在队列中，则添加
      {
        //对应事件位图置位
        descriptor_data->set_ready_events(events[i].events);
		//epoll operation对应的回调函数是epoll_reactor::descriptor_state::do_complete
        ops.push(descriptor_data); 
      }
      else  //descriptor_data已经在ops的队列中了，对应事件位图置位
      {
        descriptor_data->add_ready_events(events[i].events);
      }
    }
  }
  ......
}

2.2 react_op对应网络IO事件处理流程
    前面章节已经知道，ASIO的网络IO处理及其回调都是通过reactor_op的几个基础类中赋值，其真正的调度执行通过
以下几个接口实现：
//初始化一个描述符信息descriptor_state
epoll_reactor::descriptor_state::descriptor_state(bool locking)
  //这里可以看出opration对应的func为descriptor_state::do_complete
  : operation(&epoll_reactor::descriptor_state::do_complete),
    mutex_(locking)
{
}
当scheduler::do_wait_one中获取operation执行complete的时候，最终会调用该descriptor_state::do_complete执行，该接口实现如下：
void epoll_reactor::descriptor_state::do_complete(
    void* owner, operation* base,
    const asio::error_code& ec, std::size_t bytes_transferred)
{
  if (owner)
  {
    descriptor_state* descriptor_data = static_cast<descriptor_state*>(base);
	//事件位图
    uint32_t events = static_cast<uint32_t>(bytes_transferred); 
	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
    if (operation* op = descriptor_data->perform_io(events)) 
    {
      //执行complete_func, 也就是reactive_socket_accept_op_base  reactive_socket_recv_op_base reactive_socket_send_op_base  
      //这三个IO操作对应的complete_func回调

	  //注意这里只执行了网络IO事件任务中的一个，其他的在perform_io中入队到全局队列中了，等待其他线程执行
      op->complete(owner, ec, 0);
    }
  }
}
    descriptor_state::do_complete最终会调用descriptor_state::perform_io执行，perform_io实现如下：
//网络IO相关的任务处理，如accept接收新链接、接收数据、发送数据
operation* epoll_reactor::descriptor_state::perform_io(uint32_t events)
{
  //上锁
  mutex_.lock();
  //这里构造一个perform_io_cleanup_on_block_exit类，用于后续析构时候的收尾处理
  perform_io_cleanup_on_block_exit io_cleanup(reactor_);
  mutex::scoped_lock descriptor_lock(mutex_, mutex::scoped_lock::adopt_lock);


  //分别对应新链接到达或者数据来临、可以写数据、有紧急的数据可读(这里应该表示有带外数据到来)
  static const int flag[max_ops] = { EPOLLIN, EPOLLOUT, EPOLLPRI };
  //循环处理如下三种类型的reactor_op:
  // 1. reactive_socket_accept_op_base
  // 2. reactive_socket_recv_op_base
  // 3. reactive_socket_send_op_base    
  for (int j = max_ops - 1; j >= 0; --j)
  {
    //有读写事件、或者epoll_wait有获取到异常，如链接断开等
    if (events & (flag[j] | EPOLLERR | EPOLLHUP)) 
    {
      try_speculative_[j] = true;
	  //遍历对应数组成员的整个队列
      while (reactor_op* op = op_queue_[j].front())
      {
       //reactive_socket_accept_op_base::do_perform  reactive_socket_recv_op_base::do_perform
  //reactive_socket_send_op_base::do_perform  分别对应新链接，读取数据，发送数据的底层实现
  	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
        if (reactor_op::status status = op->perform()) 
		//status为true表示成功，false表示底层处理失败
        {
          op_queue_[j].pop();
          io_cleanup.ops_.push(op);
          if (status == reactor_op::done_and_exhausted)
          {
            try_speculative_[j] = false;
            break;
          }
        }
		//例如如果没读到一个完整的数据，则这里直接退出，继续循环处理其他网络数据，下次继续读取。
        else
          break;
      }
    }
  }

  // The first operation will be returned for completion now. The others will
  // be posted for later by the io_cleanup object's destructor.
   //这里只返回了第一个op，其他的op在~perform_io_cleanup_on_block_exit处理
  io_cleanup.first_op_ = io_cleanup.ops_.front();
  //把返回的第一个从队列中清除，剩余的op还在队列中
  io_cleanup.ops_.pop(); 

  //该逻辑的总体思路：队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行

  //只返回第一个op,外层的epoll_reactor::descriptor_state::do_complete中执行该op对应的complete
  return io_cleanup.first_op_;
}

//epoll_reactor::descriptor_state::do_complete执行
//网络IO相关的任务处理，如accept接收新链接、接收数据、发送数据
operation* epoll_reactor::descriptor_state::perform_io(uint32_t events)
{
  ......
  
  //分别对应链接到达或者数据来临、可以写数据、有紧急的数据可读(这里应该表示有带外数据到来)
  static const int flag[max_ops] = { EPOLLIN, EPOLLOUT, EPOLLPRI };
  //循环处理各自不同的reactor_op(reactive_socket_accept_op_base   reactive_socket_recv_op_base reactive_socket_send_op_base)
  for (int j = max_ops - 1; j >= 0; --j)
  {
    if (events & (flag[j] | EPOLLERR | EPOLLHUP)) //有读写事件、或者epoll_wait有获取到异常，如链接断开等
    {
      try_speculative_[j] = true;
      while (reactor_op* op = op_queue_[j].front())
      {
       //reactive_socket_accept_op_base::do_perform  reactive_socket_recv_op_base::do_perform
    //reactive_socket_send_op_base::do_perform  分别对应新链接，读取数据，发送数据的底层实现
  	//执行底层数据收发的perform_io，也就是如下：
	//1. accept处理底层实现:reactive_socket_accept_op_base::do_perform 
	//2. 读处理底层实现：reactive_socket_recv_op_base::do_perform
	//3. 写处理底层实现：reactive_socket_send_op_base::do_perform
        if (reactor_op::status status = op->perform()) 
		//status为true表示成功，false表示底层处理失败
        {
          //取出对应的op，入队到临时队列io_cleanup.ops_
          op_queue_[j].pop();
          io_cleanup.ops_.push(op);
          if (status == reactor_op::done_and_exhausted)
          {
            try_speculative_[j] = false;
            break;
          }
        }
		//如果有异常
        else
          break;
      }
    }
  }

   //这里只返回了第一个op，其他的op在~perform_io_cleanup_on_block_exit处理
  io_cleanup.first_op_ = io_cleanup.ops_.front();
  //把返回的第一个从队列中清除，剩余的op还在队列中
  io_cleanup.ops_.pop(); 

  //该逻辑的总体思路：队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行

  //只返回第一个op,外层的epoll_reactor::descriptor_state::do_complete中执行该op对应的complete
  return io_cleanup.first_op_;
}
    从上面的分析可以看出，op_queue_队列上的所有reactor_op的底层IO处理成功后，会把该reactor_op入队到临时io_cleanup.first_op_队列中，
函数结束的时候把队首的reactor_op取出然后返回，在外层的do_complete中直接执行op->complete(也就是reactor_op中的complete_func，分别对应accetp、读、写中的
reactive_socket_accept_op_base::do_perform、reactive_socket_accept_op_base::do_perform、reactive_socket_recv_op_base reactive_socket_send_op_base::do_perform  )。
    那么这里有个问题？临时队列的其他reactor_op是如何处理的呢？其他reactor_op的处理实际上由perform_io_cleanup_on_block_exit析构函数处理，下面看下该析构函数
实现：

struct epoll_reactor::perform_io_cleanup_on_block_exit
{
 //初始化构造
  explicit perform_io_cleanup_on_block_exit(epoll_reactor* r)
    : reactor_(r), first_op_(0)
  {
  }
  
  //epoll_reactor::descriptor_state::perform_io中后续收尾处理
  ~perform_io_cleanup_on_block_exit()
  { 
    //配合epoll_reactor::descriptor_state::perform_io阅读，可以看出第一个op由本线程获取，其他op放入到了ops_队列
    //队首的op任务由本线程处理，其他op任务放入全局任务队列，由线程池中线程调度执行
	if (first_op_)
    {
      //队列不为空
      if (!ops_.empty())
	  	//scheduler::post_deferred_completions，op任务放入全局队列，延迟执行
        reactor_->scheduler_.post_deferred_completions(ops_);
    }
    else
    {
	  //队首的线程由本线程处理，其他op任务放入全局任务队列, 见epoll_reactor::descriptor_state::do_complete
      reactor_->scheduler_.compensating_work_started();
    }
  }
  
  //所属epoll_reactor
  epoll_reactor* reactor_;
  //临时队列
  op_queue<operation> ops_;
  //标记队首位置
  operation* first_op_;
};





线程队列：thread_context

除了全局任务队列op_queue_外，每个工作线程还有各自的私有线程队列，该私有队列由scheduler_thread_info结构管理，如下所示：

全局任务队列，为啥还要设计一个私有任务队列，因为可以一次性入队
	completion_handler
	
如何保证读写某个fd的数据有序，如果第一个线程度一部分数据，第二个线程又读取一部分数据，无法连贯
并发如何控制

asio
1. 并发控制
2. 队列组成
3. 调度实现





























状态机依赖服务上下文ServiceContext，因此可以把ServiceContext单独出一个章节分析



